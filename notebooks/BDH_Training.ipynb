{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0AOXH5UrPfs"
      },
      "source": [
        "# Baby Dragon Hatchling (BDH) Training\n",
        "\n",
        "This notebook trains the BDH model - a biologically-inspired language model architecture.\n",
        "\n",
        "## Setup Instructions\n",
        "1. **Enable GPU**: Go to `Runtime` → `Change runtime type` → Select `T4 GPU`\n",
        "2. Run all cells in order\n",
        "\n",
        "Training takes ~10-15 minutes on Colab's free T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UDHBeLF1rPfu",
        "outputId": "00a4dfa7-ebca-4f4f-8649-5b689d4c8802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 12 09:07:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0             26W /   70W |    1570MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory from any previous sessions\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared!\")"
      ],
      "metadata": {
        "id": "8xk1AxCzrPfv",
        "outputId": "91868fe1-44bf-405a-aa20-96ca3ff69992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M0E9UY52rPfv",
        "outputId": "ee462cc5-4743-4f08-e42e-b31930b83136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bdh'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 78 (delta 15), reused 18 (delta 9), pack-reused 53 (from 1)\u001b[K\n",
            "Receiving objects: 100% (78/78), 997.08 KiB | 30.21 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "/content/bdh/bdh\n",
            "/content/bdh/bdh\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository fresh (remove any old clone first)\n",
        "!rm -rf bdh\n",
        "!git clone https://github.com/Git-Faisal/bdh.git\n",
        "%cd bdh\n",
        "!pwd  # Verify we're in /content/bdh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FL-W0EiQrPfw"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (already installed in Colab, but just to be safe)\n",
        "!pip install torch numpy requests -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SME4v9vhrPfw"
      },
      "source": [
        "## View the Model Architecture\n",
        "Let's take a quick look at the BDH model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wWYM2tB_rPfw",
        "outputId": "d5244aa7-cfd9-4014-f893-52c7369ed816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BDH Model Configuration:\n",
            "  Layers: 6\n",
            "  Embedding dimension: 256\n",
            "  Attention heads: 4\n",
            "  Dropout: 0.1\n",
            "  Vocabulary size: 256 (byte-level)\n",
            "\n",
            "Total parameters: 25,296,896 (~25.3M)\n"
          ]
        }
      ],
      "source": [
        "# Import the BDH module\n",
        "import sys\n",
        "import importlib.util\n",
        "import torch\n",
        "\n",
        "# Load bdh module from current directory\n",
        "spec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\n",
        "bdh = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"bdh\"] = bdh\n",
        "spec.loader.exec_module(bdh)\n",
        "\n",
        "# Show model configuration\n",
        "config = bdh.BDHConfig()\n",
        "print(\"BDH Model Configuration:\")\n",
        "print(f\"  Layers: {config.n_layer}\")\n",
        "print(f\"  Embedding dimension: {config.n_embd}\")\n",
        "print(f\"  Attention heads: {config.n_head}\")\n",
        "print(f\"  Dropout: {config.dropout}\")\n",
        "print(f\"  Vocabulary size: {config.vocab_size} (byte-level)\")\n",
        "\n",
        "# Create model and show parameter count\n",
        "model = bdh.BDH(config)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params:,} (~{total_params/1e6:.1f}M)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMOfXwiQrPfw"
      },
      "source": [
        "## Start Training!\n",
        "This will:\n",
        "1. Download the tiny Shakespeare dataset (~1MB)\n",
        "2. Train for 3000 iterations (~10-15 minutes)\n",
        "3. Show loss every 100 steps\n",
        "4. Generate sample text at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CexqcOFBrPfx",
        "outputId": "47799255-afd4-4703-d5d8-c5c1e49aa7a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda with dtype bfloat16\n",
            "Step: 0/3000 loss 5.66\n",
            "Step: 100/3000 loss 3.03\n",
            "Step: 200/3000 loss 2.47\n",
            "Step: 300/3000 loss 2.04\n",
            "Step: 400/3000 loss 1.75\n",
            "Step: 500/3000 loss 1.59\n",
            "Step: 600/3000 loss 1.49\n",
            "Step: 700/3000 loss 1.41\n",
            "Step: 800/3000 loss 1.37\n",
            "Step: 900/3000 loss 1.34\n",
            "Step: 1000/3000 loss 1.3\n",
            "Step: 1100/3000 loss 1.28\n",
            "Step: 1200/3000 loss 1.25\n",
            "Step: 1300/3000 loss 1.23\n",
            "Step: 1400/3000 loss 1.2\n",
            "Step: 1500/3000 loss 1.18\n",
            "Step: 1600/3000 loss 1.17\n",
            "Step: 1700/3000 loss 1.16\n",
            "Step: 1800/3000 loss 1.13\n",
            "Step: 1900/3000 loss 1.11\n",
            "Step: 2000/3000 loss 1.09\n",
            "Step: 2100/3000 loss 1.08\n",
            "Step: 2200/3000 loss 1.05\n",
            "Step: 2300/3000 loss 1.03\n",
            "Step: 2400/3000 loss 1.01\n",
            "Step: 2500/3000 loss 0.998\n",
            "Step: 2600/3000 loss 0.966\n",
            "Step: 2700/3000 loss 0.944\n",
            "Step: 2800/3000 loss 0.92\n",
            "Step: 2900/3000 loss 0.897\n",
            "Training done, now generating a sample \n",
            "To be or none with stones.\n",
            "\n",
            "LEONTES:\n",
            "It is a man that this is not to die?\n",
            "\n",
            "LEONTES:\n",
            "Why, then, I'll prove him\n"
          ]
        }
      ],
      "source": [
        "# Run training\n",
        "!python train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4FeXCqIrPfx"
      },
      "source": [
        "## Custom Text Generation (Optional)\n",
        "After training, you can generate your own text with custom prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yG0WdgSRrPfx",
        "outputId": "a11b0965-6c89-4a3a-cbda-0fa9900462a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/bdh.py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-240460938.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bdh\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Note: This cell assumes you've trained the model above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/bdh.py'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import importlib.util\n",
        "import torch\n",
        "\n",
        "# Load bdh module\n",
        "spec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\n",
        "bdh = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"bdh\"] = bdh\n",
        "spec.loader.exec_module(bdh)\n",
        "\n",
        "# Note: This cell assumes you've trained the model above\n",
        "# In the vanilla code, the model isn't saved, so this only works\n",
        "# immediately after training in the same session\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create and load model (you'd need to save/load weights for this to work)\n",
        "config = bdh.BDHConfig()\n",
        "model = bdh.BDH(config).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Your custom prompt\n",
        "prompt_text = \"To be or not to be\"  # Change this!\n",
        "\n",
        "prompt = torch.tensor(\n",
        "    bytearray(prompt_text, \"utf-8\"),\n",
        "    dtype=torch.long,\n",
        "    device=device\n",
        ").unsqueeze(0)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(prompt, max_new_tokens=200, top_k=5)\n",
        "    result = bytes(output.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(\n",
        "        errors=\"backslashreplace\"\n",
        "    )\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7umXOdfrPfx"
      },
      "source": [
        "## What to Observe\n",
        "\n",
        "During training, watch the **loss** value:\n",
        "- **Initial loss (~4-5)**: Random guessing\n",
        "- **After training (~1.0-1.5)**: Model has learned patterns!\n",
        "\n",
        "The generated text should look Shakespearean-ish by the end.\n",
        "\n",
        "## Next Steps\n",
        "- Try modifying the model config (more layers, bigger embeddings)\n",
        "- Train for more iterations\n",
        "- Experiment with different generation parameters (temperature, top_k)\n",
        "- Add code to save/load the model weights"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}