{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Dragon Hatchling (BDH) Training\n",
    "\n",
    "This notebook trains the BDH model - a biologically-inspired language model architecture.\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Enable GPU**: Go to `Runtime` → `Change runtime type` → Select `T4 GPU`\n",
    "2. Run all cells in order\n",
    "\n",
    "Training takes ~10-15 minutes on Colab's free T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Clear GPU memory from any previous sessions\nimport torch\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"GPU memory cleared!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository fresh (remove any old clone first)\n!rm -rf bdh\n!git clone https://github.com/Git-Faisal/bdh.git\n%cd bdh\n!pwd  # Verify we're in /content/bdh"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (already installed in Colab, but just to be safe)\n",
    "!pip install torch numpy requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Model Architecture\n",
    "Let's take a quick look at the BDH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the BDH module\nimport sys\nimport importlib.util\nimport torch\n\n# Load bdh module from current directory\nspec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\nbdh = importlib.util.module_from_spec(spec)\nsys.modules[\"bdh\"] = bdh\nspec.loader.exec_module(bdh)\n\n# Show model configuration\nconfig = bdh.BDHConfig()\nprint(\"BDH Model Configuration:\")\nprint(f\"  Layers: {config.n_layer}\")\nprint(f\"  Embedding dimension: {config.n_embd}\")\nprint(f\"  Attention heads: {config.n_head}\")\nprint(f\"  Dropout: {config.dropout}\")\nprint(f\"  Vocabulary size: {config.vocab_size} (byte-level)\")\n\n# Create model and show parameter count\nmodel = bdh.BDH(config)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,} (~{total_params/1e6:.1f}M)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training!\n",
    "This will:\n",
    "1. Download the tiny Shakespeare dataset (~1MB)\n",
    "2. Train for 3000 iterations (~10-15 minutes)\n",
    "3. Show loss every 100 steps\n",
    "4. Generate sample text at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Text Generation (Optional)\n",
    "After training, you can generate your own text with custom prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport importlib.util\nimport torch\n\n# Load bdh module\nspec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\nbdh = importlib.util.module_from_spec(spec)\nsys.modules[\"bdh\"] = bdh\nspec.loader.exec_module(bdh)\n\n# Note: This cell assumes you've trained the model above\n# In the vanilla code, the model isn't saved, so this only works\n# immediately after training in the same session\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create and load model (you'd need to save/load weights for this to work)\nconfig = bdh.BDHConfig()\nmodel = bdh.BDH(config).to(device)\nmodel.eval()\n\n# Your custom prompt\nprompt_text = \"To be or not to be\"  # Change this!\n\nprompt = torch.tensor(\n    bytearray(prompt_text, \"utf-8\"), \n    dtype=torch.long, \n    device=device\n).unsqueeze(0)\n\n# Generate\nwith torch.no_grad():\n    output = model.generate(prompt, max_new_tokens=200, top_k=5)\n    result = bytes(output.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(\n        errors=\"backslashreplace\"\n    )\n    print(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Observe\n",
    "\n",
    "During training, watch the **loss** value:\n",
    "- **Initial loss (~4-5)**: Random guessing\n",
    "- **After training (~1.0-1.5)**: Model has learned patterns!\n",
    "\n",
    "The generated text should look Shakespearean-ish by the end.\n",
    "\n",
    "## Next Steps\n",
    "- Try modifying the model config (more layers, bigger embeddings)\n",
    "- Train for more iterations\n",
    "- Experiment with different generation parameters (temperature, top_k)\n",
    "- Add code to save/load the model weights"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}