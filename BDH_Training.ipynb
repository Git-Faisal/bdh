{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Dragon Hatchling (BDH) Training\n",
    "\n",
    "This notebook trains the BDH model - a biologically-inspired language model architecture.\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Enable GPU**: Go to `Runtime` → `Change runtime type` → Select `T4 GPU`\n",
    "2. Run all cells in order\n",
    "\n",
    "Training takes ~10-15 minutes on Colab's free T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Git-Faisal/bdh.git\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (already installed in Colab, but just to be safe)\n",
    "!pip install torch numpy requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Model Architecture\n",
    "Let's take a quick look at the BDH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bdh\n",
    "\n",
    "# Show model configuration\n",
    "config = bdh.BDHConfig()\n",
    "print(\"BDH Model Configuration:\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Embedding dimension: {config.n_embd}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Dropout: {config.dropout}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size} (byte-level)\")\n",
    "\n",
    "# Create model and show parameter count\n",
    "import torch\n",
    "model = bdh.BDH(config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} (~{total_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training!\n",
    "This will:\n",
    "1. Download the tiny Shakespeare dataset (~1MB)\n",
    "2. Train for 3000 iterations (~10-15 minutes)\n",
    "3. Show loss every 100 steps\n",
    "4. Generate sample text at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Text Generation (Optional)\n",
    "After training, you can generate your own text with custom prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bdh\n",
    "\n",
    "# Note: This cell assumes you've trained the model above\n",
    "# In the vanilla code, the model isn't saved, so this only works\n",
    "# immediately after training in the same session\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create and load model (you'd need to save/load weights for this to work)\n",
    "config = bdh.BDHConfig()\n",
    "model = bdh.BDH(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Your custom prompt\n",
    "prompt_text = \"To be or not to be\"  # Change this!\n",
    "\n",
    "prompt = torch.tensor(\n",
    "    bytearray(prompt_text, \"utf-8\"), \n",
    "    dtype=torch.long, \n",
    "    device=device\n",
    ").unsqueeze(0)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    output = model.generate(prompt, max_new_tokens=200, top_k=5)\n",
    "    result = bytes(output.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(\n",
    "        errors=\"backslashreplace\"\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Observe\n",
    "\n",
    "During training, watch the **loss** value:\n",
    "- **Initial loss (~4-5)**: Random guessing\n",
    "- **After training (~1.0-1.5)**: Model has learned patterns!\n",
    "\n",
    "The generated text should look Shakespearean-ish by the end.\n",
    "\n",
    "## Next Steps\n",
    "- Try modifying the model config (more layers, bigger embeddings)\n",
    "- Train for more iterations\n",
    "- Experiment with different generation parameters (temperature, top_k)\n",
    "- Add code to save/load the model weights"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
