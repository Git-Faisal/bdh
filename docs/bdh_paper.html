<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</title>
<!--Generated on Tue Sep 30 16:50:07 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.26507v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS1" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Motivation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS1.SSS0.Px1" title="In 1.1 Motivation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Reconciling Reasoning Function of the Brain with Language Models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS1.SSS0.Px2" title="In 1.1 Motivation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Towards scale-free foreseeable AI.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS1.SSS0.Px3" title="In 1.1 Motivation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Introducing Axiomatic AI.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS2" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Intuition of results: combining <span class="ltx_text ltx_font_slanted">modus ponens</span> reasoning with Hebbian learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Contribution of this work</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3.SSS0.Px1" title="In 1.3 Contribution of this work ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Language Models as Local Graph Dynamics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3.SSS0.Px2" title="In 1.3 Contribution of this work ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">A tensor-friendly case of BDH: the BDH-GPU architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3.SSS0.Px3" title="In 1.3 Contribution of this work ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">The bridge between the Transformer and Brain models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3.SSS0.Px4" title="In 1.3 Contribution of this work ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Implications for learning dynamics of natural lifelong inference systems.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Notation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4.SSS0.Px1" title="In 1.4 Notation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">State-space models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4.SSS0.Px2" title="In 1.4 Notation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Models as programs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4.SSS0.Px3" title="In 1.4 Notation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Graphs and their dynamical systems interpretation.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>BDH: a language model architecture given by local distributed graph dynamics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Formalism for local graph-based language models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1.SSS0.Px1" title="In 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Introduction to distributed graph systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1.SSS0.Px2" title="In 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Programmable rulesets and the interaction kernel.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1.SSS0.Px3" title="In 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Restricting the interaction kernel to spiking signals and graph systems.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1.SSS0.Px4" title="In 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Definition of the edge-reweighting kernel.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS2" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Definition of BDH as a local edge-reweighting process (equations of reasoning)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS2.SSS0.Px1" title="In 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Inference dynamics of BDH.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS2.SSS0.Px2" title="In 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Notes on training.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS3" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Interpretation of attention as a micro-inductive bias of reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Interpretation of BDH as an oscillator network toy-model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4.SSS0.Px1" title="In 2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Definition of the toy-model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4.SSS0.Px2" title="In 2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Effects captured by the toy-model.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS5" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Expressing BDH using brain models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>BDH-GPU: a tensor-friendly version of the BDH architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS1" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Notation for BDH-GPU</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subparagraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS1.SSS0.P0.SPx1" title="In 3.1 Notation for BDH-GPU ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Nonlinearities: ReLU and LayerNorm.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subparagraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS1.SSS0.P0.SPx2" title="In 3.1 Notation for BDH-GPU ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Activation vectors and parameter matrices.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS2" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Definition of BDH-GPU as a state-space system</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS2.SSS0.Px1" title="In 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">BDH-GPU as a language model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS2.SSS0.Px2" title="In 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">State-space representation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS3" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Interpretation of BDH-GPU as a local interacting particle system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS4" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Expressing BDH-GPU using BDH: preserving parameter and state size</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS4.SSS1" title="In 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Expressing matrices <math alttext="{D_{x}},{D_{y}},E" class="ltx_Math" display="inline" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{x}},{D_{y}},E</annotation></semantics></math> as graphs <math alttext="{G_{x}},{G_{y}}" class="ltx_Math" display="inline" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>x</mi></msub><mo>,</mo><msub><mi>G</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}},{G_{y}}</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS4.SSS2" title="In 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Expressing BDH-GPU attention on graphs: sparsification and trainability of <math alttext="{G_{s}}" class="ltx_Math" display="inline" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implementation and scaling laws</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation characteristics of BDH-GPU</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1.SSS0.Px1" title="In 4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Model scaling in neuron dimension <math alttext="n" class="ltx_Math" display="inline" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1.SSS0.Px2" title="In 4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Layers and heads.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1.SSS0.Px3" title="In 4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Linear attention with state aligned to neurons.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1.SSS0.Px4" title="In 4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Sparse positive activation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparison of BDH-GPU to GPT2-like Transformers</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2.SSS0.Px1" title="In 4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Architecture differences.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2.SSS0.Px2" title="In 4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Transformer-like scaling laws.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2.SSS0.Px3" title="In 4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">FLOPS counts.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison of BDH-GPU to other sequence processing architectures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3.SSS0.Px1" title="In 4.3 Comparison of BDH-GPU to other sequence processing architectures ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Transformers with Linear Attention.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3.SSS0.Px2" title="In 4.3 Comparison of BDH-GPU to other sequence processing architectures ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Other types of Transformers.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3.SSS0.Px3" title="In 4.3 Comparison of BDH-GPU to other sequence processing architectures ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Networks with sparse activation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3.SSS0.Px4" title="In 4.3 Comparison of BDH-GPU to other sequence processing architectures ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Oscillatory SSM’s.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis: emergence of modularity and scale-free structure</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS1" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Background: modularity and scale-free property of systems</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS1.SSS0.Px1" title="In 5.1 Background: modularity and scale-free property of systems ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Importance of modularity for information propagation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS1.SSS0.Px2" title="In 5.1 Background: modularity and scale-free property of systems ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Scale-free property.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS2" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS2.SSS0.Px1" title="In 5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Definition of ReLU-lowrank.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS2.SSS0.Px2" title="In 5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Expressiveness of ReLU-lowrank in BDH-GPU and MLP in the Transformer.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>ReLU-lowrank as a signal propagation dynamics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3.SSS0.Px1" title="In 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Error of low-rank approximation (without ReLU).</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3.SSS0.Px2" title="In 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Expressiveness of ReLU-lowrank for Markov chain propagation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3.SSS0.Px3" title="In 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Propagation and reinforcement of signal.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS4" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Modularity in BDH-GPU signal propagation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS4.SSS0.Px1" title="In 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Supermodularity on input perturbation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Empirical findings: parameter distribution in ReLU-lowrank matrix products</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5.SSS0.Px1" title="In 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Choice of prior of matrix parameter distributions.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5.SSS0.Px2" title="In 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Experimental setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5.SSS0.Px3" title="In 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Findings.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Analysis: linear attention, sparse positive activation, and monosemanticity</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Macro-expressiveness of attention in BDH-GPU</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px1" title="In 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Basic properties of BDH-GPU attention.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px2" title="In 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">State capacity vs. distinction capacity.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px3" title="In 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Expressiveness of linear attention in dimension <math alttext="n" class="ltx_Math" display="inline" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px4" title="In 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Preparation of positive keys for Linear Attention.</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_subparagraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px4.SPx1" title="In Preparation of positive keys for Linear Attention. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Using LSH to move key vectors into the positive orthant.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subparagraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px4.SPx2" title="In Preparation of positive keys for Linear Attention. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Attention in the positive concept space of language and reasoning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1.SSS0.Px5" title="In 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Natural support for long context.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Micro-interpretation of attention in BDH-GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS3" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Empirical findings: monosemantic synapses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS4" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Empirical findings: sparse neuron activations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Playing with the Hatchling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS1" title="In 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Model merging: concatenating two models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS2" title="In 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Training without backpropagation through time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS1" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Takeaways for model engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS2" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Implications for brain science</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS2.SSS0.Px1" title="In 8.2 Implications for brain science ‣ 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">How this work helps with axiomatization of learning theory in the brain.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS3" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Societal impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A1" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Connection between generalization of reasoning and computational expressiveness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Further description of experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS1" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Language translation task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS2" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>BDH Scaling Experimental Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS3" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>BDH Monosemantic Synapse Experiment Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS4" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>BDH Merging Experiment Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Omitted formal claims and proofs</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS1" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Proof of Observation <span class="ltx_text ltx_ref_tag">1</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS2" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Formal statement of Claim <span class="ltx_text ltx_ref_tag">7</span> (linear attention)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS3" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Proof of Claim <span class="ltx_text ltx_ref_tag">3</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS3.SSS0.Px1" title="In C.3 Proof of Claim 3 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title">Considerations of building linear circuits.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS4" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Formal statement of Claim <span class="ltx_text ltx_ref_tag">4</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A4" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Desirable properties of a local graph dynamics for language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>BDH-GPU PyTorch code listing</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
The Dragon Hatchling: The Missing Link
<br class="ltx_break"/>between the Transformer and Models of the Brain</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Adrian Kosowski
</span><span class="ltx_author_notes">Author contributions are listed at the end of the paper.Corresponding author.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold">Przemysław Uznański<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">2</span></span></span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold">Jan Chorowski<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">2</span></span></span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold">Zuzanna Stamirowska<span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">2</span></span></span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold">Michał Bartoszkiewicz<span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium">2</span></span></span></span></span>
<br class="ltx_break"/></span>
Pathway, Palo Alto, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">research@pathway.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing.
Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.</p>
<p class="ltx_p">We introduce ‘Dragon Hatchling’ (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of <math alttext="n" class="ltx_Math" display="inline" id="m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.</p>
<p class="ltx_p">BDH is a practical, performant state-of-the-art
attention-based state space sequence learning architecture.
In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.</p>
<p class="ltx_p">BDH provides theoretical foundations for understanding model behavior in the limit of large size and reasoning time.
Our results, formalized as a chain of reductions of expressiveness in the framework of computational Complexity Theory and Distributed Computing, and combined with findings on the BDH model, show a macro-to-micro correspondence of function between the general attention mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain. These attention mechanisms formally converge as closed-form local graph dynamics at neurons and synapses: “the equations of reasoning”.</p>
<p class="ltx_p">BDH can be represented as a brain model. It contains <math alttext="n" class="ltx_Math" display="inline" id="m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons, organized as an excitatory circuit and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.</p>
<p class="ltx_p">BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks, including representation of concept abstractions, which happens even for small models, below 100M-parameter scale. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.</p>
<p class="ltx_p">We believe BDH opens the door to a new theory of “Thermodynamic Limit” behavior for language and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds for generalization of reasoning over time.</p>
<p class="ltx_p"><math alttext="\rhd" class="ltx_Math" display="inline" id="m3" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\rhd</annotation></semantics></math> Technical blog entry: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pathway.com/research/bdh" title="">https://pathway.com/research/bdh</a>.</p>
<p class="ltx_p"><math alttext="\rhd" class="ltx_Math" display="inline" id="m4" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\rhd</annotation></semantics></math> Code listings: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pathwaycom/bdh" title="">https://github.com/pathwaycom/bdh</a>.</p>
</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS1" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS2" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Intuition of results: combining <span class="ltx_text ltx_font_slanted">modus ponens</span> reasoning with Hebbian learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS3" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Contribution of this work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4" title="In 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Notation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>BDH: a language model architecture given by local distributed graph dynamics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Formalism for local graph-based language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS2" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Definition of BDH as a local edge-reweighting process (equations of reasoning)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS3" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Interpretation of attention as a micro-inductive bias of reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Interpretation of BDH as an oscillator network toy-model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS5" title="In 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Expressing BDH using brain models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>BDH-GPU: a tensor-friendly version of the BDH architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS1" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Notation for BDH-GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS2" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Definition of BDH-GPU as a state-space system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS3" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Interpretation of BDH-GPU as a local interacting particle system</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.SS4" title="In 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Expressing BDH-GPU using BDH: preserving parameter and state size</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implementation and scaling laws</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation characteristics of BDH-GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparison of BDH-GPU to GPT2-like Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS3" title="In 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison of BDH-GPU to other sequence processing architectures</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis: emergence of modularity and scale-free structure</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS1" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Background: modularity and scale-free property of systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS2" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>ReLU-lowrank as a signal propagation dynamics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS4" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Modularity in BDH-GPU signal propagation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="In 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Empirical findings: parameter distribution in ReLU-lowrank matrix products</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Analysis: linear attention, sparse positive activation, and monosemanticity</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Macro-expressiveness of attention in BDH-GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Micro-interpretation of attention in BDH-GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS3" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Empirical findings: monosemantic synapses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS4" title="In 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Empirical findings: sparse neuron activations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Playing with the Hatchling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS1" title="In 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Model merging: concatenating two models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS2" title="In 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Training without backpropagation through time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS1" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Takeaways for model engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS2" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Implications for brain science</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.SS3" title="In 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Societal impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A1" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Connection between generalization of reasoning and computational expressiveness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Further description of experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS1" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Language translation task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS2" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>BDH Scaling Experimental Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS3" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>BDH Monosemantic Synapse Experiment Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS4" title="In Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>BDH Merging Experiment Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Omitted formal claims and proofs</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS1" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Proof of Observation <span class="ltx_text ltx_ref_tag">1</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS2" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Formal statement of Claim <span class="ltx_text ltx_ref_tag">7</span> (linear attention)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS3" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Proof of Claim <span class="ltx_text ltx_ref_tag">3</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS4" title="In Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Formal statement of Claim <span class="ltx_text ltx_ref_tag">4</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A4" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Desirable properties of a local graph dynamics for language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="In The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>BDH-GPU PyTorch code listing</span></a></li>
</ol></nav>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Long reasoning and long context inference pose a severe challenge of generalization across scales of time. From vibe coding to market research, users of Language Models and agentic systems are increasingly relying on defining tasks through informal prompts, which the language model is expected to follow over long sequences of actions or decisions, like a reasonable human actor would. Implicitly, most users expect machines to follow the generalization patterns of human reasoning, i.e., to generalize reasoning in the same way as humans do. The complexity of tasks attempted in this way has gone from the equivalent of hours of human work for a single prompt, to weeks <cite class="ltx_cite ltx_citemacro_citep">(Emberson et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib31" title="">2025</a>)</cite>. However, experimental evidence suggests that the Transformer and other state-of-the-art architectures do not systematically generalize chain-of-thought (CoT) reasoning to scenarios longer than the ones seen during training <cite class="ltx_cite ltx_citemacro_citep">(Shojaee et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib98" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">Chain-of-Thought reasoning models can be considered through the lens of computational complexity theory. For a Language Model to generalize human reasoning on a given class of tasks, we expect this model to be able to emulate the corresponding reasoning function of the human brain efficiently.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We provide a more formal explanation of this point in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A1" title="Appendix A Connection between generalization of reasoning and computational expressiveness ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span>
While the Transformer with Chain-of-Thought is Turing-complete and can efficiently emulate certain restricted classes of formal languages <cite class="ltx_cite ltx_citemacro_citep">(Merrill and
Sabharwal, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib72" title="">2024</a>)</cite>, this does not in itself provide a satisfactory answer as to how it emulates human reasoning. The human brain is an extremely complex graph-based distributed computing system with <math alttext="n\approx 8\cdot 10^{10}" class="ltx_Math" display="inline" id="S1.p2.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>≈</mo><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>10</mn><mn>10</mn></msup></mrow></mrow><annotation encoding="application/x-tex">n\approx 8\cdot 10^{10}</annotation></semantics></math> neurons, and <math alttext="m&gt;10^{14}" class="ltx_Math" display="inline" id="S1.p2.m2" intent=":literal"><semantics><mrow><mi>m</mi><mo>&gt;</mo><msup><mn>10</mn><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">m&gt;10^{14}</annotation></semantics></math> neuron connections (synapses), of which a certain percentage is actively used. The direct simulation of such a distributed system by a Language Model through generic Turing-machine reductions would require billions of CoT tokens of the Language Model to represent a single step of reasoning in the brain. So, do Transformer-like models actually relate to brain function?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">Such a relationship should follow more closely from a tighter, more direct simulation. Finding such a connection between Language Models and human brain function has, so far, proved elusive. Indeed, when comparing a tensor-based Language Model based on feed-forward network blocks and attention, to a uniform, scale-free graph-based distributed system, such as the brain, the two may, at first glance, appear very dissimilar.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">This apparent dissimilarity of structure between Language Models and brain structure has been one of the main causes of concern in attempts to reconcile Computation and the Brain <cite class="ltx_cite ltx_citemacro_citep">(Olshausen, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib80" title="">2018</a>)</cite>, as well as a cause of concern regarding the difficulty to foresee the behavior of autonomous AI systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p">In this paper, we show the link between the Transformer and Brain models.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>
<div class="ltx_para ltx_noindent" id="S1.SS1.p1">
<p class="ltx_p">The development of Artificial Intelligence and the understanding of Neural Science have gone hand in hand since the 1940’s, both being efforts to understand the “mystery of intelligence”. The relationship between computing systems and the brain served as motivation for the pioneering theoreticians such as John von Neumann (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib78" title="">1958</a></cite>), Alan Turing (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib102" title="">1950</a></cite>), Goeff Hinton (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib41" title="">2005</a></cite>), Warren McCulloch and Walter Pitts (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib70" title="">1943</a></cite>), and Horace Barlow (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib8" title="">1972</a></cite>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.p2">
<p class="ltx_p">Since then, milestones in Machine Learning around Artificial Neural Networks — using backpropagation with SGD <cite class="ltx_cite ltx_citemacro_citep">(Rumelhart et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib92" title="">1986</a>)</cite>, followed by Deep Learning <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib62" title="">2015</a>)</cite>, and the Attention mechanism <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib7" title="">2015</a>; Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib104" title="">2017</a>)</cite> — have split the “mystery of how intelligence works” into two. First, we still have no clear explanation for the micro-to-macro correspondence of the reasoning function of the brain. Second, we do not understand the correspondence between the artificial and natural systems — notably, how effects observed in the brain (emergent network; sparse activations; oscillatory phenomena; unknown relationship to backpropagation mechanisms) map into those which appear in systems based on dense tensors, trained using gradient back-propagation over time.</p>
</div>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Reconciling Reasoning Function of the Brain with Language Models.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p1">
<p class="ltx_p">There is a seemingly deep divide between state-of-the-art language models, like the Transformer, and natural distributed systems with local graph dynamics, like those of the brain. Specifically, for the brain, we do not understand how the reasoning function emerges from neuronal dynamics at the microscale. For the Transformer, the interpretation of function is given at the level of vectors, but not at the level of particle dynamics or a uniform distributed computing system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p2">
<p class="ltx_p">Language and reasoning are the key areas of higher-order brain function for which we do not yet have a complete understanding. Many other areas of brain function have been explained through analogies to Machine Learning architectures.
For example, the visual cortex is becoming well-understood, especially in its peripheral layers, and the observed inference dynamics are shown to have a correspondence to known Deep Learning architectures <cite class="ltx_cite ltx_citemacro_citep">(Mohsenzadeh et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib75" title="">2020</a>)</cite>. The use of sparse coding by the brain was considered in the context of processing visual cues <cite class="ltx_cite ltx_citemacro_citep">(Olshausen and Field, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib81" title="">1997</a>)</cite>, as well as for the olfactory systems <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib64" title="">2014</a>)</cite>. By contrast, higher-order cognitive functions of the association cortex of the human brain, such as language and reasoning, are among the least understood. A number of models provide partial explanations and have been verified at small scales. Some of the first attempts include explaining context-dependent computation in the prefrontal cortex using population dynamics of an RNN <cite class="ltx_cite ltx_citemacro_citep">(Mante et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib68" title="">2013</a>)</cite>. Later approaches include the Tolman-Eichenbaum Machine <cite class="ltx_cite ltx_citemacro_citep">(Whittington et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib107" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib108" title="">2022</a>)</cite>, as well as a number of more recent works <cite class="ltx_cite ltx_citemacro_citep">(Papadimitriou et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib84" title="">2020</a>; Dabagia et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib25" title="">2024</a>; Mitropolsky and Papadimitriou, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib74" title="">2025</a>)</cite>. One of the main stumbling blocks concerns going from spiking activation patterns at neurons, and localized attention effects at synapses, to a higher-order function, serving a reasoning purpose, efficiently organized at a scale of millions to billions of neurons.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p3">
<p class="ltx_p">Conversely, for Language Models architectures such as the Transformer, we miss a compact micro-interpretation as a distributed system. The expressiveness of the Transformer has been approximated using approaches from centralized computing and Complexity Theory, rather than from distributed systems. In the centralized perspective, a language model can be seen as a transformation function from inputs into outputs. The computational expressiveness of the Transformer architecture may then be approximated through frameworks based on RASP, such as RASP-L <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib114" title="">2024</a>)</cite> or C-RASP <cite class="ltx_cite ltx_citemacro_citep">(Yang and Chiang, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib110" title="">2024</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib48" title="">2025</a>)</cite>. RASP-L provides a very convenient heuristic for estimating Transformer expressiveness at the rather coarse level of vector operations, while C-RASP provides a more specialized lower-bound on expressiveness, capturing a class of formulas of temporal counting logic. Both frameworks have been used to suggest theoretical models of task length generalization.
This type of expressiveness techniques, however, do not lead to a uniform asymptotic model for the behavior of the Transformer, whether in GPT2 architecture or simplified. The scaling of the Transformer in its different dimensions, and the need to manipulate context length, complicate this goal.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p4">
<p class="ltx_p">The lack of such a uniform model also makes it hard to compare the capabilities of the Transformer to the capabilities of the brain at the level of correspondence of structure. Generally, the temporal behavior of a state-space system is reflected in its structure<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For a linear system, temporal behavior would be a direct consequence of the spectral properties of the system. The considered systems dynamics are not linear.</span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p5">
<p class="ltx_p">Understanding whether it is possible to show alignment of the temporal behavior of two systems, which do not display any structural correspondence, and without a clear idea of how the weight tensors and state representation of one system ‘embed’ into the graph structure and state representation of the other system, is an awkward task.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px1.p6">
<p class="ltx_p">This brings us naturally to our motivational objective: Can we create Machine Learning models which are closer to the desirable properties of natural (human) reasoning systems, and which exhibit the same types of limit and scaling behavior as such natural systems?</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Towards scale-free foreseeable AI.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px2.p1">
<p class="ltx_p">Ensuring correct scaling behavior of inference over time is of paramount importance for the deployment of AI whose reasoning or actions are not subject to strict human supervision. Most reasoning models and AI agentic systems admit limit objects (i.e., extensions to infinite time and infinite size) which are Turing-complete (cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Merrill and
Sabharwal, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib72" title="">2024</a>; Pérez et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib87" title="">2021</a>; Jojic et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib50" title="">2023</a>)</cite>). This means that they should be treated like computer programs — and should be approached by the users with the same standards of care, as a computer program of unknown origin and unknown purpose.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px2.p2">
<p class="ltx_p">An AI model can malfunction when allowed to run for a long time autonomously, i.e., without human validation of actions and reasoning outcomes. The most painful of all consequences, perhaps, is the concept of a failed generalization of reasoning (a malfunction with respect to the original task objective) over time, leading to a grotesque effect known as the “Paperclip Factory” <cite class="ltx_cite ltx_citemacro_citep">(Bostrom, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib15" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px2.p3">
<p class="ltx_p">Can the risk of such unsuccessful generalization be bounded?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px2.p4">
<p class="ltx_p">There are at least two scenarios in which a black-box model <math alttext="M" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> cannot be considered to have undergone previous empirical validation, and consequently cannot be used in higher-risk autonomous AI use cases.</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">Length-generalization scenario: Model <math alttext="M" class="ltx_Math" display="inline" id="S1.I1.i1.p1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is expected to act autonomously on a task which is longer than tasks forming part of its validation set.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p">Model scaling scenario: Model <math alttext="M" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is not exactly the same closed system as the one which was tested during validation. For example, suppose that models <math alttext="M_{1}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m2" intent=":literal"><semantics><msub><mi>M</mi><mn>1</mn></msub><annotation encoding="application/x-tex">M_{1}</annotation></semantics></math> and <math alttext="M_{2}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m3" intent=":literal"><semantics><msub><mi>M</mi><mn>2</mn></msub><annotation encoding="application/x-tex">M_{2}</annotation></semantics></math> were tested individually on smaller tasks, and let <math alttext="M" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> be an agentic system composed of instances of <math alttext="M_{1}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m5" intent=":literal"><semantics><msub><mi>M</mi><mn>1</mn></msub><annotation encoding="application/x-tex">M_{1}</annotation></semantics></math> and <math alttext="M_{2}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.m6" intent=":literal"><semantics><msub><mi>M</mi><mn>2</mn></msub><annotation encoding="application/x-tex">M_{2}</annotation></semantics></math> which communicate with exchange messages with each other during inference.</p>
</div>
</li>
</ol>
<p class="ltx_p">A natural way of avoiding both difficulties consists in studying systems which are scale-free with respect to size and time, and admit a form of uniform “thermodynamic limit” behavior. The limit behavior of computational systems at criticality naturally connects the size of the system with the probable duration of its operation, with the connection usually taking polynomial form (cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Björner et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib12" title="">1991</a>; Cairns, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib19" title="">2018</a>; Rolla, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib90" title="">2020</a>)</cite> for examples of graph-based interacting particle systems for which rigorous results have been obtained in this direction). Consider a model <math alttext="M_{n}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m2" intent=":literal"><semantics><msub><mi>M</mi><mi>n</mi></msub><annotation encoding="application/x-tex">M_{n}</annotation></semantics></math> with architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>, parameterized by its size <math alttext="n" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> (with the interpretation of the number of uniform particles), and sampled from some space of <math alttext="n" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-neuron models in architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m6" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> in some space equipped with a probability measure, <math alttext="M_{n}\sim\mathcal{P}_{\mathcal{A}}(n)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m7" intent=":literal"><semantics><mrow><msub><mi>M</mi><mi>n</mi></msub><mo>∼</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi class="ltx_font_mathcaligraphic">𝒜</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">M_{n}\sim\mathcal{P}_{\mathcal{A}}(n)</annotation></semantics></math>. Informally, if the limit object <math alttext="\mathcal{P}_{\mathcal{A}}:=\lim_{n\to\infty}\mathcal{P}_{\mathcal{A}}(n)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m8" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi class="ltx_font_mathcaligraphic">𝒜</mi></msub><mo rspace="0.1389em">:=</mo><mrow><msub><mo lspace="0.1389em" rspace="0.167em">lim</mo><mrow><mi>n</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></msub><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi class="ltx_font_mathcaligraphic">𝒜</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{\mathcal{A}}:=\lim_{n\to\infty}\mathcal{P}_{\mathcal{A}}(n)</annotation></semantics></math> exists (under an appropriate, well-defined sense of uniformity of limit) then models <math alttext="M_{n}" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m9" intent=":literal"><semantics><msub><mi>M</mi><mi>n</mi></msub><annotation encoding="application/x-tex">M_{n}</annotation></semantics></math>, for <math alttext="n" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p4.m10" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> sufficiently large, will admit in the limit asymptotic properties, which can be used to characterize their behavior over time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px2.p5">
<p class="ltx_p">The existence of such a limit theory means that we can characterize, with bounded probability of error, the behavior of a family of large models, having <math alttext="O(n)" class="ltx_Math" display="inline" id="S1.SS1.SSS0.Px2.p5.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math> parameters, while relying on a theory which is independent of the specific structure and size of the specific model. In this way, the limit behavior of a system of a very large number of interacting uniform particles over time becomes (stochastically) foreseeable in the sense of its adherence to expected behavior, which can be extrapolated from observations at shorter time scales. Thus, small tests may be conceived in order to provide validation for a scale-free system at long time scales.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Introducing Axiomatic AI.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px3.p1">
<p class="ltx_p">Axiomatic systems are those in which micro-foundations and the macro-description which arises from them are consistent and well-understood. The need for axiomatic understanding was highlighted by David Hilbert (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib40" title="">1902</a></cite>), and has become the foundation in Statistical Physics (e.g. thermodynamics, fluid dynamics, spin glass theory), cellular mechanisms, Social Networks Science, and reconciliation of Microeconomics and Macroeconomics through a Network Economics perspective.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px3.p2">
<p class="ltx_p">This paper brings a micro-foundational understanding to Language Model inference, to the mechanisms of in-context learning, and Chain-of-Thought reasoning dynamics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.SSS0.Px3.p3">
<p class="ltx_p">The considerations in this work naturally support a shift of perspective from <em class="ltx_emph ltx_font_italic">Interpretable AI</em>, which gives an approximate understanding of what the model is doing now (without necessarily telling us what its current actions are going to lead to over longer time scales), to <em class="ltx_emph ltx_font_italic">Axiomatic AI</em>, where we also understand the micro-foundations of how the model can be expected to behave subsequently over time.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Intuition of results: combining <span class="ltx_text ltx_font_slanted">modus ponens</span> reasoning with Hebbian learning</h3>
<div class="ltx_para ltx_noindent" id="S1.SS2.p1">
<p class="ltx_p">In this section we provide the reader with some of the main intuitions behind this work which, we hope, will help to navigate the remaining, more formal parts of this paper with ease.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p2">
<p class="ltx_p">While there are many formal deductive systems in logic, they predominantly rely on the <span class="ltx_text ltx_font_slanted">modus ponens</span> inference rule. Applied to a rule-based reasoning system, it takes the following form:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p3">
<p class="ltx_p">If we know that the <math alttext="i" class="ltx_Math" display="inline" id="S1.SS2.p3.m1" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th fact is true, and our ruleset <math alttext="\sigma" class="ltx_Math" display="inline" id="S1.SS2.p3.m2" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> indicates that the <math alttext="i" class="ltx_Math" display="inline" id="S1.SS2.p3.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th fact implies the <math alttext="j" class="ltx_Math" display="inline" id="S1.SS2.p3.m4" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th fact, then we know that the <math alttext="j" class="ltx_Math" display="inline" id="S1.SS2.p3.m5" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th fact is true as well. In an approximate reasoning system, the strength of the rule <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S1.SS2.p3.m6" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math> indicates how the belief <math alttext="X(i)" class="ltx_Math" display="inline" id="S1.SS2.p3.m7" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> of the system affects its belief <math alttext="A(j)" class="ltx_Math" display="inline" id="S1.SS2.p3.m8" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(j)</annotation></semantics></math>. We could write:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="X(i),\sigma(i,j)\xrightarrow{}A(j)," class="ltx_Math" display="block" id="S1.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">X(i),\sigma(i,j)\xrightarrow{}A(j),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">to indicate that if <math alttext="X(i)" class="ltx_Math" display="inline" id="S1.SS2.p3.m9" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> is a weighted belief, it contributes <math alttext="X(i)\sigma(i,j)" class="ltx_Math" display="inline" id="S1.SS2.p3.m10" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)\sigma(i,j)</annotation></semantics></math> to the system’s belief <math alttext="A(j)" class="ltx_Math" display="inline" id="S1.SS2.p3.m11" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(j)</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p4">
<p class="ltx_p">Practical logical inference systems differ in strategies employed for rule selection, with the most advanced ones allowing direct manipulation of the ruleset, effectively resulting in a form of program evolution during inference<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The authors’ personal experience with writing efficient Prolog programs confirms that such direct ruleset management is often a necessary pragmatic evil, guiding the inference system in the right direction.</span></span></span>. For an approximate reasoning system, such a heuristic could manipulate the strength of rules, modulating the impact of belief <math alttext="X(i)" class="ltx_Math" display="inline" id="S1.SS2.p4.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> on the system’s belief <math alttext="A(j)" class="ltx_Math" display="inline" id="S1.SS2.p4.m2" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(j)</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p5">
<p class="ltx_p">Hebbian learning <cite class="ltx_cite ltx_citemacro_citep">(Hebb, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib38" title="">1949</a>)</cite>, often presented as the mnemonic “<em class="ltx_emph ltx_font_italic">Neurons that fire together wire together</em>”, can be seen as a heuristic for ruleset manipulation. It postulates that synaptic connections are strengthened when the activity of one neuron, <math alttext="Y(i)" class="ltx_Math" display="inline" id="S1.SS2.p5.m1" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)</annotation></semantics></math>, led to the firing of another neuron, <math alttext="X(j)" class="ltx_Math" display="inline" id="S1.SS2.p5.m2" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(j)</annotation></semantics></math>. In the context of an adaptive, approximate inference system, the Hebbian heuristic means that if during the course of operation a fact <math alttext="i" class="ltx_Math" display="inline" id="S1.SS2.p5.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> contributed some evidence for <math alttext="j" class="ltx_Math" display="inline" id="S1.SS2.p5.m4" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, the system increases the significance of the implication <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S1.SS2.p5.m5" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math>. We could write this rule as:</p>
<table class="ltx_equation ltx_eqn_table" id="S1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Y(i),X(j)\xrightarrow{}\sigma(i,j)," class="ltx_Math" display="block" id="S1.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">Y(i),X(j)\xrightarrow{}\sigma(i,j),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">with the interpretation that co-presence (or a spike) of <math alttext="Y(i)" class="ltx_Math" display="inline" id="S1.SS2.p5.m6" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)</annotation></semantics></math> followed by <math alttext="X(j)" class="ltx_Math" display="inline" id="S1.SS2.p5.m7" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(j)</annotation></semantics></math> increases <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S1.SS2.p5.m8" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math> by <math alttext="Y(i)X(j)" class="ltx_Math" display="inline" id="S1.SS2.p5.m9" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)X(j)</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p6">
<p class="ltx_p">The relations (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E1" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>) and (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E2" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>), over a set of <math alttext="n" class="ltx_Math" display="inline" id="S1.SS2.p6.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> facts, may form the basis of a simple approximate reasoning system that adapts its operation to the problem at hand. Starting with some initial connections between facts, the system applies the rules to discover new facts, at the same time reweighting the ruleset in a way that strengthens the connections between the initial and derived facts. Effectively, should the system be rerun with the new ruleset, it would arrive at similar conclusions faster.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p7">
<p class="ltx_p">Suppose now that the reasoning system is equipped with two sets of rules: a fixed set <math alttext="G" class="ltx_Math" display="inline" id="S1.SS2.p7.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> and an evolving set <math alttext="\sigma" class="ltx_Math" display="inline" id="S1.SS2.p7.m2" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>. From a machine learning perspective, the fixed ruleset <math alttext="G" class="ltx_Math" display="inline" id="S1.SS2.p7.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> can be seen as model weights in Deep Learning terminology, learned using e.g. error backpropagation on a training set. On the other hand, the evolving ruleset can be seen as the temporal state of the reasoning system, sometimes called “fast weights” <cite class="ltx_cite ltx_citemacro_citep">(Hinton and Plaut, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib42" title="">1987</a>; Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib95" title="">1993</a>; Ba et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib5" title="">2016a</a>)</cite>. Fast-weights systems have a favorable ratio of state size to parameter count. A system with <math alttext="n" class="ltx_Math" display="inline" id="S1.SS2.p7.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> facts has <math alttext="m=O(n^{2})" class="ltx_Math" display="inline" id="S1.SS2.p7.m5" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">m=O(n^{2})</annotation></semantics></math> trainable parameters (expressed using one or more <math alttext="n\times n" class="ltx_Math" display="inline" id="S1.SS2.p7.m6" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrices). A classical recurrent neural net, such as the LSTM <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib44" title="">1997</a>)</cite>, treats individual fact (neuron) activations as its state, thus maintaining only <math alttext="O(n)" class="ltx_Math" display="inline" id="S1.SS2.p7.m7" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math> state variables. On the other hand, the evolving set of fast-weights <math alttext="\sigma" class="ltx_Math" display="inline" id="S1.SS2.p7.m8" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> has <math alttext="m=O(n^{2})" class="ltx_Math" display="inline" id="S1.SS2.p7.m9" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">m=O(n^{2})</annotation></semantics></math> state entries. We believe this 1-1 ratio of trainable parameter to state size is important in designing practical reasoning systems and may justify the success of the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib104" title="">2017</a>)</cite> and state-space <cite class="ltx_cite ltx_citemacro_citep">(Gu and Dao, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib35" title="">2024</a>)</cite> sequence processing models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p8">
<p class="ltx_p">Now, bearing in mind that the trainable parameters and state have comparable size <math alttext="m" class="ltx_Math" display="inline" id="S1.SS2.p8.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, we can adjust the ratio between this value <math alttext="m" class="ltx_Math" display="inline" id="S1.SS2.p8.m2" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> and the size <math alttext="n" class="ltx_Math" display="inline" id="S1.SS2.p8.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> of the fact base. This will happen through a choice of sparsity for the <math alttext="n\times n" class="ltx_Math" display="inline" id="S1.SS2.p8.m4" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrices carrying parameters and state, resulting in a specific relationship of the two values, <math alttext="n\ll m\ll n^{2}" class="ltx_Math" display="inline" id="S1.SS2.p8.m5" intent=":literal"><semantics><mrow><mi>n</mi><mo>≪</mo><mi>m</mi><mo>≪</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n\ll m\ll n^{2}</annotation></semantics></math>. In this way, our system gets a natural interpretation in terms of graphs on <math alttext="n" class="ltx_Math" display="inline" id="S1.SS2.p8.m6" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> nodes and <math alttext="m" class="ltx_Math" display="inline" id="S1.SS2.p8.m7" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edges, with the graph edges tasked with their first two roles: carrying state, and, carrying trainable parameters. Finally, we will give our system an interpretation of a dynamical system with distributed (localized) dynamics, and we will task our edges with their third crucial role: mediating in communication between nodes of the system. In this way, through assimilation of edges to natural function in the brain, we will refer to the <math alttext="m" class="ltx_Math" display="inline" id="S1.SS2.p8.m8" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edges as <em class="ltx_emph ltx_font_italic">synapses</em> connecting a set of <math alttext="n" class="ltx_Math" display="inline" id="S1.SS2.p8.m9" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> <em class="ltx_emph ltx_font_italic">neurons</em> into a distributed graph-based system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p9">
<p class="ltx_p">In the following Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2" title="2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>, we will introduce BDH, a reasoning system that formalizes and combines relations (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E1" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>) and (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E2" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>) with dynamics involving fixed rules. The BDH system:</p>
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p">is a reasoning system, efficiently using the <span class="ltx_text ltx_font_slanted">modus ponens</span> reasoning rule with heuristic rule reweighting, based on (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E1" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>) and (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.E2" title="In 1.2 Intuition of results: combining modus ponens reasoning with Hebbian learning ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>),</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p">can be implemented with local graph dynamics, making it suitable for brain-like execution model, and amenable to a principled, axiomatic description,</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p">contains a set of fixed connections (parameters), and a set of dynamically adjusted connections (<math alttext="\sigma" class="ltx_Math" display="inline" id="S1.I2.i3.p1.m1" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>), which can be seen as its dynamic state updated with a Hebbian learning rule,</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S1.I2.i4.p1">
<p class="ltx_p">admits as its special case BDH-GPU, a GPU-efficient reasoning model architecture, introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a> and experimentally validated at scale in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7" title="7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a> in direct comparison to state-of-the-art GPT2-like Transformers.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Contribution of this work</h3>
<div class="ltx_para ltx_noindent" id="S1.SS3.p1">
<p class="ltx_p">The focus of this paper is in explaining the dynamics of the primary function of language and reasoning models: inference. We provide a description of a language model architecture which is directly comparable to the Transformer, and admits a clear and interpretable local interpretation of its inference dynamics as a programmable interacting particle system.</p>
</div>
<section class="ltx_paragraph" id="S1.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Language Models as Local Graph Dynamics.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px1.p1">
<p class="ltx_p">In Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2" title="2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>, we introduce a graph-based model architecture called <em class="ltx_emph ltx_font_italic">BDH</em>, where all model parameters are represented as topology and weights of the communication graph, and model state during inference is represented as edge-reweighting applied to this graph topology.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 1</span></span><span class="ltx_text ltx_font_bold"> </span>(informal overview of theoretical results for BDH)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">We introduce a state-space Machine Learning architecture called BDH, formed by a system of <math alttext="n" class="ltx_Math" display="inline" id="Thmclaim1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> particles called neurons which communicate in a way governed by the weights and topology of the system graph, representing a “communication by wire” network.</span></p>
<ul class="ltx_itemize" id="S1.I3">
<li class="ltx_item" id="S1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The inference dynamics of BDH, treated as a distributed system, can be represented as execution of local rulesets for </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I3.i1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic"> particles with programmable interactions, with particles acting as nodes of the interaction graph and scalar state variables located on its edges (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S2.SS2" title="2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.2</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The local kernel of BDH can be naturally expressed (emulated) by a graph-based Spiking Neural Network system capable of Hebbian learning dynamics, an Excitatory circuit, and an Inhibitory circuit on an </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I3.i2.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic">-neuron system described by a neuron interaction graph (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S2.SS5" title="2.5 Expressing BDH using brain models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.5</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px1.p2">
<p class="ltx_p">In order to train BDH efficiently and analyze its performance, we restrict it, making this restriction the core of a GPU-friendly architecture called <em class="ltx_emph ltx_font_italic">BDH-GPU</em>. This restriction is obtained by treating the communication of the <math alttext="n" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> particles as proceeding through a mean-field (“radio network”), rather than a graph (“communication by wire”), cf. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F3" title="Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a> for an explanation of how the state-space equations of BDH-GPU are obtained from BDH.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px1.p3">
<p class="ltx_p">This allows us to train a mathematically equivalent model, while localizing its state in short vectors at neurons, not at connections (synapses) of the system.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">A tensor-friendly case of BDH: the BDH-GPU architecture.</h5>
<div class="ltx_para" id="S1.SS3.SSS0.Px2.p1">
<p class="ltx_p">The BDH-GPU architecture, like the Transformer, crucially relies on an attention mechanism, and is amenable to token-parallel training on GPU for next token prediction tasks. Unlike the Transformer, activation vectors of BDH-GPU appear in a very high dimension <math alttext="n" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, are positive by design, and turn out to be sparse.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 2</span></span><span class="ltx_text ltx_font_bold"> </span>(informal overview of theoretical results for BDH-GPU)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">We introduce a Machine Learning architecture called BDH-GPU, parameterized by a single (very large) scaling parameter <math alttext="n" class="ltx_Math" display="inline" id="Thmclaim2.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> and a second parameter <math alttext="d" class="ltx_Math" display="inline" id="Thmclaim2.p1.m2" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, <math alttext="\log n&lt;d\ll n" class="ltx_Math" display="inline" id="Thmclaim2.p1.m3" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow><mo>&lt;</mo><mi>d</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">\log n&lt;d\ll n</annotation></semantics></math> (<math alttext="d=256" class="ltx_Math" display="inline" id="Thmclaim2.p1.m4" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math> in practice), such that:</span></p>
<ul class="ltx_itemize" id="S1.I4">
<li class="ltx_item" id="S1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I4.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A model in BDH-GPU </span><math alttext="(n,d)" class="ltx_Math" display="inline" id="S1.I4.i1.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n,d)</annotation></semantics></math><span class="ltx_text ltx_font_italic"> has </span><math alttext="(3+o(1))nd" class="ltx_Math" display="inline" id="S1.I4.i1.p1.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">(3+o(1))nd</annotation></semantics></math><span class="ltx_text ltx_font_italic"> parameters, and admits a precise interpretation as a state-space system following the local dynamics of a </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I4.i1.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic">-particle system in an interaction field subject to equations of state (</span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text ltx_font_italic">). This system is described by </span><math alttext="O(d)" class="ltx_Math" display="inline" id="S1.I4.i1.p1.m4" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math><span class="ltx_text ltx_font_italic"> parameters per particle, whose interaction field has mean field interpretation, which in a computational view corresponds to a particle communication network realized by means of “noisy radio broadcast”.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I4.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">BDH-GPU is a special case of BDH in the sense that, for any BDH-GPU model with </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I4.i2.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic"> particles, there exists a BDH model with </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I4.i2.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic"> particles with the same inference behavior and the same size </span><math alttext="O(nd)" class="ltx_Math" display="inline" id="S1.I4.i2.p1.m3" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math><span class="ltx_text ltx_font_italic"> of trainable parameters, with the two models being formally equivalent up to placement of Layer Norms (cf. Claims </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#Thmclaim3" title="Claim 3. ‣ 3.4.1 Expressing matrices 𝐷_𝑥,𝐷_𝑦,𝐸 as graphs 𝐺_𝑥,𝐺_𝑦 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_italic"> and </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#Thmclaim4" title="Claim 4. ‣ 3.4.2 Expressing BDH-GPU attention on graphs: sparsification and trainability of 𝐺_𝑠 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I4.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The BDH-GPU architecture relies on a combination of two blocks: a specific kind of </span><em class="ltx_emph">ReLU-lowrank</em><span class="ltx_text ltx_font_italic"> feed-forward network, and a </span><em class="ltx_emph">linear attention</em><span class="ltx_text ltx_font_italic"> mechanism, which both operate in the same neuron dimension </span><math alttext="n" class="ltx_Math" display="inline" id="S1.I4.i3.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><span class="ltx_text ltx_font_italic">, using positive activation vectors.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I4.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The mechanisms of BDH-GPU, considered at the macro-level of activation vectors in </span><math alttext="R^{n}" class="ltx_Math" display="inline" id="S1.I4.i4.p1.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math><span class="ltx_text ltx_font_italic">, can be compared to those of the Transformer (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S6.SS1" title="6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.1</span></a><span class="ltx_text ltx_font_italic">, Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S5.SS2" title="5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.2</span></a><span class="ltx_text ltx_font_italic">). This justifies the applicability of the frameworks of approximate macro-expressiveness, based on RASP </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_italic">(</span>Weiss et al.<span class="ltx_text ltx_font_italic">, </span><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib106" title="">2021</a>; Zhou et al.<span class="ltx_text ltx_font_italic">, </span><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib114" title="">2024</a>; Yang and Chiang<span class="ltx_text ltx_font_italic">, </span><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib110" title="">2024</a><span class="ltx_text ltx_font_italic">)</span></cite><span class="ltx_text ltx_font_italic"> and designed for the Transformer, to BDH-GPU.</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I4.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The micro-interpretation of BDH-GPU mechanisms as neuron-neuron interaction dynamics: (1) explains mechanisms of in-cluster communication of neurons and the spontaneous emergence of graph structure with high Newman modularity in the neuron-neuron communication network (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S5" title="5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_italic">), and (2) provides a strict correspondence between the macro-mechanism of in-context inference based on attention and the local representation of state on individual neuron-neuron pairs (synapses) with state update dynamics based on sporadic updates to synaptic edge weight (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S6" title="6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px2.p2">
<p class="ltx_p">The above results are complemented by empirical findings.</p>
</div>
<div class="ltx_theorem ltx_theorem_finding" id="Thmfinding1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Empirical Finding 1</span></span><span class="ltx_text ltx_font_bold"> </span>(informal overview of empirical results of BDH-GPU)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmfinding1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">BDH-GPU is represented as a tensor-based architecture and can be trained with standard back-propagation methods (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>).</span></p>
<ul class="ltx_itemize" id="S1.I5">
<li class="ltx_item" id="S1.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I5.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The BDH-GPU architecture is shown to follow scaling laws (parameters vs. loss) of optimized Transformers in the GPT architecture, at parameter scales between 10M to 1B, on all next token prediction tasks we tested, including tasks of language and translation reminiscent of those in the original benchmark set for the Transformer architecture (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S4.SS2" title="4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4.2</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I5.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">An emergent network reflecting the associated BDH graph dynamics can be read out directly from the parameter matrices of a trained BDH-GPU model, showing emergence of graph structure (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.5</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I5.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The positive activations of BDH-GPU exhibit sparsity (at about 5% level) in the </span><math alttext="y" class="ltx_Math" display="inline" id="S1.I5.i3.p1.m1" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math><span class="ltx_text ltx_font_italic"> vectors of its state space dynamics, with sparsity levels reflecting the amount of activity being performed by BDH-GPU for a given token (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.2</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I5.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In-context state of BDH-GPU attention is shown to localize on the same synapses (neuron-neuron links) consistently across multiple prompts, allowing for some basic features, the interpretation of the current in-context state based on the reading of state of an individual synapse associated with such a feature (cf. Section </span><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2509.26507v1#S6.SS3" title="6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.3</span></a><span class="ltx_text ltx_font_italic">).</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px2.p3">
<p class="ltx_p">A more detailed discussion of the training approach is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS2" title="B.2 BDH Scaling Experimental Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.2</span></a>, while the code listing for BDH-GPU is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>. For the purposes of our experiments, we did not apply any specific training method which would be known to guide the system towards any of the observed emergent properties. (In particular, L1-regularization was disabled.) The observed emergent effects follow naturally from the design choices of the BDH and BDH-GPU architectures, and are largely attributable to the combination of: the choice of model dimensions with comparable model-to-state ratio, reliance on linear attention in high dimension, reliance on ReLU thresholds for ensuring that activation vectors are positive (trivially) and sparse (an effect empirically noted in <cite class="ltx_cite ltx_citemacro_citep">(Haziza et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib36" title="">2025</a>)</cite>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px2.p4">
<p class="ltx_p">We also remark that the BDH-GPU architecture allows for the uniform asymptotic scaling of the model in one dimension, <math alttext="n" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px2.p4.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. For example, a composition of models, obtained by concatenation, is also model in the same architecture, with a larger value of <math alttext="n" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px2.p4.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS1" title="7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7.1</span></a> for an empirical study of this effect for practical translation tasks).
Historically, a link has been established between infinitely wide feedforward networks and Gaussian Processes <cite class="ltx_cite ltx_citemacro_citep">(Neal, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib77" title="">2012</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib63" title="">2017</a>; Yang, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib111" title="">2019</a>)</cite>. BDH allows the study of limit behavior of reasoning models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px2.p5">
<blockquote class="ltx_quote">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">With BDH and BDH-GPU, we show that Language Models can be amenable to a particle-based interpretation. In fact, two micro-foundations — particle-based behavior and logic-programming behavior of a reasoning system — fuse together in these architectures.</em></p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">The bridge between the Transformer and Brain models.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px3.p1">
<p class="ltx_p">The inference dynamics of BDH and BDH-GPU act as a natural bridge between Transformer, and neuromorphic models of the brain and its subsystems. We illustrate this in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.F1" title="Figure 1 ‣ The bridge between the Transformer and Brain models. ‣ 1.3 Contribution of this work ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="S1.F1.g1" src="x1.png" width="660"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>General overview of architectures and their relationships: the inference dynamics of BDH and BDH-GPU act as a natural bridge between Transformer and models of the brain. The two main inference mechanisms of a reasoning architecture, attention and the feed-forward network, are defined at a macro-level through tensor operations for the Transformer, and at the micro-level of neuron interactions through local graph dynamics for Brain models. The new BDH-GPU architecture is naturally defined both at the level of vectors and of particle dynamics of neurons and synapses, acting as a bridge between these two approaches. See also Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S8.T3" title="Table 3 ‣ 8.3 Societal impact ‣ 8 Conclusions ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a> at the end of the paper for a more detailed comparison of architecture properties.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S1.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Implications for learning dynamics of natural lifelong inference systems.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px4.p1">
<p class="ltx_p">A lifelong learning system progresses in time, performing extremely rapid inference, combined with several training mechanisms at different time scales.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px4.p2">
<p class="ltx_p">In this work, we provide and validate at scale a plausible explanation of what the <em class="ltx_emph ltx_font_italic">predominant</em> dynamics of such a system could look like, taking the system from ‘split-second’ scale, to the scale of inference during ‘minutes’, considering the flow of time at the natural rate of thought and language for humans.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS0.Px4.p3">
<p class="ltx_p">A complementary discussion of learning dynamics would aim to provide an explanation of how to take such a lifelong inference system from the scale of ‘minutes’ into even longer timescales. This would concern the slower transfer of “fast-weights”-like inference state to long-term memory, starting at the order of <math alttext="10^{3}" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px4.p3.m1" intent=":literal"><semantics><msup><mn>10</mn><mn>3</mn></msup><annotation encoding="application/x-tex">10^{3}</annotation></semantics></math>—<math alttext="10^{4}" class="ltx_Math" display="inline" id="S1.SS3.SSS0.Px4.p3.m2" intent=":literal"><semantics><msup><mn>10</mn><mn>4</mn></msup><annotation encoding="application/x-tex">10^{4}</annotation></semantics></math> tokens, and taking into account feedback signals. In this work, we do not provide a direct answer as to how the brain actually handles this effect at longer timescales. However, a constructive way to resolve this problem seems to be less challenging, once the local inference dynamics of the brain are better understood (we come back to this in the Conclusions). The modeling approach provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS5" title="2.5 Expressing BDH using brain models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.5</span></a> is proposed as a suitable framework for such a study.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Notation</h3>
<section class="ltx_paragraph" id="S1.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">State-space models.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px1.p1">
<p class="ltx_p">For describing inference dynamics of any system, we will use state-space notation, and consider a state-space system composed of two parts: a set of <em class="ltx_emph ltx_font_italic">model parameters</em> <math alttext="M" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> which does not change during inference, and a <em class="ltx_emph ltx_font_italic">state</em> <math alttext="\sigma(t)" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(t)</annotation></semantics></math> which changes during inference. The model performs inference following state-space equation <math alttext="\sigma(t+1):=\mathcal{A}(M,\sigma(t),a_{t})" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sigma(t+1):=\mathcal{A}(M,\sigma(t),a_{t})</annotation></semantics></math>, where <math alttext="a_{t}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m4" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> is a possible external input to the system at time <math alttext="t" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> (such as a language token), <math alttext="t=0,1,2,\ldots" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m6" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow><annotation encoding="application/x-tex">t=0,1,2,\ldots</annotation></semantics></math>, and <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m7" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> is referred to as the <em class="ltx_emph ltx_font_italic">architecture</em> <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m8" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> which drives its progress. During inference without external input, usually autoregressive inference, we will shorten this to <math alttext="\sigma(t):=\mathcal{A}^{t}(M,\sigma_{0})" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px1.p1.m9" intent=":literal"><semantics><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><msub><mi>σ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sigma(t):=\mathcal{A}^{t}(M,\sigma_{0})</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Models as programs.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px2.p1">
<p class="ltx_p">In settings that are of interest to us (inference with combining multiple facts, reasoning), we opt for terminology from computing. <math alttext="M" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> has the interpretation of a computer program code, <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px2.p1.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> has the interpretation of a computational machine architecture which runs it, and <math alttext="\sigma" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px2.p1.m3" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> has the interpretation of the variable state of the program. We will use the terms ‘model <math alttext="M" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>’ and ‘program <math alttext="M" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px2.p1.m5" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>’ interchangeably.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Graphs and their dynamical systems interpretation.</h5>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px3.p1">
<p class="ltx_p">For a square matrix with non-negative coefficients, <math alttext="H\in(\mathbb{R}^{+})^{n,n}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m1" intent=":literal"><semantics><mrow><mi>H</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo>,</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">H\in(\mathbb{R}^{+})^{n,n}</annotation></semantics></math>, <math alttext="n\in\mathbb{N}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m2" intent=":literal"><semantics><mrow><mi>n</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">n\in\mathbb{N}</annotation></semantics></math>, we will consider two more equivalent representations. In one, we will treat <math alttext="H" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m3" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> as a graph defined on some nodeset <math alttext="V" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m4" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>, with <math alttext="V=|n|" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m5" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="false">|</mo><mi>n</mi><mo stretchy="false">|</mo></mrow></mrow><annotation encoding="application/x-tex">V=|n|</annotation></semantics></math>. Formally, we can take <math alttext="V=\{e_{1},\ldots,e_{n}\}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m6" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">V=\{e_{1},\ldots,e_{n}\}</annotation></semantics></math>, where <math alttext="e_{i}=(0,\ldots,0,1,0\ldots,0)\in\mathbb{R}^{n\times 1}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m7" intent=":literal"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mrow><mn>0</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e_{i}=(0,\ldots,0,1,0\ldots,0)\in\mathbb{R}^{n\times 1}</annotation></semantics></math> with <math alttext="1" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m8" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> on the <math alttext="i" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m9" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th position, forming an orthonormal basis. Non-zero entries of <math alttext="H" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m10" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> are referred to as <em class="ltx_emph ltx_font_italic">edges</em>. By an overloading of notation, we will write <math alttext="H(i,j):={e_{j}}^{T}H{e_{i}}\geq 0" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m11" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mmultiscripts><mi>e</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>e</mi><mi>i</mi></msub></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">H(i,j):={e_{j}}^{T}H{e_{i}}\geq 0</annotation></semantics></math>, to represent the node affinity function, or <em class="ltx_emph ltx_font_italic">edge weight</em>, from <math alttext="i" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m12" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> to <math alttext="j" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m13" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. We define the <em class="ltx_emph ltx_font_italic">edge set</em> <math alttext="E(H):=\{(i,j)\in V\times V:H(i,j)&gt;0\}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p1.m14" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>H</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mo stretchy="false">{</mo><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mi>V</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>V</mi></mrow></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">E(H):=\{(i,j)\in V\times V:H(i,j)&gt;0\}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px3.p2">
<p class="ltx_p">In discussions of graph-based model architectures, we will take the standard interpretation of graphs from a linear dynamical systems perspective, applied to positive vectors. When <math alttext="v\in(\mathbb{R}^{+})^{n\times 1}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m1" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v\in(\mathbb{R}^{+})^{n\times 1}</annotation></semantics></math> is a non-negative vector, <math alttext="Hv\in(\mathbb{R}^{+})^{n}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m2" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>ℝ</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">Hv\in(\mathbb{R}^{+})^{n}</annotation></semantics></math> has the interpretation of a linear transformation of <math alttext="v" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m3" intent=":literal"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>. If <math alttext="H" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m4" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> satisfies the condition of stochasticity (column-normalization to <math alttext="1" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m5" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>), then <math alttext="v\mapsto Hv" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m6" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto Hv</annotation></semantics></math> is a Markov chain transition, with <math alttext="\|Hv\|_{1}=\|v\|_{1}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m7" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>=</mo><msub><mrow><mo stretchy="false">‖</mo><mi>v</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\|Hv\|_{1}=\|v\|_{1}</annotation></semantics></math>. From a distributed systems perspective, transitions of stochastic matrices can be represented either through the direct simulation of (probabilities) of such a Markov chain, or described by the token dynamics of an extremely simple stochastic token distribution scheme in which a token located at node <math alttext="e_{i}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m8" intent=":literal"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding="application/x-tex">e_{i}</annotation></semantics></math> goes to node <math alttext="e_{j}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m9" intent=":literal"><semantics><msub><mi>e</mi><mi>j</mi></msub><annotation encoding="application/x-tex">e_{j}</annotation></semantics></math> with probability <math alttext="H(i,j)" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m10" intent=":literal"><semantics><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(i,j)</annotation></semantics></math>. If <math alttext="H" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m11" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> is not stochastic, the operation <math alttext="v\mapsto Hv" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p2.m12" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto Hv</annotation></semantics></math> additionally necessitates the suppression of a fraction of tokens, or the multiplication of tokens, at each step at each node, depending on the column-normalization of a given node.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We provide a graph distributed systems interpretation only for dynamics on graphs with non-negative matrix entries (positive-weight edges). Negative-weight edges are hard to represent using natural local dynamics based on token distribution or spiking models.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px3.p3">
<p class="ltx_p">For two graphs <math alttext="H_{1},H_{2}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>H</mi><mn>1</mn></msub><mo>,</mo><msub><mi>H</mi><mn>2</mn></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">H_{1},H_{2}\in\mathbb{R}^{n\times n}</annotation></semantics></math>, the graph <math alttext="H=H_{2}H_{1}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p3.m2" intent=":literal"><semantics><mrow><mi>H</mi><mo>=</mo><mrow><msub><mi>H</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>H</mi><mn>1</mn></msub></mrow></mrow><annotation encoding="application/x-tex">H=H_{2}H_{1}</annotation></semantics></math> is obtained through (linear algebraic) matrix multiplication, and in a distributed system, the corresponding transition <math alttext="v\mapsto Hv" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p3.m3" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto Hv</annotation></semantics></math> is obtained with two steps of token dynamics, one following graph <math alttext="H_{1}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p3.m4" intent=":literal"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_{1}</annotation></semantics></math>, the next following graph <math alttext="H_{2}" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p3.m5" intent=":literal"><semantics><msub><mi>H</mi><mn>2</mn></msub><annotation encoding="application/x-tex">H_{2}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS0.Px3.p4">
<p class="ltx_p">Representing <math alttext="m" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edge-weights of a sparse <math alttext="n" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-node graph with <math alttext="b" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m3" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> bits of numerical precision per parameter is possible with <math alttext="O(m(b+\log n))" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m4" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>b</mi><mo>+</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(m(b+\log n))</annotation></semantics></math> bits of information, which corresponds to <math alttext="O(m(1+\frac{\log n}{b}))" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m5" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow><mi>b</mi></mfrac></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(m(1+\frac{\log n}{b}))</annotation></semantics></math> parameters. For the sake of simplicity, we will assume in asymptotics that the second term of the sum does not dominate (i.e., <math alttext="\log n=O(b)" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m6" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\log n=O(b)</annotation></semantics></math>), and so we simply say that we represent the graph with <math alttext="O(m)" class="ltx_Math" display="inline" id="S1.SS4.SSS0.Px3.p4.m7" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(m)</annotation></semantics></math> parameters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>BDH: a language model architecture given by local distributed graph dynamics</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Formalism for local graph-based language models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p">We consider model architectures <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> which correspond to models of graph-based distributed computing (cf. <cite class="ltx_cite ltx_citemacro_citep">(Peleg, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib86" title="">2000</a>; Hirvonen and Suomela, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib43" title="">2025</a>)</cite>). A specific model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.p1.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> in architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.p1.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> corresponds to the weights and topology of the communication graph or graphs used by such a system.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Introduction to distributed graph systems.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">The distributed system architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>, representing the model architecture, is defined through a <em class="ltx_emph ltx_font_italic">scheduler</em>, and a local dynamics (<em class="ltx_emph ltx_font_italic">kernel</em> <math alttext="K(\mathcal{A})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">K(\mathcal{A})</annotation></semantics></math>) describing the local computations to be performed at each node of the system, and, communication between pairs of nodes connected by edges of the graph representing a given model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px1.p2">
<p class="ltx_p">We will generally accept that computations are performed only at <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neuron nodes (particles), whereas state variables of the system may appear both on nodes and edges. We will, for simplicity of analysis, consider systems governed by a <em class="ltx_emph ltx_font_italic">synchronous scheduler</em>, which in successive rounds, acts in two sub-rounds:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Computation:</span> computations of the kernel of <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> are run at all neuron nodes independently.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Communication “over wire”</span>: each neuron node sends specified ‘output variables’ to specified ‘input variables’ of its neighboring neurons.</p>
</div>
</li>
</ol>
<p class="ltx_p">We expect the scheduler to follow the same communication pattern between neurons over time in a uniform way. In order to avoid artificial constructions of cyclic time-counters at nodes, we will define the architecture kernel through a short sequence of kernels, with the scheduler executing them in successive rounds in round-robin manner. Specifically, when <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> is BDH, we will have a sequence of four kernels, <math alttext="K(\mathcal{A})=(K_{1}(\mathcal{A}),K_{2}(\mathcal{A}),K_{3}(\mathcal{A}),K_{4}(\mathcal{A}))" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>K</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>K</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>K</mi><mn>3</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>K</mi><mn>4</mn></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">K(\mathcal{A})=(K_{1}(\mathcal{A}),K_{2}(\mathcal{A}),K_{3}(\mathcal{A}),K_{4}(\mathcal{A}))</annotation></semantics></math>, with <math alttext="K_{i}(\mathcal{A})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">K_{i}(\mathcal{A})</annotation></semantics></math> being executed in every round <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m5" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> such that <math alttext="r\equiv i\textrm{\, mod \, }4" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p2.m6" intent=":literal"><semantics><mrow><mi>r</mi><mo>≡</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mtext> mod </mtext><mo lspace="0em" rspace="0em">​</mo><mn>4</mn></mrow></mrow><annotation encoding="application/x-tex">r\equiv i\textrm{\, mod \, }4</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Programmable rulesets and the interaction kernel.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p">We recall from Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4" title="1.4 Notation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1.4</span></a> that a model architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> has the interpretation of a computational machine architecture, and models <math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> have the interpretation of programs in architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>. We also recall that a graph-based model <math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is defined through a set of parameters which represent the topology and weights of the communication graph of the system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p2">
<p class="ltx_p">The above considerations lead directly to the following observation: <em class="ltx_emph ltx_font_italic">The graph of the communication network, which is used for communication between sites by the distributed system architecture <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> during reasoning and language inference, has the interpretation of a (trainable, rule-based) program.</em> Consequently, we embed the subsequent definition of BDH in a kernel formalism, given through a form of <em class="ltx_emph ltx_font_italic">programmable rulesets</em>, using two-particle interaction rules on a graph.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We refer the reader to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A4" title="Appendix D Desirable properties of a local graph dynamics for language models ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">D</span></a> for a more principled background discussion, guiding the appropriate choice of formalism for rule-based local interaction.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p3">
<p class="ltx_p">The rulesets which we will use to define BDH will closely resemble rulesets (protocols) known from evolutionary and population dynamics <cite class="ltx_cite ltx_citemacro_citep">(Hofbauer and Sigmund, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib45" title="">1998</a>; Angluin et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib3" title="">2006</a>; Aspnes and Ruppert, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib4" title="">2009</a>)</cite> and chemical reaction networks <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib20" title="">2014</a>; Feinberg, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib32" title="">2019</a>)</cite>, however, they will be restricted to a special class of interactions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p4">
<p class="ltx_p">We start by presenting the more general form of this <em class="ltx_emph ltx_font_italic">interaction kernel</em>. We then explain how such a kernel can be restricted, allowing it to be naturally implemented using a local graph-based distributed system (in particular, one relying spiking dynamics), while remaining sufficiently expressive to describe an attention-based language model. The resulting restriction will be called the <em class="ltx_emph ltx_font_italic">edge-reweighting kernel</em>.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Interaction kernel, general form)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A system with <math alttext="z" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m1" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> species, <math alttext="z\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m2" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">z\in\mathbb{N}</annotation></semantics></math>, and state <math alttext="(q_{1},\ldots,q_{z})\in Q" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>q</mi><mi>z</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q_{1},\ldots,q_{z})\in Q</annotation></semantics></math>, <math alttext="q_{i}\in R^{+}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>R</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">q_{i}\in R^{+}</annotation></semantics></math>, performs the <em class="ltx_emph ltx_font_upright">interaction kernel with a ruleset (protocol) <math alttext="P" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math></em> given by a set of transition rates called <em class="ltx_emph ltx_font_upright">rule weights</em>, <math alttext="P=((r_{ijk}\in R^{+})_{i,j,k\in\{1\ldots,z\}},(d_{k}\in R^{+})_{k\in\{1\ldots,z\}})" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m6" intent=":literal"><semantics><mrow><mi>P</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mo>+</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mi>z</mi><mo stretchy="false">}</mo></mrow></mrow></msub><mo>,</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>R</mi><mo>+</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mi>z</mi><mo stretchy="false">}</mo></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P=((r_{ijk}\in R^{+})_{i,j,k\in\{1\ldots,z\}},(d_{k}\in R^{+})_{k\in\{1\ldots,z\}})</annotation></semantics></math>, producing the following transition from a state <math alttext="(q_{1},\ldots,q_{z})\in Q" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m7" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>q</mi><mi>z</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q_{1},\ldots,q_{z})\in Q</annotation></semantics></math> to a state <math alttext="(q^{\prime}_{1},\ldots,q^{\prime}_{z})\in Q" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m8" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msubsup><mi>q</mi><mn>1</mn><mo>′</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>q</mi><mi>z</mi><mo>′</mo></msubsup><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">(q^{\prime}_{1},\ldots,q^{\prime}_{z})\in Q</annotation></semantics></math>:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q_{k}^{\prime}:=(1-d_{k})q_{k}+\sum_{i,j}r_{ijk}q_{i}q_{j}" class="ltx_Math" display="block" id="S2.E3.m1" intent=":literal"><semantics><mrow><msubsup><mi>q</mi><mi>k</mi><mo>′</mo></msubsup><mo>:=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>d</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>k</mi></msub></mrow><mo rspace="0.055em">+</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">q_{k}^{\prime}:=(1-d_{k})q_{k}+\sum_{i,j}r_{ijk}q_{i}q_{j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">We will describe such a ruleset <math alttext="P" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m9" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> using the notational form:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext='P=(\{``q_{i},q_{j}\xrightarrow{r_{ijk}}q_{k}"\}_{i,j,k\in\{1\ldots,z\}},\{``q_{k}\downarrow_{d_{k}}\!\!"\}_{k\in\{1\ldots,z\}}).' class="ltx_Math" display="block" id="S2.Ex1.m1" intent=":literal"><semantics><mrow><mrow><mi>P</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mrow><mo stretchy="false">{</mo><mrow><mrow><mrow><mi mathvariant="normal">‘</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">‘</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo>,</mo><msub><mi>q</mi><mi>j</mi></msub></mrow><mover accent="true"><mo stretchy="false">→</mo><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub></mover><mrow><msub><mi>q</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">”</mi></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mi>z</mi><mo stretchy="false">}</mo></mrow></mrow></msub><mo>,</mo><msub><mrow><mo stretchy="false">{</mo><mrow><mrow><mi mathvariant="normal">‘</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">‘</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>k</mi></msub></mrow><msub><mo stretchy="false">↓</mo><msub><mi>d</mi><mi>k</mi></msub></msub><mi mathvariant="normal">”</mi></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mi>z</mi><mo stretchy="false">}</mo></mrow></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">P=(\{``q_{i},q_{j}\xrightarrow{r_{ijk}}q_{k}"\}_{i,j,k\in\{1\ldots,z\}},\{``q_{k}\downarrow_{d_{k}}\!\!"\}_{k\in\{1\ldots,z\}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">As a matter of convention, omitted rules correspond to <math alttext="r_{ijk}=0" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m10" intent=":literal"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">r_{ijk}=0</annotation></semantics></math> (respectively, <math alttext="d_{k}=0" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m11" intent=":literal"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d_{k}=0</annotation></semantics></math>), while rules with no rate value stated next the pointer correspond to <math alttext="r_{ijk}=1" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m12" intent=":literal"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{ijk}=1</annotation></semantics></math> (respectively, <math alttext="d_{k}=1" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m13" intent=":literal"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d_{k}=1</annotation></semantics></math>). If <math alttext="q_{j}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m14" intent=":literal"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding="application/x-tex">q_{j}</annotation></semantics></math> is omitted from notation on the left-hand side, we assume <math alttext="q_{j}=1" class="ltx_Math" display="inline" id="Thmdefinition1.p1.m15" intent=":literal"><semantics><mrow><msub><mi>q</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">q_{j}=1</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p5">
<p class="ltx_p">Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.E3" title="In Definition 1 (Interaction kernel, general form). ‣ Programmable rulesets and the interaction kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>) captures the dynamics of the following differential equation: <math alttext="\frac{dq_{k}}{dt}=-d_{k}q_{k}+\sum_{i,j}r_{ijk}q_{i}q_{j}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>k</mi></msub></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mrow><mo>−</mo><mrow><msub><mi>d</mi><mi>k</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>k</mi></msub></mrow></mrow><mo rspace="0.055em">+</mo><mrow><msub><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\frac{dq_{k}}{dt}=-d_{k}q_{k}+\sum_{i,j}r_{ijk}q_{i}q_{j}</annotation></semantics></math>. Assuming <math alttext="q_{i},q_{j},r_{ijk}\in[0,1]" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msub><mi>q</mi><mi>j</mi></msub><mo>,</mo><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">q_{i},q_{j},r_{ijk}\in[0,1]</annotation></semantics></math>, the expression <math alttext="r_{ijk}q_{i}q_{j}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m3" intent=":literal"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">r_{ijk}q_{i}q_{j}</annotation></semantics></math> has the interpretation of a population dynamics or chemical process of the form “<math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m4" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m5" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> give <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m6" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>”, with this processes happening at rate <math alttext="r_{ijk}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m7" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow></msub><annotation encoding="application/x-tex">r_{ijk}</annotation></semantics></math>, assuming <math alttext="q_{i},q_{j},q_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m8" intent=":literal"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msub><mi>q</mi><mi>j</mi></msub><mo>,</mo><msub><mi>q</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">q_{i},q_{j},q_{k}</annotation></semantics></math> have the interpretation of concentrations of species <math alttext="i,j,k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p5.m9" intent=":literal"><semantics><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">i,j,k</annotation></semantics></math>. The formalism we use here assumes non-normalized state variables.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px2.p6">
<p class="ltx_p">We will subsequently use a restriction of the interaction kernel to graph-based systems, which we call the <em class="ltx_emph ltx_font_italic">edge-reweighting kernel</em>, to describe BDH.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Restricting the interaction kernel to spiking signals and graph systems.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p1">
<p class="ltx_p">First, we observe that rules of the form used in the interaction kernel from Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition1" title="Definition 1 (Interaction kernel, general form). ‣ Programmable rulesets and the interaction kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a> are extremely easy to implement in systems which rely on stochastic 0/1-valued signals. When <math alttext="\hat{q}_{i}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m1" intent=":literal"><semantics><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{q}_{i}</annotation></semantics></math> and <math alttext="\hat{q}_{j}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m2" intent=":literal"><semantics><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\hat{q}_{j}</annotation></semantics></math> are independent random variables in <math alttext="\{0,1\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0,1\}</annotation></semantics></math>, with <math alttext="\Pr[\hat{q}_{i}=1]=q_{i}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m4" intent=":literal"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Pr[\hat{q}_{i}=1]=q_{i}</annotation></semantics></math> and <math alttext="\Pr[\hat{q}_{j}=1]=q_{j}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><msub><mi>q</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\Pr[\hat{q}_{j}=1]=q_{j}</annotation></semantics></math>, then <math alttext="q_{i},q_{j}\xrightarrow{}q_{k}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><msub><mi>q</mi><mi>j</mi></msub></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><msub><mi>q</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">q_{i},q_{j}\xrightarrow{}q_{k}</annotation></semantics></math> is expressible as the “AND gate” of probability: the random variable <math alttext="\delta\hat{q}_{k}:=q_{i}q_{j}\in\{0,1\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m7" intent=":literal"><semantics><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>k</mi></msub></mrow><mo>:=</mo><mrow><msub><mi>q</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>j</mi></msub></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\delta\hat{q}_{k}:=q_{i}q_{j}\in\{0,1\}</annotation></semantics></math> gives the same expected contribution <math alttext="\mathbb{E}\delta\hat{q}_{k}=q_{i}q_{j}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p1.m8" intent=":literal"><semantics><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>k</mi></msub></mrow><mo>=</mo><mrow><msub><mi>q</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>q</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}\delta\hat{q}_{k}=q_{i}q_{j}</annotation></semantics></math> as the considered rule.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p2">
<p class="ltx_p">We now consider the restriction of interaction kernels to the case of graph systems. In the general formalism, <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> can be arbitrary with respect to <math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. By contrast, consider graph systems, which describe binary relations between nodes, and not (directly) three-point relations. To resolve this, we will require that <math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m4" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, <math alttext="j" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m5" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, and <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p2.m6" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> have the interpretation of two nodes of a graph and an edge which is incident to them.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p3">
<p class="ltx_p">For an anchoring in the literature of dynamical systems, we note that already systems following an interaction kernel with a strongly constrained <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p3.m1" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> of the form <math alttext="k\in\{i,j\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p3.m2" intent=":literal"><semantics><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k\in\{i,j\}</annotation></semantics></math>, exhibit powerful nonlinearities: with such a restriction on <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px3.p3.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>, Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.E3" title="In Definition 1 (Interaction kernel, general form). ‣ Programmable rulesets and the interaction kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>) describes the class of evolutionary systems following the equations of <em class="ltx_emph ltx_font_italic">replicator dynamics</em> <cite class="ltx_cite ltx_citemacro_citep">(Hofbauer and Sigmund, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib45" title="">1998</a>)</cite>, also equivalently known as a non-normalized form of the fundamental Lotka-Volterra predator-prey dynamics. Replicator dynamics can naturally be represented as graph systems whose parameters are defined on <em class="ltx_emph ltx_font_italic">on edges of the graph</em>, but whose state is updated on <em class="ltx_emph ltx_font_italic">on nodes of the graph</em>. By contrast, when defining dynamics for reasoning in the current work, we will also need to capture a more powerful class of graph-based systems, where, crucially, state is larger than the number of neuron nodes, appearing on neuron-neuron edges (synapses).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px3.p4">
<p class="ltx_p">We are now ready to describe a restriction of the interaction kernel from Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition1" title="Definition 1 (Interaction kernel, general form). ‣ Programmable rulesets and the interaction kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a> to the case of node-edge-node interaction rulesets in a graph: the <em class="ltx_emph ltx_font_italic">edge-reweighting kernel</em>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Definition of the edge-reweighting kernel.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px4.p1">
<p class="ltx_p">We consider a graph system with <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> <em class="ltx_emph ltx_font_italic">nodes</em>, indexed <math alttext="V=\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m2" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">V=\{1,\ldots,n\}</annotation></semantics></math>. Additionally, a subset <math alttext="E" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m3" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> of pairs of indexes <math alttext="(i,j)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>, for <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math> forms the <em class="ltx_emph ltx_font_italic">edges</em> of the system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px4.p2">
<p class="ltx_p">The system has state variables associated (uniformly) with nodes and edges, which we denote with capital letters, e.g., <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math>, for <math alttext="i\in V" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m2" intent=":literal"><semantics><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">i\in V</annotation></semantics></math> or <math alttext="Z(i,j)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m3" intent=":literal"><semantics><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z(i,j)</annotation></semantics></math>, for <math alttext="(i,j)\in E" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p2.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">(i,j)\in E</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 2</span></span><span class="ltx_text ltx_font_bold"> </span>(edge-reweighting kernel)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A distributed system follows the <em class="ltx_emph ltx_font_upright">edge-reweighting kernel</em> if its dynamics are given by the interaction kernel (Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition1" title="Definition 1 (Interaction kernel, general form). ‣ Programmable rulesets and the interaction kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>) with a set of non-negative state variables, defined on the set of nodes <math alttext="V" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m1" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> and set of edges <math alttext="E" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m2" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> of a graph, such that each local rule with non-zero rate is either a <em class="ltx_emph ltx_font_upright">computational rule</em> involving only state variables on a single node <math alttext="i\in V" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m3" intent=":literal"><semantics><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">i\in V</annotation></semantics></math>, or a <em class="ltx_emph ltx_font_upright">communication rule</em> for an edge <math alttext="(i,j)\in E" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">(i,j)\in E</annotation></semantics></math>, involving state variables from the nodes <math alttext="i,j" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m5" intent=":literal"><semantics><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math> and edge <math alttext="(i,j)" class="ltx_Math" display="inline" id="Thmdefinition2.p1.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px4.p3">
<p class="ltx_p">For context, we remark that, in comparison to the strictly simpler dynamics of node-reweighting governed by graph-based replicator dynamics equations, dynamical systems based on the edge-reweighting kernel given by Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition2" title="Definition 2 (edge-reweighting kernel). ‣ Definition of the edge-reweighting kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a> are rather elusive to study. We credit the seminal work of Algorithms theory <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib23" title="">2011</a>)</cite>[Fig. 1, Thm 3.2] as the first rigorous study of local edge-reweighting graph dynamics, combining fast-paced linear kernels on nodes with a slower-paced edge-reweighting process, in order to refine (‘focus’) electrical flows on graphs towards a sharper form of cost optimality.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>The graph dynamics used in this approach are naturally phrased in distributed computing parlance, see <cite class="ltx_cite ltx_citemacro_citep">(Becchetti et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib9" title="">2018</a>; Zou, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib115" title="">2019</a>)</cite>.</span></span></span> The BDH dynamics that we will introduce here rely on fundamentally different nonlinearities in the process, and will have the interpretation of guiding the system from premises defined at a subset of nodes, towards search targets at nodes representing a desired outcome, through reasoning inference rules with tunable weights set on edges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px4.p4">
<p class="ltx_p">In the following Subsection, we will use the introduced formalism to define BDH as an edge-reweighting kernel on the union of edges of several graphs (<math alttext="{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{i}},{G_{s}}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p4.m1" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{i}},{G_{s}}</annotation></semantics></math>) with the same set of <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p4.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> nodes.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Definition of BDH as a local edge-reweighting process (equations of reasoning)</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p">Bearing in mind the discussion of graph dynamics suitable for the case of language inference, and specifically the definition of the edge-reweighting kernel (Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition2" title="Definition 2 (edge-reweighting kernel). ‣ Definition of the edge-reweighting kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>), we are now ready to formalize the state-space dynamics of Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>) as a local graph dynamics.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The BDH model with <math alttext="n" class="ltx_Math" display="inline" id="Thmdefinition3.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons, with parameters expressed through graphs <math alttext="{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{i}},{G_{s}}" class="ltx_Math" display="inline" id="Thmdefinition3.p1.m2" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{i}},{G_{s}}</annotation></semantics></math> is represented as the ruleset of the edge-reweighting kernel, with <math alttext="O(n+|E({G_{s}})|)" class="ltx_Math" display="inline" id="Thmdefinition3.p1.m3" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>+</mo><mrow><mo stretchy="false">|</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>G</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">|</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n+|E({G_{s}})|)</annotation></semantics></math> state variables, with rule amplitudes given by “the equations of reasoning” in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>.</span></p>
</div>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Inference dynamics of BDH.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p">The BDH dynamics rely on rapid pulse dynamics with state variables <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math>, <math alttext="Y(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)</annotation></semantics></math>, <math alttext="A(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(i)</annotation></semantics></math>, defined on the <math alttext="n" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neuron sites of the system, and fast-weight-like state variables <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m5" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math>, defined on a subset of edges of the system, <math alttext="(i,j)\in E({G_{s}})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>G</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(i,j)\in E({G_{s}})</annotation></semantics></math>. The full implementation of BDH shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>(b) also includes auxiliary state variables <math alttext="X^{\mathfrak{e}}(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m7" intent=":literal"><semantics><mrow><msup><mi>X</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X^{\mathfrak{e}}(i)</annotation></semantics></math>, <math alttext="X^{\mathfrak{i}}(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m8" intent=":literal"><semantics><mrow><msup><mi>X</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X^{\mathfrak{i}}(i)</annotation></semantics></math>, <math alttext="Y^{\mathfrak{e}}(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m9" intent=":literal"><semantics><mrow><msup><mi>Y</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y^{\mathfrak{e}}(i)</annotation></semantics></math>, <math alttext="Y^{\mathfrak{i}}(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m10" intent=":literal"><semantics><mrow><msup><mi>Y</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y^{\mathfrak{i}}(i)</annotation></semantics></math> which are used as temporary counters, for integration of excitatory and inhibitory signals received by neurons. The dynamics also rely on a set of damping hyperparameters on state, <math alttext="u&gt;0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m11" intent=":literal"><semantics><mrow><mi>u</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">u&gt;0</annotation></semantics></math>, which may in full generality be defined separately as <math alttext="u(i,j)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m12" intent=":literal"><semantics><mrow><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(i,j)</annotation></semantics></math> for each edge <math alttext="(i,j)\in E({G_{s}})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.m13" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>G</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(i,j)\in E({G_{s}})</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p2">
<p class="ltx_p">Inference with BDH is performed as follows. For some parameter <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> (e.g. <math alttext="L=8" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">L=8</annotation></semantics></math> in most of this paper), which would correspond to the number of layers in a Transformer-like system, the system scheduler proceeds through rules in round-robin manner, ingesting new tokens every <math alttext="4L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">4L</annotation></semantics></math> rounds and retrieving results <math alttext="4L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">4L</annotation></semantics></math> rounds later. During round <math alttext="4l+k" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m5" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">4l+k</annotation></semantics></math>, for <math alttext="0\leq l&lt;L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m6" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>l</mi><mo>&lt;</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">0\leq l&lt;L</annotation></semantics></math>, the system performs rules from the <math alttext="k" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.m7" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-th column of Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>, with each such round consisting of a communication step on edges and a local computation step on nodes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p3">
<p class="ltx_p">The state-space dynamics of BDH can be rewritten in vector-tensor form, equivalent to the local dynamics of the interaction kernel given in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>. This representation is given by Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>) in the following Section.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The BDH-Graph protocol for the interaction kernel, given for any time round <math alttext="T=4Lt+(4l+k)" class="ltx_Math" display="inline" id="Thmobservation1.p1.m1" intent=":literal"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><mo>+</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">T=4Lt+(4l+k)</annotation></semantics></math>, <math alttext="0\leq l&lt;L" class="ltx_Math" display="inline" id="Thmobservation1.p1.m2" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>l</mi><mo>&lt;</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">0\leq l&lt;L</annotation></semantics></math>, <math alttext="k=\{0,1,2,3\}" class="ltx_Math" display="inline" id="Thmobservation1.p1.m3" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">k=\{0,1,2,3\}</annotation></semantics></math> by the ruleset in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a> is equivalent to the state-space dynamics over time <math alttext="t" class="ltx_Math" display="inline" id="Thmobservation1.p1.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and layers <math alttext="l" class="ltx_Math" display="inline" id="Thmobservation1.p1.m5" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, given by Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>).</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p4">
<p class="ltx_p">For completeness, a detailed explanation of the equivalence is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS1" title="C.1 Proof of Observation 1 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">C.1</span></a>.
∎</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p5">
<p class="ltx_p">The variables <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math>, <math alttext="Y(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m2" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)</annotation></semantics></math>, <math alttext="A(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m3" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(i)</annotation></semantics></math>, defined for each of the <math alttext="n" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> nodes of the system, are updated in successive rounds. The state variables <math alttext="\sigma" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m5" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> defined on edges are assumed to be distinct over <math alttext="l" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m6" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> as <math alttext="\sigma_{l}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m7" intent=":literal"><semantics><msub><mi>σ</mi><mi>l</mi></msub><annotation encoding="application/x-tex">\sigma_{l}</annotation></semantics></math>, for <math alttext="0\leq l&lt;L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m8" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>l</mi><mo>&lt;</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">0\leq l&lt;L</annotation></semantics></math>; this distinction serves to facilitate interpretation and to strike a balance between the number of parameters and the size of state of the system (assuming a single state matrix <math alttext="\sigma" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m9" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>, uniform across <math alttext="l" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p5.m10" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, does not fundamentally change the operation and scaling laws of the architecture).</p>
</div>
<figure class="ltx_table" id="S2.T1">
<p class="ltx_p">(a) Simple equations of reasoning
<br class="ltx_break"/>
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" style="padding-bottom:2.84526pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l" class="ltx_Math" display="inline" id="S2.T1.m1" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">4l</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" style="padding-bottom:2.84526pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+1" class="ltx_Math" display="inline" id="S2.T1.m2" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">4l+1</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" style="padding-bottom:2.84526pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+2" class="ltx_Math" display="inline" id="S2.T1.m3" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4l+2</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" style="padding-bottom:2.84526pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+3" class="ltx_Math" display="inline" id="S2.T1.m4" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4l+3</annotation></semantics></math></span>
</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_l ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center"><em class="ltx_emph ltx_font_italic">Inference from state</em></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center"><em class="ltx_emph ltx_font_italic">Reweighting of synapse state</em></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p"><em class="ltx_emph ltx_font_italic"></em></span>
<span class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_italic">Neuron replicator dynamics +</span></span>
<br class="ltx_break"/>
<span class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_italic">inference from parameters</span></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center"><em class="ltx_emph ltx_font_italic">Inference from parameters</em></span>
</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx1">
<span id="S2.Ex2"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle X(i),\ \sigma_{l}(i,j)\xrightarrow{}A(j)" class="ltx_Math" display="inline" id="S2.Ex2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle X(i),\ \sigma_{l}(i,j)\xrightarrow{}A(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex3"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sigma_{l}(i,j)\downarrow_{1-u(i,j)}" class="ltx_Math" display="inline" id="S2.Ex3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><msub><mo stretchy="false">↓</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle\sigma_{l}(i,j)\downarrow_{1-u(i,j)}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx2">
<span id="S2.Ex4"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i),\ X(j)\xrightarrow{G_{s}(i,j)}\sigma_{l}(i,j)" class="ltx_Math" display="inline" id="S2.Ex4.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msub><mi>G</mi><mi>s</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Y(i),\ X(j)\xrightarrow{G_{s}(i,j)}\sigma_{l}(i,j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex5"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex5.m1" intent=":literal"><semantics><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle Y(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx3">
<span id="S2.Ex6"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle A(i),\ X(j)\xrightarrow{G_{y}^{\mathfrak{e}}(i,j)}Y(j)" class="ltx_Math" display="inline" id="S2.Ex6.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>y</mi><mi>𝔢</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle A(i),\ X(j)\xrightarrow{G_{y}^{\mathfrak{e}}(i,j)}Y(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex7"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle A(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex7.m1" intent=":literal"><semantics><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle A(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx4">
<span id="S2.Ex8"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{e}}(i,j)}X(j)" class="ltx_Math" display="inline" id="S2.Ex8.m1" intent=":literal"><semantics><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>x</mi><mi>𝔢</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{e}}(i,j)}X(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span></span>
</span>
</span>
<br class="ltx_break"/>
(b) Complete equations of reasoning of BDH
<br class="ltx_break"/>
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l" class="ltx_Math" display="inline" id="S2.T1.m5" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">4l</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+1" class="ltx_Math" display="inline" id="S2.T1.m6" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">4l+1</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+2" class="ltx_Math" display="inline" id="S2.T1.m7" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4l+2</annotation></semantics></math></span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_center">Round <math alttext="4l+3" class="ltx_Math" display="inline" id="S2.T1.m8" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4l+3</annotation></semantics></math></span>
</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-bottom:2.84526pt;">Communication</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_l ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx5">
<span id="S2.Ex9"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle X(i),\ \sigma_{l}(i,j)\xrightarrow{}A(j)" class="ltx_Math" display="inline" id="S2.Ex9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle X(i),\ \sigma_{l}(i,j)\xrightarrow{}A(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx6">
<span id="S2.Ex10"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i),\ X(j)\xrightarrow{G_{s}(i,j)}\sigma_{l}(i,j)" class="ltx_Math" display="inline" id="S2.Ex10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.667em">,</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msub><mi>G</mi><mi>s</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Y(i),\ X(j)\xrightarrow{G_{s}(i,j)}\sigma_{l}(i,j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx7">
<span id="S2.Ex11"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle A(i)\xrightarrow{G_{y}^{\mathfrak{e}}(i,j)}Y^{\mathfrak{e}}(j)" class="ltx_Math" display="inline" id="S2.Ex11.m1" intent=":literal"><semantics><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>y</mi><mi>𝔢</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msup><mi>Y</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle A(i)\xrightarrow{G_{y}^{\mathfrak{e}}(i,j)}Y^{\mathfrak{e}}(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex12"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle A(i)\xrightarrow{G_{y}^{\mathfrak{i}}(i,j)}Y^{\mathfrak{i}}(j)" class="ltx_Math" display="inline" id="S2.Ex12.m1" intent=":literal"><semantics><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>y</mi><mi>𝔦</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msup><mi>Y</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle A(i)\xrightarrow{G_{y}^{\mathfrak{i}}(i,j)}Y^{\mathfrak{i}}(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx8">
<span id="S2.Ex13"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{e}}(i,j)}X^{\mathfrak{e}}(j)" class="ltx_Math" display="inline" id="S2.Ex13.m1" intent=":literal"><semantics><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>x</mi><mi>𝔢</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msup><mi>X</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{e}}(i,j)}X^{\mathfrak{e}}(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex14"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{i}}(i,j)}X^{\mathfrak{i}}(j)" class="ltx_Math" display="inline" id="S2.Ex14.m1" intent=":literal"><semantics><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mrow><msubsup><mi>G</mi><mi>x</mi><mi>𝔦</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mover><mrow><msup><mi>X</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Y(i)\xrightarrow{G_{x}^{\mathfrak{i}}(i,j)}X^{\mathfrak{i}}(j)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_colspan ltx_colspan_4" style="padding-bottom:-8.53581pt;"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_colspan ltx_colspan_4" style="padding-bottom:2.84526pt;">Computation</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_l ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx9">
<span id="S2.Ex15"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sigma_{l}(i,j)\downarrow_{1-u(i,j)}" class="ltx_Math" display="inline" id="S2.Ex15.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><msub><mo stretchy="false">↓</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle\sigma_{l}(i,j)\downarrow_{1-u(i,j)}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex16"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle X^{\mathfrak{e}}(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex16.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>X</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle X^{\mathfrak{e}}(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex17"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle X^{\mathfrak{i}}(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex17.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>X</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle X^{\mathfrak{i}}(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx10">
<span id="S2.Ex18"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex18.m1" intent=":literal"><semantics><mrow><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle Y(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex19"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y^{\mathfrak{e}}(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex19.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>Y</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle Y^{\mathfrak{e}}(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex20"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle Y^{\mathfrak{i}}(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex20.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>Y</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle Y^{\mathfrak{i}}(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx11">
<span id="S2.Ex21"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle(Y^{\mathfrak{e}}(i)-Y^{\mathfrak{i}}(i))^{+},\ X(i)\xrightarrow{}Y(i)\hskip-1.72668pt" class="ltx_Math" display="inline" id="S2.Ex21.m1" intent=":literal"><semantics><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>Y</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>Y</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>+</mo></msup><mo rspace="0.667em">,</mo><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle(Y^{\mathfrak{e}}(i)-Y^{\mathfrak{i}}(i))^{+},\ X(i)\xrightarrow{}Y(i)\hskip-1.72668pt</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
<span id="S2.Ex22"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle A(i)\downarrow" class="ltx_Math" display="inline" id="S2.Ex22.m1" intent=":literal"><semantics><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">↓</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle A(i)\downarrow</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span>
<span class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx12">
<span id="S2.Ex23"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle(X^{\mathfrak{e}}(i)-X^{\mathfrak{i}}(i))^{+}\xrightarrow{}X(i)" class="ltx_Math" display="inline" id="S2.Ex23.m1" intent=":literal"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>X</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>X</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>+</mo></msup><mover accent="true"><mo stretchy="false">→</mo><mi></mi></mover><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle(X^{\mathfrak{e}}(i)-X^{\mathfrak{i}}(i))^{+}\xrightarrow{}X(i)</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span></span></span>
</span>
</span></span></span>
</span>
</span>
<br class="ltx_break"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The “equations of reasoning”: State-space dynamics of the BDH language model expressed through local graph dynamics with the edge reweighting kernel (Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition2" title="Definition 2 (edge-reweighting kernel). ‣ Definition of the edge-reweighting kernel. ‣ 2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>). The rules are executed for a distributed system of <math alttext="n" class="ltx_Math" display="inline" id="S2.T1.m26" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons performing steps of parallel computation and communication during inference. Model parameters are expressed through the weights of edges of graphs <math alttext="{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{s}}" class="ltx_Math" display="inline" id="S2.T1.m27" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>,</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{y}}^{\mathfrak{e}},{G_{x}}^{\mathfrak{i}},{G_{s}}</annotation></semantics></math>, and BDH model training is equivalent to defining rule probability amplitudes <math alttext="{G_{x}}^{\mathfrak{e}}(i,j),{G_{x}}^{\mathfrak{i}}(i,j),{G_{y}}^{\mathfrak{e}}(i,j),{G_{y}}^{\mathfrak{i}}(i,j),{G_{s}}(i,j)\geq 0" class="ltx_Math" display="inline" id="S2.T1.m28" intent=":literal"><semantics><mrow><mrow><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>G</mi><mi>s</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}}(i,j),{G_{x}}^{\mathfrak{i}}(i,j),{G_{y}}^{\mathfrak{e}}(i,j),{G_{y}}^{\mathfrak{i}}(i,j),{G_{s}}(i,j)\geq 0</annotation></semantics></math> for pairs of neurons <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S2.T1.m29" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math> connected by the edges of these graphs. State is encoded in variables <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S2.T1.m30" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math> at synapses, representing edges of graph <math alttext="G_{s}" class="ltx_Math" display="inline" id="S2.T1.m31" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">G_{s}</annotation></semantics></math>. The system proceeds in parallel rounds, with new tokens arriving into the system encoded through variables <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.T1.m32" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> at neurons and introduced every <math alttext="4L" class="ltx_Math" display="inline" id="S2.T1.m33" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">4L</annotation></semantics></math> rounds, where <math alttext="L" class="ltx_Math" display="inline" id="S2.T1.m34" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is a parameter of the model (e.g., <math alttext="L=8" class="ltx_Math" display="inline" id="S2.T1.m35" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">L=8</annotation></semantics></math>). The set of rules being executed (for each round modulo <math alttext="4L" class="ltx_Math" display="inline" id="S2.T1.m36" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">4L</annotation></semantics></math>) is given in the table. The readout of the system also happens through variables <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.T1.m37" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> at the end of each <math alttext="4L" class="ltx_Math" display="inline" id="S2.T1.m38" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">4L</annotation></semantics></math> rounds.
(a) Set of rules for the simplified version of the BDH model with no neuron inhibitory circuits and no thresholding (<math alttext="{G_{x}}^{\mathfrak{i}}={G_{y}}^{\mathfrak{i}}=0" class="ltx_Math" display="inline" id="S2.T1.m39" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>=</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{i}}={G_{y}}^{\mathfrak{i}}=0</annotation></semantics></math>), capturing the general form of the communication structure and synaptic attention of the model. (b) Set of rules for the general case of BDH, including inhibitory circuits <math alttext="{G_{x}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S2.T1.m40" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{i}}</annotation></semantics></math>, <math alttext="{G_{y}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S2.T1.m41" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><annotation encoding="application/x-tex">{G_{y}}^{\mathfrak{i}}</annotation></semantics></math>. An execution of the provided rules is equivalent to the state-space dynamics given by Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>).
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p6">
<p class="ltx_p">In the representation in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a> we do not impose how the local thresholding operation within some neuron <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p6.m1" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, of the form <math alttext="A(i)\ ,B(i)\dashrightarrow\left(A(i)-B(i)\right)^{+}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p6.m2" intent=":literal"><semantics><mrow><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo rspace="0.500em" stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">⇢</mo><msup><mrow><mo>(</mo><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">A(i)\ ,B(i)\dashrightarrow\left(A(i)-B(i)\right)^{+}</annotation></semantics></math>, should be performed. We leave this as a computational primitive, which can be realized based on approximate counting or a comparator. The way natural neurons achieve thresholding to determine whether input signal excitation outweighs inhibition relies on time-integration of impulses. For realizations in other types of distributed systems and population protocols, we refer the reader to the literature on thresholding and Majority Protocols, cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Doty et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib29" title="">2021</a>; Czyzowicz et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib24" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p7">
<p class="ltx_p">The definition of the protocol does not specify how variable <math alttext="X(i)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p7.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math> should be reset when the scheduler passes from layer <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p7.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> of one input token to layer <math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p7.m3" intent=":literal"><mn>0</mn></math> for the next input token. As with the definition of state-space equations in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>, we leave this open to allow the dynamics to work both with externally provided input (for next-token prediction), or in a self-feedback loop (for autoregressive operation).</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Notes on training.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p">Direct training of the BDH model would be performed by selecting the edges of the considered graphs, and then setting rule weights <math alttext="{G_{x}}^{\mathfrak{e}}(i,j),{G_{x}}^{\mathfrak{i}}(i,j),{G_{y}}^{\mathfrak{e}}(i,j),{G_{y}}^{\mathfrak{i}}(i,j),{G_{s}}(i,j)\geq 0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>G</mi><mi>s</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}}(i,j),{G_{x}}^{\mathfrak{i}}(i,j),{G_{y}}^{\mathfrak{e}}(i,j),{G_{y}}^{\mathfrak{i}}(i,j),{G_{s}}(i,j)\geq 0</annotation></semantics></math> for pairs of neurons <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math> connected by the edges of these graphs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p">In what follows, we will train a tensor-friendly special case of BDH, called BDH-GPU, relying on an implicit (generally more efficient) representation of the considered graph parameter weights, using a low-rank product representation for the matrices of these graphs. This representation is reminiscent of the hub-labeling graph representation technique, but is directly suitable for describing and evolving high-conductance scale-free networks. The appropriate architecture is introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3" title="3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Interpretation of attention as a micro-inductive bias of reasoning</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p">Rule weights in the edge-reweighting kernel have the interpretation of micro-programs, governed by rules of transformation of state variables of the form <math alttext="A(i),B(j)\to\sigma(i,j)" class="ltx_Math" display="inline" id="S2.SS3.p1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>B</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">A(i),B(j)\to\sigma(i,j)</annotation></semantics></math> and <math alttext="A(i),\sigma(i,j)\to C(j)" class="ltx_Math" display="inline" id="S2.SS3.p1.m2" intent=":literal"><semantics><mrow><mrow><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">A(i),\sigma(i,j)\to C(j)</annotation></semantics></math>, defined on edges between nodes <math alttext="i,j" class="ltx_Math" display="inline" id="S2.SS3.p1.m3" intent=":literal"><semantics><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math> of some <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-node graph.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p">This formalism can be seen as running an enormous circuit with a form of universal gates given by the transition rules, over a structure of computational elements at nodes, and memory elements on edges of a graph.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p">While the local rulesets have the form of a rule-based micro-assembly, we leave open the extent to which they should be considered to have an interpretation of programming in logic (as would be the case, e.g., for C-RASP <cite class="ltx_cite ltx_citemacro_citep">(Yang and Chiang, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib110" title="">2024</a>)</cite>). The natural interpretation of <math alttext="\sigma(i,j)&gt;0" class="ltx_Math" display="inline" id="S2.SS3.p3.m1" intent=":literal"><semantics><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma(i,j)&gt;0</annotation></semantics></math> is a positive bias associated with the neuron pair <math alttext="(i,j)" class="ltx_Math" display="inline" id="S2.SS3.p3.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>, <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S2.SS3.p3.m3" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math>, which follows from past context. This can be considered by phrasing the local rules of the system in a framework of logic inference; we do so informally, omitting discussion of layers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<blockquote class="ltx_quote">
<p class="ltx_p">If past context <math alttext="(x_{\tau}:\tau&lt;t)" class="ltx_Math" display="inline" id="S2.SS3.p4.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mi>τ</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_{\tau}:\tau&lt;t)</annotation></semantics></math> implies that implication <math alttext="i\to j" class="ltx_Math" display="inline" id="S2.SS3.p4.m2" intent=":literal"><semantics><mrow><mi>i</mi><mo stretchy="false">→</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i\to j</annotation></semantics></math> has weight <math alttext="\sigma_{t-1}(i,j)" class="ltx_Math" display="inline" id="S2.SS3.p4.m3" intent=":literal"><semantics><mrow><msub><mi>σ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma_{t-1}(i,j)</annotation></semantics></math>, and if the current state at time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS3.p4.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> implies that <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.p4.m5" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> follows from this state with weight <math alttext="x_{t}(i)" class="ltx_Math" display="inline" id="S2.SS3.p4.m6" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{t}(i)</annotation></semantics></math>, then the current state at time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS3.p4.m7" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> implies that <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.p4.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> follows from this state with weight <math alttext="x_{t}(i)\sigma_{t-1}(i,j)" class="ltx_Math" display="inline" id="S2.SS3.p4.m9" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>σ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{t}(i)\sigma_{t-1}(i,j)</annotation></semantics></math>.</p>
</blockquote>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p">The above is intentionally phrased to resemble the logical axiom <math alttext="(X\to(i\to j))\to((X\to i)\to(X\to j))" class="ltx_Math" display="inline" id="S2.SS3.p5.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo stretchy="false">→</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo stretchy="false">→</mo><mi>j</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo stretchy="false">→</mo><mi>i</mi></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">(</mo><mrow><mi>X</mi><mo stretchy="false">→</mo><mi>j</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(X\to(i\to j))\to((X\to i)\to(X\to j))</annotation></semantics></math>, which is perhaps most prevalent across different formalizations of axiomatic logic, with an application of <span class="ltx_text ltx_font_slanted">modus ponens</span> as an inference rule. The inference system of the considered model uses state and model weights to devise its own heuristic for the order of evaluation, i.e., to consider which facts appear to be most plausible to be evaluated next, and to evaluate them in an order based on what follows most strongly from context. In a way consistent with what we expect from informal reasoning in language, the considered weights have a more direct interpretation of an increment of utility associated with a given inference.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Here, the term <em class="ltx_emph ltx_font_italic">utility</em> is understood in the sense of evolutionary game theory, as applied to the population of neurons, considering the standard interpretation of replicator dynamics, as applied in the ruleset from Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>. Neurons which win in the natural selection process are added to the activation <math alttext="Y" class="ltx_Math" display="inline" id="footnote7.m1" intent=":literal"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</span></span></span> In the setting of argumentation, this utility-based approach could, for example, guide the inference process from a pair of known concepts in context, a source and a target, to an intermediate concept likely to be a common-neighbor shortcut lying on a logical path between this source and target (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a> for a discussion of how this type of mechanism is enforced in the feed-forward network of BDH-GPU).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p6">
<p class="ltx_p">The considered micro-foundational interpretation of attention, defined at the level of individual neurons (or logical variables), does not contradict the way in which Transformer attention is often regarded at the coarser level of vectors through key-query lookup intuitions. At the same time, it highlights that an attention state entry <math alttext="\sigma(i,j)" class="ltx_Math" display="inline" id="S2.SS3.p6.m1" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(i,j)</annotation></semantics></math> (and similarly, a model edge weight leading from <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.p6.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> to <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.p6.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>) does not have the interpretation of a logical value (i.e., something that is true or false), but an inductive bias associated with how likely the system is to consider the implication ‘<math alttext="i\to j" class="ltx_Math" display="inline" id="S2.SS3.p6.m4" intent=":literal"><semantics><mrow><mi>i</mi><mo stretchy="false">→</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i\to j</annotation></semantics></math>’ in its next steps of reasoning, when proposing its next conclusions or next ideas for consideration.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p7">
<p class="ltx_p">Chains of implications in BDH guide activations along paths in the system graphs <math alttext="{G_{x}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{e}},{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S2.SS3.p7.m1" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mi>𝝈</mi></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}},{G_{y}}^{\mathfrak{e}},{\boldsymbol{\sigma}}</annotation></semantics></math>. For the latter, attention allows specific implications to enter into paths of thought once the corresponding synapses are open in state <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S2.SS3.p7.m2" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Interpretation of BDH as an oscillator network toy-model</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p">Whereas the interpretation from Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS3" title="2.3 Interpretation of attention as a micro-inductive bias of reasoning ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.3</span></a> focuses on properties which fallow from the computational function (purpose) of the system, here we outline an interpretation of the behavior of BDH considered purely as a dynamical system.</p>
</div>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Definition of the toy-model.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p">We will consider the toy-model of an <math alttext="n" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-particle system shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.F2" title="Figure 2 ‣ Definition of the toy-model. ‣ 2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a> as an illustration of the general form of dynamics of the state-space equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>) of BDH. We draw the <math alttext="n" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> particles in a circle.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This is a direct tribute to the Kuromato coupled oscillators model; the crucial difference being that in BDH, the elements of state with an interpretation similar to oscillators appear on connections between nodes, not nodes.</span></span></span></p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:138.0pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="284" id="S2.F2.g1" src="x2.png" width="264"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle" style="width:196.7pt;">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><em class="ltx_emph ltx_font_italic">Symbol</em></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><em class="ltx_emph ltx_font_italic">Interpretation in:</em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>, State Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>)</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Oscillator Network Toy-Model</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><math alttext="{G_{x}},{G_{y}},{G_{s}}" class="ltx_Math" display="inline" id="S2.F2.m1" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>x</mi></msub><mo>,</mo><msub><mi>G</mi><mi>y</mi></msub><mo>,</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}},{G_{y}},{G_{s}}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">graph parameters of model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">wires, prods, and elastic connections</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><math alttext="\sigma" class="ltx_Math" display="inline" id="S2.F2.m2" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">synaptic state of model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">displacement of elastic connections</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><math alttext="x,y" class="ltx_Math" display="inline" id="S2.F2.m3" intent=":literal"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x,y</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">activation vectors</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">pulses at nodes, state correction</span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The ‘physical system’ representation of BDH as a physical graph toy-model.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p2">
<p class="ltx_p">The particles are connected with each other by state elements, represented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.F2" title="Figure 2 ‣ Definition of the toy-model. ‣ 2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a> as elastic connectors. The topology of these pairwise connections is given by graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p2.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>, and may in general be dense.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p3">
<p class="ltx_p">The signal displays dynamics of state <math alttext="{\boldsymbol{\rho}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m1" intent=":literal"><semantics><mi>𝝆</mi><annotation encoding="application/x-tex">{\boldsymbol{\rho}}</annotation></semantics></math> through tension on connectors, which evolves at a slower time scale, and a more pulse-like activation dynamics <math alttext="x,y" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p3.m2" intent=":literal"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x,y</annotation></semantics></math> (on nodes), appearing and vanishing regularly, at a rapid time scale.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p4">
<p class="ltx_p">The slower state dynamics represent, in the first order, oscillation or relaxation of the system of elastic connectors. Once an elastic connector between particles <math alttext="i" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m1" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m2" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> has had its endpoints displaced through state <math alttext="x" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m3" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m4" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>, respectively, a tension appears on this connector, which causes its displacement <math alttext="{\boldsymbol{\sigma}}(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m5" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,j)</annotation></semantics></math> that relaxes over time (damping variant, corresponding to ALiBi), and/or acts as a spring element (oscillator variant, a simplified illustration of RoPE). Initially, <math alttext="{\boldsymbol{\sigma}}(i,j)=0" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p4.m6" intent=":literal"><semantics><mrow><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,j)=0</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p5">
<p class="ltx_p">The faster dynamics represent the node dynamics of particles. Over time, pulse displacements <math alttext="x(i)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m1" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i)</annotation></semantics></math> happen at nodes, as a result of either previous behavior of the system, or perturbation by an external forcing field (in reality this field would be language input). A node <math alttext="i" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> with displacement <math alttext="x(i)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m3" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i)</annotation></semantics></math> may, due to the aggregated action of tension of <em class="ltx_emph ltx_font_italic">elastic connectors</em> <math alttext="{\boldsymbol{\sigma}}(i,\cdot)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m4" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,\cdot)</annotation></semantics></math> adjacent to it, activate a system of <em class="ltx_emph ltx_font_italic">prods</em> <math alttext="{G_{y}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m5" intent=":literal"><semantics><msub><mi>G</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{G_{y}}</annotation></semantics></math> adjacent to it, perturbing nodes it hits in this way. If another node <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m6" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> is prodded sufficiently hard, it may cause it to activate a perturbation <math alttext="y(j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m7" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(j)</annotation></semantics></math>. The perturbation <math alttext="y(j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m8" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(j)</annotation></semantics></math> of a node <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m9" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> will, in the next step, propagate again to those other nodes <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m10" intent=":literal"><semantics><msup><mi>i</mi><mo>′</mo></msup><annotation encoding="application/x-tex">i^{\prime}</annotation></semantics></math>, which are connected to <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m11" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> by a system of <em class="ltx_emph ltx_font_italic">wires</em> (<math alttext="{G_{x}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m12" intent=":literal"><semantics><msub><mi>G</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{G_{x}}</annotation></semantics></math>). If the aggregated pull of wires on a node <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m13" intent=":literal"><semantics><msup><mi>i</mi><mo>′</mo></msup><annotation encoding="application/x-tex">i^{\prime}</annotation></semantics></math> is sufficiently strong, this modifies its pulse displacement <math alttext="x(i^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m14" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i^{\prime})</annotation></semantics></math>. The pulse activation <math alttext="y(j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m15" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(j^{\prime})</annotation></semantics></math> of some node <math alttext="j^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m16" intent=":literal"><semantics><msup><mi>j</mi><mo>′</mo></msup><annotation encoding="application/x-tex">j^{\prime}</annotation></semantics></math>, directly followed by pulse activation <math alttext="x(i^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m17" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i^{\prime})</annotation></semantics></math> of node <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m18" intent=":literal"><semantics><msup><mi>i</mi><mo>′</mo></msup><annotation encoding="application/x-tex">i^{\prime}</annotation></semantics></math>, results in an increase in the tension on the connector <math alttext="(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m19" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>, adding to the value of the tension <math alttext="{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m20" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo>,</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})</annotation></semantics></math>. All pulse activations <math alttext="y" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m21" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> subside, and the pulses propagate, consequently altering the slow state <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p5.m22" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p6">
<p class="ltx_p">In general, <math alttext="{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m1" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo>,</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})</annotation></semantics></math> is triggered simply by the temporal connection between the pulse <math alttext="y(j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m2" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(j^{\prime})</annotation></semantics></math> activating, followed by the pulse <math alttext="x(i^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m3" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i^{\prime})</annotation></semantics></math> activating immediately afterwards, even if there was no direct causality between the two (although <math alttext="y(j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m4" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(j^{\prime})</annotation></semantics></math> contributed to pulse <math alttext="x(i^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m5" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(i^{\prime})</annotation></semantics></math> happening if <math alttext="(j^{\prime},i^{\prime})\in{G_{x}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>j</mi><mo>′</mo></msup><mo>,</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo>∈</mo><msub><mi>G</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">(j^{\prime},i^{\prime})\in{G_{x}}</annotation></semantics></math>). An appropriate correspondence of the graphs, <math alttext="{G_{s}}\subseteq{G_{x}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p6.m7" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>s</mi></msub><mo>⊆</mo><msub><mi>G</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">{G_{s}}\subseteq{G_{x}}</annotation></semantics></math>, would bring the system close to an observed causal effect on the activated synapse.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p7">
<p class="ltx_p">The above description of the pulse dynamics was given from the perspective of nodes. From the perspective of connectors, an existing tension on some connector <math alttext="{\boldsymbol{\sigma}}(i,k)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m1" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,k)</annotation></semantics></math> propagates through prods <math alttext="{G_{y}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m2" intent=":literal"><semantics><msub><mi>G</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{G_{y}}</annotation></semantics></math> to some nodes <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, then through wires <math alttext="{G_{x}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m4" intent=":literal"><semantics><msub><mi>G</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{G_{x}}</annotation></semantics></math> to some nodes <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m5" intent=":literal"><semantics><msup><mi>i</mi><mo>′</mo></msup><annotation encoding="application/x-tex">i^{\prime}</annotation></semantics></math>, and this finally contributes to tensions on other connectors <math alttext="{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m6" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo>,</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i^{\prime},j^{\prime})</annotation></semantics></math>. This propagation of state thus happens to 3-hop neighbors, through <math alttext="i" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m7" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, <math alttext="j" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p7.m9" intent=":literal"><semantics><msup><mi>i</mi><mo>′</mo></msup><annotation encoding="application/x-tex">i^{\prime}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px1.p8">
<p class="ltx_p">During training, the behavior of the system may, in even longer time scales, result in the propagation of changes of connection weight and structures to graphs <math alttext="{G_{x}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p8.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{G_{x}}</annotation></semantics></math> and <math alttext="{G_{y}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p8.m2" intent=":literal"><semantics><msub><mi>G</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{G_{y}}</annotation></semantics></math>, as well as (optionally) <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p8.m3" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Effects captured by the toy-model.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p">We have described a small local graph kernel, with 3-hop locality, capturing the two key effects of the local graph kernel.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p2">
<p class="ltx_p">The first effect is the graph form of communication pattern between nodes, and thresholding of updates. (We have omitted direct mention of inhibition from discussion of the toy-model, but it is direct to include.)</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p3">
<p class="ltx_p">The second effect is the placement of attention state on node connections, its update patterns, and the dynamics of its relaxation over time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p4">
<p class="ltx_p">We intentionally convey the interpretation of node pulses as a differential (gradient) of state on node connections. This interpretation is consistent with our empirical study from Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7" title="7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a>. It is worth considering once every how many steps of the operation of the toy-model, a single element of state <math alttext="{\boldsymbol{\sigma}}(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m1" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,j)</annotation></semantics></math> is updated. This depends directly on the sparsity of the pulse signals <math alttext="y(i)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m2" intent=":literal"><semantics><mrow><mi>y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">y(i)</annotation></semantics></math>, <math alttext="x(j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m3" intent=":literal"><semantics><mrow><mi>x</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(j)</annotation></semantics></math>; at least one of them is, generally, sparse. If the pulses where to happen very seldom for such a pair <math alttext="(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>, state updates are essentially a “second-order” correction effect. By adjusting the frequency of updates, the system can be made to operate exactly at the critical point where this pulse dynamics ceases to be a second-order correction of state <math alttext="{\boldsymbol{\sigma}}(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m5" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,j)</annotation></semantics></math>, giving the random variable describing the time between updates of a connection pair <math alttext="{\boldsymbol{\sigma}}(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m6" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}(i,j)</annotation></semantics></math> a heavy power-law-like tail distribution (possibly with different distribution parameters for different pairs <math alttext="(i,j)" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p4.m7" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p5">
<p class="ltx_p">In the description of state dynamics, we noted the hop-distance of 3 in the forward propagation of changes to state. Bearing this in mind is helpful when considering how a gradient backpropagation mechanism would follow dependencies between changes of state if such a system were to have its graph weights altered through backpropagation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p6">
<p class="ltx_p">Finally, let us clarify the specific choice of kernel we made for BDH. We found it to work well, and we knew how to train BDH models which implement it on GPU (which we will call BDH-GPU). This, with current hardware, made it <math alttext="10^{2}-10^{5}" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px2.p6.m1" intent=":literal"><semantics><mrow><msup><mn>10</mn><mn>2</mn></msup><mo>−</mo><msup><mn>10</mn><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">10^{2}-10^{5}</annotation></semantics></math> times more cost- and time-effective to train models and analyze outcomes than kernels, for which we only knew how to train on CPU. Nonetheless, the question of finding optimal kernels according to different criteria (e.g.: minimality of kernel, best training rate per token, closeness to brain function based on known evidence from brain studies), is an extremely pertinent foundational problem. The problem can be phrased in a “closed-ended” way, leaving a finite number of possibilities to be checked, at least when considering small graph kernels. Some kernels may also prove to have superior learning capabilities to the Transformer (and BDH), and if this quality difference is overwhelming, they may eventually prove commercially viable.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS0.Px2.p7">
<p class="ltx_p">In the following, we formalize the choice of kernel for BDH, and also provide a framework to describe other kernels capturing the same effects of graph communication and synaptic attention.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Expressing BDH using brain models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p">The results we obtain for BDH provide direct corollaries on the expressiveness of brain models which are capable of emulating the local graph kernels of BDH. Specifically, a distributed system, which is able to efficiently emulate the local kernels of BDH, has sufficient expressiveness to perform language inference and reasoning at least to the same extent as BDH.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The local ruleset of BDH (Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>) can be expressed through a combination of simple mechanisms: neuron activation with positive state variables, Hebbian learning, and communication through excitatory and inhibitory circuits with thresholding.
∎</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<p class="ltx_p">We note that in the description of the rulesets in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>, Round (<math alttext="4l+2" class="ltx_Math" display="inline" id="S2.SS5.p2.m1" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4l+2</annotation></semantics></math>) and (<math alttext="4l+3" class="ltx_Math" display="inline" id="S2.SS5.p2.m2" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">4l+3</annotation></semantics></math>) directly describe the use of excitatory and inhibitory circuits with integrate-and-fire thresholding at neurons. Round (<math alttext="4l+2" class="ltx_Math" display="inline" id="S2.SS5.p2.m3" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">4l+2</annotation></semantics></math>) additionally includes a form of competition effect between neurons, realized fully locally at a neurons using the multiplication effect of replicator dynamics. The communication rule of Round (<math alttext="4l+1" class="ltx_Math" display="inline" id="S2.SS5.p2.m4" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">4l+1</annotation></semantics></math>) involves the potentiation of a synapse based on activations of neurons at its endpoints. As was discussed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS1" title="2.1 Formalism for local graph-based language models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.1</span></a>, the natural mechanism for implementing increase in synaptic strength is through spiking dynamics, where the execution of the communication rule of Round (<math alttext="4l+1" class="ltx_Math" display="inline" id="S2.SS5.p2.m5" intent=":literal"><semantics><mrow><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">4l+1</annotation></semantics></math>) is a stochastic AND-gate on signals. Finally, Round (<math alttext="4l" class="ltx_Math" display="inline" id="S2.SS5.p2.m6" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow><annotation encoding="application/x-tex">4l</annotation></semantics></math>) describes the long-term effects of using a strengthened synapse for transmission of signals, and its strength decrease.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p3">
<p class="ltx_p">We can use the framework of expressiveness, as captured in Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation2" title="Observation 2. ‣ 2.5 Expressing BDH using brain models ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>, to shed light on the capabilities of natural systems through their ability to emulate artificial ones. Specifically, if a natural system A can plausibly emulate some artificial system B by using the resources it has at its disposal, and artificial system B is able to solve a problem P, this can be used to explain: (1) why the natural system A is sufficiently powerful to solve problem P, and (2) plausibly, that the purpose for which system A is equipped with certain mechanisms includes solving problem P, if such mechanisms prove useful in the emulation of B.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p4">
<p class="ltx_p">The experimental validation of the performance of BDH architecture at Transformer level (Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2" title="4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4.2</span></a>) confirms that BDH is sufficient to provide language and reasoning function at scale. We can thus make the following statement.</p>
</div>
<div class="ltx_theorem ltx_theorem_finding" id="Thmfinding2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Empirical Finding 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmfinding2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The <em class="ltx_emph ltx_font_upright">Hebbian learning mechanism</em> is plausibly needed, and in combination with neural circuits, sufficient, for performing the <em class="ltx_emph ltx_font_upright">reasoning</em> function at the scale of the brain. This includes performing language function with attention, and performing thought processes, at a time scale of minutes.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p5">
<p class="ltx_p">In view of our results, Hebbian learning can be seen as a form of unsupervised learning over time, expressed through graph edge reweighting, to perform reasoning and language inference using the attention mechanism. This type of result can be compared to an analogous interpretation for Hebbian learning in the context of vision, as pioneered in <cite class="ltx_cite ltx_citemacro_citep">(Brunel, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib16" title="">1996</a>)</cite>. With the setting of language and chain-of-thought reasoning, we are able to directly capture effects of time in the brain.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p6">
<p class="ltx_p">Given the interpretation of neuron activations as carrying the necessary gradients of synaptic state (Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4" title="2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.4</span></a>), the problem of supervised learning (i.e., taking into account feedback signals) plausibly becomes deferred to a selective transfer and re-encoding of gradients from state into weights, at longer time scales. We return to a discussion of this point in the Conclusions, bearing in mind the fact that the general difficulty of the problem is now reduced through restrictions on the considered edge-reweighting kernel, and the relative rarity of synapse activation events.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p7">
<p class="ltx_p">Our work also suggests a framework for further discussion of reasoning function, with an anchoring point for this type of investigation in the time-scale of ‘split-seconds’ to ‘minutes’. The question of shorter time scales is then one of designing more precise communication and computational primitives for spiking neurons and synaptic plasticity, which can be used to perform primitives for individual rules of graph kernels for the inference dynamics.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>While we do not provide direct explanations for effects at shorter time scales and scheduler primitives, we note the type of kernels we rely on are well understood in terms of the ability to work with asynchronous schedulers, and obtaining emergence of synchronization. <cite class="ltx_cite ltx_citemacro_citep">(Kosowski and Uznański, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib58" title="">2018</a>; Dudek and Kosowski, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib30" title="">2018</a>)</cite></span></span></span> The question of longer time scales, and the changes to model structure that follow in a learning process, naturally follows any explanation of unsupervised (Hebbian) learning from the shorter time scale that is considered here, as a mechanism of transfer from state to weights; we come back to this point in the Conclusions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>BDH-GPU: a tensor-friendly version of the BDH architecture</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p">We will now introduce BDH-GPU, a variant of the BDH reasoning system, expressed in the language of tensor operations typical for Deep Learning models. BDH-GPU provides a GPU-compatible implementation of BDH. BDH-GPU can be easily implemented in PyTorch, a didactic code listing is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>). Furthermore, BDH-GPU can be trained on large text datasets using error backpropagation, and has been shown experimentally to match performance of GPT-based LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p">The main steps towards the efficient implementation of BDH-GPU on GPU are:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">Express graphs <math alttext="G_{x}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>x</mi></msub><annotation encoding="application/x-tex">G_{x}</annotation></semantics></math> and <math alttext="G_{y}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.m2" intent=":literal"><semantics><msub><mi>G</mi><mi>y</mi></msub><annotation encoding="application/x-tex">G_{y}</annotation></semantics></math> a low-rank factorizations of their transition matrices, followed by ReLU nonlinearities <cite class="ltx_cite ltx_citemacro_citep">(Nair and Hinton, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib76" title="">2010</a>)</cite> (we explore graph properties of these approximations in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5" title="5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a>). We never materialize these matrices, but maintain instead a low dimensional state per each neuron.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">Never materialize the <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m1" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> state matrix, preferring instead to access it using a linear attention operation over low-rank representation of values (we explore the properties of this attention mechanism in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6" title="6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p">Normalize all state variables using LayerNorm <cite class="ltx_cite ltx_citemacro_citep">(Ba et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib6" title="">2016b</a>)</cite>.</p>
</div>
</li>
</ol>
<p class="ltx_p">We will refer to the architecture in the final intermediate step, before the introduction of LayerNorm, as BDH-Normfree.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Notation for BDH-GPU</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">We consider the <math alttext="\textrm{BDH-GPU}(n,d)" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><mrow><mtext>BDH-GPU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{BDH-GPU}(n,d)</annotation></semantics></math> architecture parameterized by positive integers <math alttext="n,d" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n,d</annotation></semantics></math>. The system scales in dimension <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> — the number of particles. In what follows, we will use the terms <em class="ltx_emph ltx_font_italic">particle</em> and <em class="ltx_emph ltx_font_italic">neuron</em> interchangeably. Dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is a measure of the number of parameters per neuron required to represent the interaction of this neuron with the particle interaction field or interaction graph. For asymptotic analysis, we assume that <math alttext="n\to+\infty" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mrow><mi>n</mi><mo stretchy="false">→</mo><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">n\to+\infty</annotation></semantics></math> is the basis for all asymptotics, and <math alttext="n\gg d&gt;C\log n" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><mrow><mi>n</mi><mo>≫</mo><mi>d</mi><mo>&gt;</mo><mrow><mi>C</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">n\gg d&gt;C\log n</annotation></semantics></math> holds for some sufficiently large constant <math alttext="C&gt;0" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><mrow><mi>C</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">C&gt;0</annotation></semantics></math>. For the tensor representation of the model, which is the primary one for implementation and empirical studies here in this paper, vectors in <math alttext="R^{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><msup><mi>R</mi><mi>d</mi></msup><annotation encoding="application/x-tex">R^{d}</annotation></semantics></math> have an interpretation as (fuzzy) addresses of a virtual memory space of size <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.m9" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, hence the assumption <math alttext="d=\Omega(\log n)" class="ltx_Math" display="inline" id="S3.SS1.p1.m10" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">d=\Omega(\log n)</annotation></semantics></math> cannot be dispensed with while using natural (linear-algebraic) arithmetic on real numbers. We later show how to avoid this assumption in graph-based models, by using uniform local graph kernels of smaller degree with a graph communication structure.</p>
</div>
<section class="ltx_subparagraph" id="S3.SS1.SSS0.P0.SPx1">
<h6 class="ltx_title ltx_title_subparagraph">Nonlinearities: ReLU and LayerNorm.</h6>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.P0.SPx1.p1">
<p class="ltx_p">In what follows, we assume that a one-dimensional vector is denoted by a lower-case letter, e.g., <math alttext="z" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p1.m1" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>, with <math alttext="z\in R^{n\times 1}\cong R^{n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p1.m2" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup><mo>≅</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in R^{n\times 1}\cong R^{n}</annotation></semantics></math> unless otherwise stated. Vectors which appear in dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p1.m3" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> are named with an asterisk, e.g., as <math alttext="z^{*}\in R^{d\times 1}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p1.m4" intent=":literal"><semantics><mrow><msup><mi>z</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z^{*}\in R^{d\times 1}</annotation></semantics></math>. We denote the <em class="ltx_emph ltx_font_italic">ReLU operation</em> <math alttext="\left(z\right)^{+}:=\max_{i\in{1,\ldots,n}}\{0,z_{i}\}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p1.m5" intent=":literal"><semantics><mrow><msup><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>+</mo></msup><mo>:=</mo><mrow><msub><mi>max</mi><mrow><mi>i</mi><mo>∈</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\left(z\right)^{+}:=\max_{i\in{1,\ldots,n}}\{0,z_{i}\}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.P0.SPx1.p2">
<p class="ltx_p">We further define <em class="ltx_emph ltx_font_italic">LayerNorm</em> of a vector <math alttext="z^{*}\in R^{d\times 1}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p2.m1" intent=":literal"><semantics><mrow><msup><mi>z</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z^{*}\in R^{d\times 1}</annotation></semantics></math> in a uniform non-parametric way, <math alttext="\mathsf{LN}\left(z^{*}\right)=\frac{z^{*}-\mathbf{1}\mathbb{E}_{d}z^{*}}{\sigma_{d}z^{*}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p2.m2" intent=":literal"><semantics><mrow><mrow><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>z</mi><mo>∗</mo></msup><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>z</mi><mo>∗</mo></msup><mo>−</mo><mrow><mn>𝟏</mn><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝔼</mi><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>z</mi><mo>∗</mo></msup></mrow></mrow><mrow><msub><mi>σ</mi><mi>d</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>z</mi><mo>∗</mo></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathsf{LN}\left(z^{*}\right)=\frac{z^{*}-\mathbf{1}\mathbb{E}_{d}z^{*}}{\sigma_{d}z^{*}}</annotation></semantics></math>, where <math alttext="\mathbb{E}_{d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p2.m3" intent=":literal"><semantics><msub><mi>𝔼</mi><mi>d</mi></msub><annotation encoding="application/x-tex">\mathbb{E}_{d}</annotation></semantics></math> and <math alttext="\sigma_{d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p2.m4" intent=":literal"><semantics><msub><mi>σ</mi><mi>d</mi></msub><annotation encoding="application/x-tex">\sigma_{d}</annotation></semantics></math> are estimators of mean and standard deviation in dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx1.p2.m5" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, respectively.</p>
</div>
</section>
<section class="ltx_subparagraph" id="S3.SS1.SSS0.P0.SPx2">
<h6 class="ltx_title ltx_title_subparagraph">Activation vectors and parameter matrices.</h6>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.P0.SPx2.p1">
<p class="ltx_p">In vectors representing activations, each scalar element (element of <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p1.m1" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>) of the activation vector has the interpretation of a ‘scalar’ activation state of a single particle. Throughout this text, <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p1.m2" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> is generally assumed be the field of real numbers <math alttext="R:=\mathbb{R}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p1.m3" intent=":literal"><semantics><mrow><mi>R</mi><mo>:=</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">R:=\mathbb{R}</annotation></semantics></math>, and scalars are represented by a fixed-precision floating point number in experiments.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>When only asymptotic analysis is the object, it is sometimes convenient to consider <math alttext="R:=\mathbb{R}^{k}" class="ltx_Math" display="inline" id="footnote10.m1" intent=":literal"><semantics><mrow><mi>R</mi><mo>:=</mo><msup><mi>ℝ</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">R:=\mathbb{R}^{k}</annotation></semantics></math> for some <math alttext="k=2,3,\ldots" class="ltx_Math" display="inline" id="footnote10.m2" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow><annotation encoding="application/x-tex">k=2,3,\ldots</annotation></semantics></math>. Specifically, considering <math alttext="R:=\mathbb{R}^{2}" class="ltx_Math" display="inline" id="footnote10.m3" intent=":literal"><semantics><mrow><mi>R</mi><mo>:=</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R:=\mathbb{R}^{2}</annotation></semantics></math> allows <math alttext="SO(2)" class="ltx_Math" display="inline" id="footnote10.m4" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">SO(2)</annotation></semantics></math> rotations on <math alttext="\mathbb{R}^{2}" class="ltx_Math" display="inline" id="footnote10.m5" intent=":literal"><semantics><msup><mi>ℝ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\mathbb{R}^{2}</annotation></semantics></math> to be expressed as ‘scalar’ ones on <math alttext="R" class="ltx_Math" display="inline" id="footnote10.m6" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>, thus making the <math alttext="\mathbb{R}^{2n\times 2n}" class="ltx_Math" display="inline" id="footnote10.m7" intent=":literal"><semantics><msup><mi>ℝ</mi><mrow><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow></msup><annotation encoding="application/x-tex">\mathbb{R}^{2n\times 2n}</annotation></semantics></math> RoPE block-diagonal matrix of a diagonal matrix in <math alttext="R^{n\times n}" class="ltx_Math" display="inline" id="footnote10.m8" intent=":literal"><semantics><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup><annotation encoding="application/x-tex">R^{n\times n}</annotation></semantics></math>  <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib100" title="">2024</a>)</cite>. This provides a consistent formalism for ALiBi <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib88" title="">2022</a>)</cite>, RoPE, and extensions such as LieRE <cite class="ltx_cite ltx_citemacro_citep">(Ostmeier et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib83" title="">2025</a>)</cite> as diagonal (communication-free) operations. In all cases, the application of ReLU <math alttext="\left(\cdot\right)^{+}" class="ltx_Math" display="inline" id="footnote10.m9" intent=":literal"><semantics><msup><mrow><mo>(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>)</mo></mrow><mo>+</mo></msup><annotation encoding="application/x-tex">\left(\cdot\right)^{+}</annotation></semantics></math> to a scalar remains coordinate-wise in <math alttext="\mathbb{R}" class="ltx_Math" display="inline" id="footnote10.m10" intent=":literal"><semantics><mi>ℝ</mi><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.P0.SPx2.p2">
<p class="ltx_p">By convention, in discussions of parameters, matrices denoted <math alttext="{G_{x}},{G_{y}},{G_{s}}\in R^{n\times n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>G</mi><mi>x</mi></msub><mo>,</mo><msub><mi>G</mi><mi>y</mi></msub><mo>,</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{G_{x}},{G_{y}},{G_{s}}\in R^{n\times n}</annotation></semantics></math> will represent neuron-neuron interaction, encoders <math alttext="E\in R^{d\times n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m2" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d\times n}</annotation></semantics></math> reduce dimensionality of activation vectors (e.g., <math alttext="a^{*}=Ez" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m3" intent=":literal"><semantics><mrow><msup><mi>a</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><annotation encoding="application/x-tex">a^{*}=Ez</annotation></semantics></math> for <math alttext="z\in R^{n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m4" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in R^{n}</annotation></semantics></math>), and decoders <math alttext="D\in R^{n\times d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m5" intent=":literal"><semantics><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D\in R^{n\times d}</annotation></semantics></math> lift them back into <math alttext="R^{n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m6" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> (e.g., <math alttext="z^{\prime}=Da^{*}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p2.m7" intent=":literal"><semantics><mrow><msup><mi>z</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>a</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">z^{\prime}=Da^{*}</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.P0.SPx2.p3">
<p class="ltx_p">Depending on the architecture variant considered, the state will either have the interpretation of a neuron-neuron correlation matrix <math alttext="{\boldsymbol{\sigma}}\in R^{n\times n}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p3.m1" intent=":literal"><semantics><mrow><mi>𝝈</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}\in R^{n\times n}</annotation></semantics></math>, or a compressed form with reduced dimensionality, <math alttext="{\boldsymbol{\rho}}=E{\boldsymbol{\sigma}}\in R^{n\times d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.P0.SPx2.p3.m2" intent=":literal"><semantics><mrow><mi>𝝆</mi><mo>=</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝝈</mi></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}=E{\boldsymbol{\sigma}}\in R^{n\times d}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Definition of BDH-GPU as a state-space system</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">We now define the main architecture of this paper in its tensor flavor, called BDH-GPU.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 4</span></span><span class="ltx_text ltx_font_bold"> </span>(inference dynamics of BDH-GPU)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para ltx_noindent" id="Thmdefinition4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A BDH-GPU state-space system <math alttext="\textrm{BDH-GPU}(n,d)" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m1" intent=":literal"><semantics><mrow><mtext class="ltx_mathvariant_italic">BDH-GPU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{BDH-GPU}(n,d)</annotation></semantics></math>, given by three parameter matrices: <math alttext="E\in R^{d\times n}" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m2" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d\times n}</annotation></semantics></math> and <math alttext="{D_{x}},{D_{y}}\in R^{n\times d}" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{D_{x}},{D_{y}}\in R^{n\times d}</annotation></semantics></math>, performs iteration over time <math alttext="t=0,1,2\ldots" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m4" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">t=0,1,2\ldots</annotation></semantics></math> and layers <math alttext="l=1,2\ldots L" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m5" intent=":literal"><semantics><mrow><mi>l</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">l=1,2\ldots L</annotation></semantics></math>, governed for any time <math alttext="t" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m6" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> by the following recurrence:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx13">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}{v^{*}_{t,l-1}}\right)^{+}\\
{a^{*}_{t,l}}&amp;:=\sum_{\tau&lt;t}{v^{*}_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}{x_{t,l}}\\
{y_{t,l}}&amp;:=\left({D_{y}}\mathsf{LN}\left({a^{*}_{t,l}}\right)\right)^{+}\odot{x_{t,l}}\quad\\
{v^{*}_{t,l}}&amp;:=\mathsf{LN}\left(E{y_{t,l}}\right)\end{split}" class="ltx_Math" display="inline" id="S3.E4.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo>+</mo><msup><mrow><mo>(</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>v</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>∗</mo></msubsup></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msubsup><mi>a</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></munder></mstyle><mrow><msubsup><mi>v</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>∗</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mi>t</mi><mo>−</mo><mi>τ</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi></mi><mo>:=</mo><mrow><msup><mrow><mo>(</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msubsup><mi>a</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup><mo>)</mo></mrow></mrow><mo rspace="0.055em">)</mo></mrow><mo>+</mo></msup><mo rspace="0.222em">⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow><mspace style="width:1em;" width="1em"></mspace></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msubsup><mi>v</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}{v^{*}_{t,l-1}}\right)^{+}\\
{a^{*}_{t,l}}&amp;:=\sum_{\tau&lt;t}{v^{*}_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}{x_{t,l}}\\
{y_{t,l}}&amp;:=\left({D_{y}}\mathsf{LN}\left({a^{*}_{t,l}}\right)\right)^{+}\odot{x_{t,l}}\quad\\
{v^{*}_{t,l}}&amp;:=\mathsf{LN}\left(E{y_{t,l}}\right)\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where inputs to the system are provided through the boundary condition <math alttext="v^{*}_{\tau,0}" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m7" intent=":literal"><semantics><msubsup><mi>v</mi><mrow><mi>τ</mi><mo>,</mo><mn>0</mn></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">v^{*}_{\tau,0}</annotation></semantics></math> in layer <math alttext="0" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m8" intent=":literal"><mn>0</mn></math>, for <math alttext="\tau=0,1,2\ldots t" class="ltx_Math" display="inline" id="Thmdefinition4.p1.m9" intent=":literal"><semantics><mrow><mi>τ</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\tau=0,1,2\ldots t</annotation></semantics></math>.</span></p>
</div>
<div class="ltx_para" id="Thmdefinition4.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Here, <math alttext="U\in R^{n\times n}" class="ltx_Math" display="inline" id="Thmdefinition4.p2.m1" intent=":literal"><semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U\in R^{n\times n}</annotation></semantics></math> is a diagonal or block-diagonal matrix representing local rotation or damping of state (such as ALiBi or RoPE), <math alttext="L\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmdefinition4.p2.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">L\in\mathbb{N}</annotation></semantics></math> is the number of layers.</span></p>
</div>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">BDH-GPU as a language model.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p">BDH-GPU is intended to be used as a language model, processing one token per time step, in which case the input <math alttext="v^{*}_{t,0}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m1" intent=":literal"><semantics><msubsup><mi>v</mi><mrow><mi>t</mi><mo>,</mo><mn>0</mn></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">v^{*}_{t,0}</annotation></semantics></math>, for <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>, is obtained using some (linear) encoding function from the token alphabet <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>, <math alttext="f_{e}:\Omega\to R^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>f</mi><mi>e</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">→</mo><msup><mi>R</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f_{e}:\Omega\to R^{d}</annotation></semantics></math>, as applied to the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-th input tokens. Similarly, the logits of the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m6" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-th output token are extracted using some decoding function applied to outputs of the <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m7" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-th layer <math alttext="v^{*}_{t,L}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m8" intent=":literal"><semantics><msubsup><mi>v</mi><mrow><mi>t</mi><mo>,</mo><mi>L</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">v^{*}_{t,L}</annotation></semantics></math>, using a (linear) token decoder function <math alttext="f_{d}:R^{d}\to\Omega" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m9" intent=":literal"><semantics><mrow><msub><mi>f</mi><mi>d</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>R</mi><mi>d</mi></msup><mo stretchy="false">→</mo><mi mathvariant="normal">Ω</mi></mrow></mrow><annotation encoding="application/x-tex">f_{d}:R^{d}\to\Omega</annotation></semantics></math>. The source of language tokens may be external, as is the case for next token prediction tasks, or auto-regressive.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p">For training, we assume that a model <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> trained in the <math alttext="\textrm{BDH-GPU}(n,d)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mtext>BDH-GPU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{BDH-GPU}(n,d)</annotation></semantics></math> architecture has the trainable parameter set <math alttext="M=(E,{D_{x}},{D_{y}},f_{e},f_{d})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>E</mi><mo>,</mo><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><msub><mi>f</mi><mi>e</mi></msub><mo>,</mo><msub><mi>f</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">M=(E,{D_{x}},{D_{y}},f_{e},f_{d})</annotation></semantics></math>, with all parameters trained together. The model has <math alttext="3nd+2\Omega d=(3+o(1))nd" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mrow><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">3nd+2\Omega d=(3+o(1))nd</annotation></semantics></math> parameters, i.e., the scalable part of the model is concentrated in the total of <math alttext="3nd" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m5" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">3nd</annotation></semantics></math> parameters of the matrices <math alttext="(E,{D_{x}},{D_{y}})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>E</mi><mo>,</mo><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(E,{D_{x}},{D_{y}})</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">State-space representation.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p">The notation of Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a> is chosen so as to exhibit its direct applicability in a Transformer-like token-parallel training framework. Vector <math alttext="v^{*}_{\tau,l-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m1" intent=":literal"><semantics><msubsup><mi>v</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">v^{*}_{\tau,l-1}</annotation></semantics></math> has the interpretation of attention ‘value’ inputs at time <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m2" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> in layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m3" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>. Vector <math alttext="a^{*}_{t,l}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m4" intent=":literal"><semantics><msubsup><mi>a</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">a^{*}_{t,l}</annotation></semantics></math> represents the result of a linear attention mechanism for time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> in layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m6" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p">Denoting in (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E4" title="In Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>) the model’s attention state as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\boldsymbol{\rho}}_{t-1,l}=\sum_{\tau&lt;t}{v^{*}_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}" class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></munder><mrow><msubsup><mi>v</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>∗</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mi>t</mi><mo>−</mo><mi>τ</mi></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t-1,l}=\sum_{\tau&lt;t}{v^{*}_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">we obtain the equivalent but more compact form of representing the inference dynamics of BDH-GPU as a state-space model, presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F3" title="Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p3">
<span class="ltx_ERROR undefined">\@mathmargin</span>
</div>
<figure class="ltx_figure" id="S3.F3">
<p class="ltx_p ltx_align_center">
<span class="ltx_inline-block ltx_minipage ltx_align_top ltx_framed ltx_framed_rectangle" style="width:162.2pt;">
<span class="ltx_p">BDH</span>
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx14">
<span id="S3.E6"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}{\boldsymbol{\sigma}}_{t,l}&amp;:=\left({\boldsymbol{\sigma}}_{t-1,l}+\left(\left({y_{t,l-1}}{x_{t,l}}^{T}\right)\odot{G_{s}}\right)\right)U\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left(\left({G_{x}}^{\mathfrak{e}}-{G_{x}}^{\mathfrak{i}}\right){y_{t,l-1}}\right)^{+}\\
{y_{t,l}}&amp;:=\left(\left({G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}\right){\boldsymbol{\sigma}}_{t-1,l}{x_{t,l}}\right)^{+}\odot{x_{t,l}}\end{split}" class="ltx_Math" display="inline" id="S3.E6.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><mo>(</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts></mrow><mo rspace="0.055em">)</mo></mrow><mo rspace="0.222em">⊙</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo>+</mo><msup><mrow><mo>(</mo><mrow><mrow><mo>(</mo><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msup><mrow><mo>(</mo><mrow><mrow><mo>(</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo rspace="0.055em">)</mo></mrow><mo>+</mo></msup><mo rspace="0.222em">⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}{\boldsymbol{\sigma}}_{t,l}&amp;:=\left({\boldsymbol{\sigma}}_{t-1,l}+\left(\left({y_{t,l-1}}{x_{t,l}}^{T}\right)\odot{G_{s}}\right)\right)U\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left(\left({G_{x}}^{\mathfrak{e}}-{G_{x}}^{\mathfrak{i}}\right){y_{t,l-1}}\right)^{+}\\
{y_{t,l}}&amp;:=\left(\left({G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}\right){\boldsymbol{\sigma}}_{t-1,l}{x_{t,l}}\right)^{+}\odot{x_{t,l}}\end{split}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></span></span></span>
</span>
</span>  
<span class="ltx_inline-block ltx_minipage ltx_align_top ltx_framed ltx_framed_rectangle" style="width:162.2pt;">
<span class="ltx_p">BDH-GPU</span>
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx15">
<span id="S3.E8"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}{\boldsymbol{\rho}}_{t,l}&amp;:=\left({\boldsymbol{\rho}}_{t-1,l}+\mathsf{LN}\left(E{y_{t,l-1}}\right){x_{t,l}}^{T}\right)U\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}\mathsf{LN}\left(E{y_{t,l-1}}\right)\right)^{+}\\
{y_{t,l}}&amp;:=\left({D_{y}}\mathsf{LN}\left({\boldsymbol{\rho}}_{t-1,l}{x_{t,l}}\right)\right)^{+}\odot{x_{t,l}}\end{split}" class="ltx_Math" display="inline" id="S3.E8.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo>+</mo><msup><mrow><mo>(</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msup><mrow><mo>(</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>𝖫𝖭</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow><mo rspace="0.055em">)</mo></mrow><mo>+</mo></msup><mo rspace="0.222em">⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}{\boldsymbol{\rho}}_{t,l}&amp;:=\left({\boldsymbol{\rho}}_{t-1,l}+\mathsf{LN}\left(E{y_{t,l-1}}\right){x_{t,l}}^{T}\right)U\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}\mathsf{LN}\left(E{y_{t,l-1}}\right)\right)^{+}\\
{y_{t,l}}&amp;:=\left({D_{y}}\mathsf{LN}\left({\boldsymbol{\rho}}_{t-1,l}{x_{t,l}}\right)\right)^{+}\odot{x_{t,l}}\end{split}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></span></span></span>
</span>
</span>
<br class="ltx_break"/><math alttext="\searrow" class="ltx_Math" display="inline" id="S3.F3.m1" intent=":literal"><semantics><mo>↘</mo><annotation encoding="application/x-tex">\searrow</annotation></semantics></math>           <math alttext="\nearrow" class="ltx_Math" display="inline" id="S3.F3.m2" intent=":literal"><semantics><mo>↗</mo><annotation encoding="application/x-tex">\nearrow</annotation></semantics></math>
<br class="ltx_break"/>
<span class="ltx_inline-block ltx_minipage ltx_align_top ltx_framed ltx_framed_rectangle" style="width:165.6pt;">
<span class="ltx_p">BDH-Normfree</span>
<span class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx16">
<span id="S3.E7"><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}&amp;\hskip-18.49411pt\left.\begin{aligned} {\boldsymbol{\sigma}}_{t,l}&amp;:=\left({\boldsymbol{\sigma}}_{t-1,l}+{y_{t,l-1}}{x_{t,l}}^{T}\right)U\\
{\boldsymbol{\rho}}_{t,l}&amp;:=\left({\boldsymbol{\rho}}_{t-1,l}+(E{y_{t,l-1}}){x_{t,l}}^{T}\right)U\end{aligned}\right\}{\,{}^{\textrm{alternative}}_{\textrm{representations}}}\hskip-56.9055pt\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}E{y_{t,l-1}}\right)^{+}\\
{y_{t,l}}&amp;:=({D_{y}}\underbrace{E{\boldsymbol{\sigma}}_{t-1,l}}_{{}^{{\boldsymbol{\rho}}_{t-1,l}}}{x_{t,l}})^{+}\odot{x_{t,l}}\end{split}" class="ltx_math_unparsed" display="inline" id="S3.E7.m1" intent=":literal"><semantics><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtable columnspacing="0pt" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow></mtd></mtr></mtable><mo rspace="0.170em">}</mo><msup><mi></mi><mtext>alternative</mtext></msup><msub><mi></mi><mtext>representations</mtext></msub></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo>+</mo><msup><mrow><mo>(</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mi></mi><mo>:=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><munder><munder accentunder="true"><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo stretchy="true">⏟</mo></munder><msup><mi></mi><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub></msup></munder><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo>+</mo></msup><mo rspace="0.222em">⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\displaystyle\begin{split}&amp;\hskip-18.49411pt\left.\begin{aligned} {\boldsymbol{\sigma}}_{t,l}&amp;:=\left({\boldsymbol{\sigma}}_{t-1,l}+{y_{t,l-1}}{x_{t,l}}^{T}\right)U\\
{\boldsymbol{\rho}}_{t,l}&amp;:=\left({\boldsymbol{\rho}}_{t-1,l}+(E{y_{t,l-1}}){x_{t,l}}^{T}\right)U\end{aligned}\right\}{\,{}^{\textrm{alternative}}_{\textrm{representations}}}\hskip-56.9055pt\\
{x_{t,l}}&amp;:=x_{t,l-1}+\left({D_{x}}E{y_{t,l-1}}\right)^{+}\\
{y_{t,l}}&amp;:=({D_{y}}\underbrace{E{\boldsymbol{\sigma}}_{t-1,l}}_{{}^{{\boldsymbol{\rho}}_{t-1,l}}}{x_{t,l}})^{+}\odot{x_{t,l}}\end{split}</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></span></span></span>
</span>
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>State-space equations of the model architectures introduced in this paper.
All architectures refer to a set of <math alttext="n" class="ltx_Math" display="inline" id="S3.F3.m14" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> interacting particles (neurons), with activation vectors <math alttext="x_{t,l}\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S3.F3.m15" intent=":literal"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x_{t,l}\in(R^{+})^{n}</annotation></semantics></math>. Vector <math alttext="y_{t,l}\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S3.F3.m16" intent=":literal"><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">y_{t,l}\in(R^{+})^{n}</annotation></semantics></math>, <math alttext="y_{t,l}" class="ltx_Math" display="inline" id="S3.F3.m17" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l}</annotation></semantics></math> is (typically) sparse in the sense of <math alttext="\|y_{t,l}\|_{0}" class="ltx_Math" display="inline" id="S3.F3.m18" intent=":literal"><semantics><msub><mrow><mo stretchy="false">‖</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo stretchy="false">‖</mo></mrow><mn>0</mn></msub><annotation encoding="application/x-tex">\|y_{t,l}\|_{0}</annotation></semantics></math>. Variables <math alttext="{\boldsymbol{\rho}}_{t,l}\in R^{n\times d}" class="ltx_Math" display="inline" id="S3.F3.m19" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t,l}\in R^{n\times d}</annotation></semantics></math> or <math alttext="{\boldsymbol{\sigma}}_{t,l}\in R^{n\times n}" class="ltx_Math" display="inline" id="S3.F3.m20" intent=":literal"><semantics><mrow><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t,l}\in R^{n\times n}</annotation></semantics></math> represent hidden state of the system.
<math alttext="\diamond" class="ltx_Math" display="inline" id="S3.F3.m21" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> The graph-based BDH dynamics equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>), equivalent to the ruleset from Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>, serves as a starting point for development of architectures represented as local graph kernels in a distributed computing system.
<math alttext="\diamond" class="ltx_Math" display="inline" id="S3.F3.m22" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> The simplified BDH-Normfree equation (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:bdhnoln</span>) is a special case of BDH. Up to lack of LayerNorms, it approximates the inference dynamics of BDH-GPU, with the correspondence <math alttext="{\boldsymbol{\rho}}_{t,l}=E{\boldsymbol{\sigma}}_{t,l}" class="ltx_Math" display="inline" id="S3.F3.m23" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>=</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t,l}=E{\boldsymbol{\sigma}}_{t,l}</annotation></semantics></math>.
<math alttext="\diamond" class="ltx_Math" display="inline" id="S3.F3.m24" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> The tensor-based BDH-GPU architecture is described by equations (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>) (mathematically equivalent to Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E4" title="In Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>) and (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E5" title="In State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a>)) and is the primary point of reference for all model training and all empirical results presented in this study. For a discussion of extensions to BDH-GPU such as heads, see Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS1" title="4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4.1</span></a>. A complete code listing for BDH-GPU is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S3.F4.g1" src="x3.png" width="238"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Scaling of BDH-GPU architecture in dimension <math alttext="n" class="ltx_Math" display="inline" id="S3.F4.m7" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. The other parameters can be considered fixed during scaling. For example, with choice of <math alttext="d=256" class="ltx_Math" display="inline" id="S3.F4.m8" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math> for low-rank dimension, <math alttext="k=2" class="ltx_Math" display="inline" id="S3.F4.m9" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k=2</annotation></semantics></math> for neuron pairing with RoPE, and <math alttext="h=1" class="ltx_Math" display="inline" id="S3.F4.m10" intent=":literal"><semantics><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">h=1</annotation></semantics></math> for a single-head architecture, the model scales linearly in dimension <math alttext="n" class="ltx_Math" display="inline" id="S3.F4.m11" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> in chunks of <math alttext="dhk=256\cdot 2\cdot 1=512" class="ltx_Math" display="inline" id="S3.F4.m12" intent=":literal"><semantics><mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><mo>=</mo><mrow><mn>256</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>1</mn></mrow><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">dhk=256\cdot 2\cdot 1=512</annotation></semantics></math> parameters.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p4">
<p class="ltx_p">In what follows, we will perform analysis focusing on the state-space representation of the architecture given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Interpretation of BDH-GPU as a local interacting particle system</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p">The BDH-GPU dynamics equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>) has the interpretation of a <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-particle system, with the state <math alttext="{\boldsymbol{\rho}}_{t}(i)" class="ltx_Math" display="inline" id="S3.SS3.p1.m2" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t}(i)</annotation></semantics></math> of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p1.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th particle, <math alttext="i=1,\ldots,n" class="ltx_Math" display="inline" id="S3.SS3.p1.m4" intent=":literal"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">i=1,\ldots,n</annotation></semantics></math>, given at the end of time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p1.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> by a vector in <math alttext="R^{d}" class="ltx_Math" display="inline" id="S3.SS3.p1.m6" intent=":literal"><semantics><msup><mi>R</mi><mi>d</mi></msup><annotation encoding="application/x-tex">R^{d}</annotation></semantics></math> for each layer:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex24">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\boldsymbol{\rho}}_{i}(t):=({\boldsymbol{\rho}}_{t,l\ (i,\cdot)}:l\in(1,\ldots L))." class="ltx_math_unparsed" display="block" id="S3.Ex24.m1" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mi>i</mi></msub><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo>:=</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo lspace="0.410em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mi>l</mi><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mi>L</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{i}(t):=({\boldsymbol{\rho}}_{t,l\ (i,\cdot)}:l\in(1,\ldots L)).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Overall, as we will see directly, the way particle <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p1.m7" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> interacts with other particles at time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p1.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is described by the following tuple <math alttext="Z_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.m9" intent=":literal"><semantics><msub><mi>Z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Z_{i}</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex25">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z_{i}(t):=({\boldsymbol{\rho}}_{i}(t),E_{(i,\cdot)},{D_{x}}_{\,(\cdot,i)},{D_{y}}_{\,(\cdot,i)})." class="ltx_Math" display="block" id="S3.Ex25.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝝆</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></msub><mo>,</mo><mmultiscripts><mi>D</mi><mi>x</mi><mrow></mrow><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow></mrow></mmultiscripts><mo>,</mo><mmultiscripts><mi>D</mi><mi>y</mi><mrow></mrow><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo>,</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow></mrow></mmultiscripts><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">Z_{i}(t):=({\boldsymbol{\rho}}_{i}(t),E_{(i,\cdot)},{D_{x}}_{\,(\cdot,i)},{D_{y}}_{\,(\cdot,i)}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="{\boldsymbol{\rho}}_{i}(t)" class="ltx_Math" display="inline" id="S3.SS3.p1.m10" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{i}(t)</annotation></semantics></math> represents the in-context state associated with particle <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p1.m11" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> (initialized as <math alttext="\mathbf{0}" class="ltx_Math" display="inline" id="S3.SS3.p1.m12" intent=":literal"><semantics><mn>𝟎</mn><annotation encoding="application/x-tex">\mathbf{0}</annotation></semantics></math> at the start of inference), while the other three vectors of length <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p1.m13" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> associated with this particle are trainable, but do not change during inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p">The system scales in dimension <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> and is completely uniform in this dimension, excepting following effect. Let <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p2.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> denote the size of largest block in the block-diagonal matrix <math alttext="U" class="ltx_Math" display="inline" id="S3.SS3.p2.m3" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>; then particles, are bound by this effect into non-uniform <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p2.m4" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-tuples when <math alttext="k&gt;1" class="ltx_Math" display="inline" id="S3.SS3.p2.m5" intent=":literal"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k&gt;1</annotation></semantics></math> (<math alttext="k=1" class="ltx_Math" display="inline" id="S3.SS3.p2.m6" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math> when <math alttext="U" class="ltx_Math" display="inline" id="S3.SS3.p2.m7" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is the ALiBi matrix, and <math alttext="k=2" class="ltx_Math" display="inline" id="S3.SS3.p2.m8" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k=2</annotation></semantics></math> when <math alttext="U" class="ltx_Math" display="inline" id="S3.SS3.p2.m9" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is the RoPE matrix). Thus the system, in general, scales in the dimension of <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p2.m10" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> uniformly, in chunks of <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p2.m11" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> particles (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F4" title="Figure 4 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p">The interaction between particles is, intuitively, local. To be able to proceed with discussion with rigor and without complicating notation, we assume for the analysis that <math alttext="k=1" class="ltx_Math" display="inline" id="S3.SS3.p3.m1" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math>. We also drop LayerNorms from the equations of inference dynamics. (Models generally do not train following BDH-GPU without any LayerNorm, but we observed empirically that there is some flexibility as to where these LayerNorms are placed; they can also be moved to the neuron dimension <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p3.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, and they are parameter-free.) The dynamics without LayerNorm are represented under the name BDH-Normfree in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F3" title="Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p">We have the following.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 3</span></span><span class="ltx_text ltx_font_bold"> </span>(local particle interaction ‘by mean-field’)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The BDH-Normfree dynamics have the interpretation of a mean-field interaction between particles, fully characterized at any time by <math alttext="O(dL)" class="ltx_Math" display="inline" id="Thmobservation3.p1.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(dL)</annotation></semantics></math> parameters of particle in state, and <math alttext="O(d)" class="ltx_Math" display="inline" id="Thmobservation3.p1.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math> parameters in particle representation.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p">This observation is essential for the subsequent discussion, and it can be expanded in three different ways.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p">In computing terms, at any time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p6.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and in any layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS3.p6.m2" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, the action of the system can be represented as an iterated application of the dynamics equations (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>), with each of the particles realizing, for each equation in each layer (i.e., a total of <math alttext="3L" class="ltx_Math" display="inline" id="S3.SS3.p6.m3" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">3L</annotation></semantics></math> times), a form of micro-program, involving local computation and communication with other particles by broadcast. In a framework of local distributed computing (cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Peleg, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib86" title="">2000</a>)</cite>), it would be represented as a node performing the following form of local kernel as a part of a networked system:</p>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p">compute some message vector <math alttext="m_{i}\in R^{d}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">m_{i}\in R^{d}</annotation></semantics></math> locally (without communication with other particles), based only on current activation <math alttext="x_{t,l,\,i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m2" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi><mo rspace="0.337em">,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l,\,i}</annotation></semantics></math>, <math alttext="y_{t,l,\,i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m3" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi><mo rspace="0.337em">,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l,\,i}</annotation></semantics></math> and previous state <math alttext="Z_{i}(t-1)" class="ltx_Math" display="inline" id="S3.I2.i1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{i}(t-1)</annotation></semantics></math>,</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p">broadcast message <math alttext="m_{i}\in R^{d}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.m1" intent=":literal"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">m_{i}\in R^{d}</annotation></semantics></math> to other particles,</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p">receive the mean-field message <math alttext="\bar{m}=\sum_{j=1}^{n}m_{j}\in R^{d}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.m1" intent=":literal"><semantics><mrow><mover accent="true"><mi>m</mi><mo>¯</mo></mover><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\bar{m}=\sum_{j=1}^{n}m_{j}\in R^{d}</annotation></semantics></math>, identical for all particles,</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i4.p1">
<p class="ltx_p">update local activation variables for the next layer <math alttext="l+1" class="ltx_Math" display="inline" id="S3.I2.i4.p1.m1" intent=":literal"><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l+1</annotation></semantics></math>, and update new state <math alttext="\sigma_{i}(t)\subseteq Z_{i}(t)" class="ltx_Math" display="inline" id="S3.I2.i4.p1.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>σ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⊆</mo><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sigma_{i}(t)\subseteq Z_{i}(t)</annotation></semantics></math>, based on the received result <math alttext="\bar{m}" class="ltx_Math" display="inline" id="S3.I2.i4.p1.m3" intent=":literal"><semantics><mover accent="true"><mi>m</mi><mo>¯</mo></mover><annotation encoding="application/x-tex">\bar{m}</annotation></semantics></math> of the broadcast and local computation.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p7">
<p class="ltx_p">In Physical terms, we observe that the interaction field of the particles, which realizes the broadcast, is localized, and can at any time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p7.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> be expressed as a sum of pairwise particle interaction terms between particles <math alttext="i,j\in 1,\ldots,t" class="ltx_Math" display="inline" id="S3.SS3.p7.m2" intent=":literal"><semantics><mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">i,j\in 1,\ldots,t</annotation></semantics></math>. These pairwise interactions depend only on parameters <math alttext="Z_{i}(t-1)" class="ltx_Math" display="inline" id="S3.SS3.p7.m3" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{i}(t-1)</annotation></semantics></math> and <math alttext="Z_{j}(t-1)" class="ltx_Math" display="inline" id="S3.SS3.p7.m4" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{j}(t-1)</annotation></semantics></math>, and the activation variables of these particles, representing properties of these particles at time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p7.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and expressible through <math alttext="O(Ld)" class="ltx_Math" display="inline" id="S3.SS3.p7.m6" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(Ld)</annotation></semantics></math> scalars. This interaction field evolves with time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p7.m7" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> together with <math alttext="Z_{i}" class="ltx_Math" display="inline" id="S3.SS3.p7.m8" intent=":literal"><semantics><msub><mi>Z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Z_{i}</annotation></semantics></math> and <math alttext="Z_{j}" class="ltx_Math" display="inline" id="S3.SS3.p7.m9" intent=":literal"><semantics><msub><mi>Z</mi><mi>j</mi></msub><annotation encoding="application/x-tex">Z_{j}</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Note that <math alttext="Z_{i}(t-1)" class="ltx_Math" display="inline" id="footnote11.m1" intent=":literal"><semantics><mrow><msub><mi>Z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z_{i}(t-1)</annotation></semantics></math> depends only on <math alttext="{\boldsymbol{\rho}}_{t-1,l}" class="ltx_Math" display="inline" id="footnote11.m2" intent=":literal"><semantics><msub><mi>𝝆</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t-1,l}</annotation></semantics></math>, not <math alttext="{\boldsymbol{\rho}}_{t,l}" class="ltx_Math" display="inline" id="footnote11.m3" intent=":literal"><semantics><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t,l}</annotation></semantics></math>. This is because of the stopping index of <math alttext="\tau=t-1" class="ltx_Math" display="inline" id="footnote11.m4" intent=":literal"><semantics><mrow><mi>τ</mi><mo>=</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\tau=t-1</annotation></semantics></math> in the definition of attention <math alttext="a^{*}" class="ltx_Math" display="inline" id="footnote11.m5" intent=":literal"><semantics><msup><mi>a</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">a^{*}</annotation></semantics></math> in Def. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>, and is intentional.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p8">
<p class="ltx_p">In Engineering terms, we observe that any transformation of a length-<math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p8.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vector into another length-<math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p8.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vector passes through an intermediary low-rank representation of dimension at most <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p8.m3" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>. An example is the equation for <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="S3.SS3.p8.m4" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math> in (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:bdhnoln</span>), which reduces length-<math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p8.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vector <math alttext="y_{t,l}" class="ltx_Math" display="inline" id="S3.SS3.p8.m6" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l}</annotation></semantics></math> to a length <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p8.m7" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-vector through application of the encoder matrix <math alttext="E" class="ltx_Math" display="inline" id="S3.SS3.p8.m8" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math>, before lifting the dimension back to <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p8.m9" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> by an application of the decoder <math alttext="{D_{x}}" class="ltx_Math" display="inline" id="S3.SS3.p8.m10" intent=":literal"><semantics><msub><mi>D</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{D_{x}}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Expressing BDH-GPU using BDH: preserving parameter and state size</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p">BDH-GPU and BDH both represent <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-particle systems. For a special parameter choice (of BDH), they have equivalent patterns of communication and of computation (up to placement of layer norms).</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 4</span></span><span class="ltx_text ltx_font_bold"> </span>(BDH-Normfree is a special case of the BDH graph model)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Models in the BDH-Normfree architecture (Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>)) and models in the BDH architecture (Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>)) are formally equivalent (i.e., the same model) subject to the following choice of model parameters of BDH:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{G_{x}}^{\mathfrak{e}}-{G_{x}}^{\mathfrak{i}}={D_{x}}E,\quad{G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}={D_{y}}E,\quad{G_{s}}=\mathbf{1}^{n\times n}," class="ltx_Math" display="block" id="S3.E9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo>=</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo>=</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>G</mi><mi>s</mi></msub><mo>=</mo><msup><mn>𝟏</mn><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}}-{G_{x}}^{\mathfrak{i}}={D_{x}}E,\quad{G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}={D_{y}}E,\quad{G_{s}}=\mathbf{1}^{n\times n},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="\mathbf{1}^{n\times n}" class="ltx_Math" display="inline" id="Thmobservation4.p1.m1" intent=":literal"><semantics><msup><mn>𝟏</mn><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup><annotation encoding="application/x-tex">\mathbf{1}^{n\times n}</annotation></semantics></math> is the all-ones matrix.∎</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p">We discuss in more details below how BDH compares to BDH-Normfree in terms of size of state and number of parameters needed for one architecture to approximate the inference dynamics of the other. In general, BDH is not less expressive than its tensor-based counterpart.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p">For BDH-GPU parameters and state are naturally expressed using tensors of <math alttext="O(nd)" class="ltx_Math" display="inline" id="S3.SS4.p3.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> model parameters. In this section, we discuss how to express model parameters and state of BDH, in such a way as to maintain comparable size of parameter and model space.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Expressing matrices <math alttext="{D_{x}},{D_{y}},E" class="ltx_Math" display="inline" id="S3.SS4.SSS1.m1" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{x}},{D_{y}},E</annotation></semantics></math> as graphs <math alttext="{G_{x}},{G_{y}}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.m2" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>x</mi></msub><mo>,</mo><msub><mi>G</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">{G_{x}},{G_{y}}</annotation></semantics></math>
</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
<p class="ltx_p">We start by taking care of the first correspondence, that of parameter spaces of BDH-GPU and BDH. Asymptotically, BDH is strictly more expressive at the same number <math alttext="O(nd)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> parameters. Recall from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>) that the parameter space of BDH-GPU consists of three matrices <math alttext="{D_{y}}{D_{x}}\in R^{n\times d}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>D</mi><mi>x</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{D_{y}}{D_{x}}\in R^{n\times d}</annotation></semantics></math>, <math alttext="E\in R^{d\times n}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.m3" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d\times n}</annotation></semantics></math>, and (up to shifting of LayerNorms), their role is to encode the pairs of matrices <math alttext="{D_{y}}E,{D_{x}}E,\in R^{n\times n}" class="ltx_math_unparsed" display="inline" id="S3.SS4.SSS1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>y</mi></msub><mi>E</mi><mo>,</mo><msub><mi>D</mi><mi>x</mi></msub><mi>E</mi><mo>,</mo><mo lspace="0em">∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{D_{y}}E,{D_{x}}E,\in R^{n\times n}</annotation></semantics></math>, as used in Eq. (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:bdhnoln</span>). In the Claim below, we capture the correct encoding of one such matrix pair in the form of a graph of <math alttext="O(nd)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.m5" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p2">
<p class="ltx_p">Consider a (directed, weighted) graph <math alttext="G\in R+^{n\times n}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>∈</mo><mrow><mi>R</mi><msup><mo>+</mo><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">G\in R+^{n\times n}</annotation></semantics></math> on a set of vertices <math alttext="V=\{1,\ldots,n\}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m2" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">V=\{1,\ldots,n\}</annotation></semantics></math>. We will consider a graph which need be directly a sparse graph, but can be represented as a square of a graph with few edges. Formally, we will say that <math alttext="G\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m3" intent=":literal"><semantics><mrow><mi>G</mi><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G\in\mathcal{G}^{2}(n,m)</annotation></semantics></math>, for some <math alttext="m\in\mathbb{N}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m4" intent=":literal"><semantics><mrow><mi>m</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">m\in\mathbb{N}</annotation></semantics></math>, if there exists a graph <math alttext="H\in R^{(n+s)\times(n+s)}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m5" intent=":literal"><semantics><mrow><mi>H</mi><mo>∈</mo><msup><mi>R</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>+</mo><mi>s</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>+</mo><mi>s</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">H\in R^{(n+s)\times(n+s)}</annotation></semantics></math>, with vertex set <math alttext="V\cup S" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m6" intent=":literal"><semantics><mrow><mi>V</mi><mo>∪</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">V\cup S</annotation></semantics></math>, where <math alttext="|S|=s" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m7" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo></mrow><mo>=</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">|S|=s</annotation></semantics></math>, such that <math alttext="G=H^{2}[V]" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m8" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><msup><mi>H</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G=H^{2}[V]</annotation></semantics></math>, i.e., <math alttext="G" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m9" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> is the induced subgraph of <math alttext="H^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m10" intent=":literal"><semantics><msup><mi>H</mi><mn>2</mn></msup><annotation encoding="application/x-tex">H^{2}</annotation></semantics></math> restricted to vertex set <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m11" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>, and <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m12" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> has at most <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m13" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (strictly positive) edges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p3">
<p class="ltx_p">For a <math alttext="G\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G\in\mathcal{G}^{2}(n,m)</annotation></semantics></math>, we can consider an interpretation of a hidden layer <math alttext="S" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m2" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> between input layer <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m3" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> and output layer <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m4" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>. All matrix weights coefficients are restricted to be non-negative, and the two linear layers are sparse with a total of at most <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m5" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> non-negative connections.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p4">
<p class="ltx_p">Graphs in <math alttext="\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m1" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}^{2}(n,m)</annotation></semantics></math> are naturally expressed through the edges of the previously defined graph <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m2" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, using <math alttext="O(n\log n+m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m3" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>n</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow><mo>+</mo><mi>m</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n\log n+m)</annotation></semantics></math> parameters. The class <math alttext="\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m4" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}^{2}(n,m)</annotation></semantics></math> is strictly more expressive than the class of sparse (<math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m5" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-edge) graphs on vertex set <math alttext="V" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m6" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>The formal expression in the definition of the class of weighted graphs <math alttext="\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="footnote12.m1" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}^{2}(n,m)</annotation></semantics></math> can be compared to that of the class of graph distance matrices admitting sparse hub labeling representation <cite class="ltx_cite ltx_citemacro_citep">(Abraham et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib1" title="">2011</a>)</cite> (or closely related landmark labeling). In our case, vertices in the hidden layer <math alttext="S" class="ltx_Math" display="inline" id="footnote12.m2" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> also have a natural interpretation of landmarks on directed paths connecting nodes of <math alttext="V" class="ltx_Math" display="inline" id="footnote12.m3" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>.</span></span></span>
We will refer to the middle layer of vertices <math alttext="S" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m7" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> that makes such a representation possible as the <em class="ltx_emph ltx_font_italic">sparse synaptic layer</em>, to the graph <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m8" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> on vertex set <math alttext="V\cup S" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m9" intent=":literal"><semantics><mrow><mi>V</mi><mo>∪</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">V\cup S</annotation></semantics></math> as the <em class="ltx_emph ltx_font_italic">sparse linear circuit</em>, and the graph <math alttext="H^{2}[V]\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m10" intent=":literal"><semantics><mrow><mrow><msup><mi>H</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H^{2}[V]\in\mathcal{G}^{2}(n,m)</annotation></semantics></math> as the <em class="ltx_emph ltx_font_italic">neuron-neuron interaction graph</em>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p5">
<p class="ltx_p">The role of the constructed graphs is to serve for propagating linear dynamics of the form <math alttext="v\mapsto Gv" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p5.m1" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto Gv</annotation></semantics></math>, <math alttext="v\in(R^{+})^{n\times 1}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p5.m2" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v\in(R^{+})^{n\times 1}</annotation></semantics></math>, for graph-based local models. We have the following Observation.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 5</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="G\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="Thmobservation5.p1.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G\in\mathcal{G}^{2}(n,m)</annotation></semantics></math> be a neuron-neuron interaction graph, with <math alttext="G=H^{2}[V]" class="ltx_Math" display="inline" id="Thmobservation5.p1.m2" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><msup><mi>H</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G=H^{2}[V]</annotation></semantics></math>, where <math alttext="H" class="ltx_Math" display="inline" id="Thmobservation5.p1.m3" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> is the sparse linear circuit on vertex set <math alttext="V\cup S" class="ltx_Math" display="inline" id="Thmobservation5.p1.m4" intent=":literal"><semantics><mrow><mi>V</mi><mo>∪</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">V\cup S</annotation></semantics></math>, which has <math alttext="m" class="ltx_Math" display="inline" id="Thmobservation5.p1.m5" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edges. Then, the linear dynamics on graph <math alttext="G" class="ltx_Math" display="inline" id="Thmobservation5.p1.m6" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, <math alttext="v\mapsto Gv" class="ltx_Math" display="inline" id="Thmobservation5.p1.m7" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto Gv</annotation></semantics></math>, can be efficiently expressed through two steps of linear dynamics on graph <math alttext="H" class="ltx_Math" display="inline" id="Thmobservation5.p1.m8" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, <math alttext="v\mapsto H^{2}v" class="ltx_Math" display="inline" id="Thmobservation5.p1.m9" intent=":literal"><semantics><mrow><mi>v</mi><mo stretchy="false">↦</mo><mrow><msup><mi>H</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><annotation encoding="application/x-tex">v\mapsto H^{2}v</annotation></semantics></math>, for <math alttext="v\in(R^{+})^{n\times 1}" class="ltx_Math" display="inline" id="Thmobservation5.p1.m10" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v\in(R^{+})^{n\times 1}</annotation></semantics></math>. This representation requires <math alttext="O(m)" class="ltx_Math" display="inline" id="Thmobservation5.p1.m11" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(m)</annotation></semantics></math> parameters.
∎</span></p>
</div>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p6">
<p class="ltx_p">In the above, thee exact number of parameters needed to represent a graph of <math alttext="m" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p6.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edges follows from conventions introduced in the Notation (Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S1.SS4" title="1.4 Notation ‣ 1 Introduction ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1.4</span></a>). In what follows, we will assume that BDH represents its parameter matrices through appropriate sparse linear circuit graphs <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p6.m2" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, which it uses to realize the linear neuron-neuron interaction dynamics <math alttext="G" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p6.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>. We illustrate the correspondence between graphs <math alttext="G" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p6.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> and <math alttext="H" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p6.m5" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F5" title="Figure 5 ‣ 3.4.1 Expressing matrices 𝐷_𝑥,𝐷_𝑦,𝐸 as graphs 𝐺_𝑥,𝐺_𝑦 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S3.F5.g1" src="x4.png" width="495"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Neuron-neuron communication using graphs <math alttext="G\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.F5.m9" intent=":literal"><semantics><mrow><mi>G</mi><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G\in\mathcal{G}^{2}(n,m)</annotation></semantics></math>: correspondence between graph <math alttext="H" class="ltx_Math" display="inline" id="S3.F5.m10" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> with <math alttext="m" class="ltx_Math" display="inline" id="S3.F5.m11" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> edges (left), and neuron-neuron interaction graph <math alttext="G=H^{2}" class="ltx_Math" display="inline" id="S3.F5.m12" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">G=H^{2}</annotation></semantics></math> (right). The approach allows to express linear signal propagation on a broad class of graphs <math alttext="\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.F5.m13" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}^{2}(n,m)</annotation></semantics></math> using two steps of linear dynamics on a sparse circuit <math alttext="H" class="ltx_Math" display="inline" id="S3.F5.m14" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, i.e., <math alttext="Gz=H^{2}z" class="ltx_Math" display="inline" id="S3.F5.m15" intent=":literal"><semantics><mrow><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>=</mo><mrow><msup><mi>H</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><annotation encoding="application/x-tex">Gz=H^{2}z</annotation></semantics></math> for <math alttext="z\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S3.F5.m16" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in(R^{+})^{n}</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p7">
<p class="ltx_p">We observe that BDH can express BDH-GPU parameter matrices with the same asymptotic number of parameters. The claim below applies to pairs of matrices <math alttext="DE" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p7.m1" intent=":literal"><semantics><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">DE</annotation></semantics></math>, for <math alttext="D={D_{y}}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p7.m2" intent=":literal"><semantics><mrow><mi>D</mi><mo>=</mo><msub><mi>D</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">D={D_{y}}</annotation></semantics></math> and <math alttext="D={D_{x}}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p7.m3" intent=":literal"><semantics><mrow><mi>D</mi><mo>=</mo><msub><mi>D</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">D={D_{x}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For any matrices <math alttext="D\in R^{n,d}" class="ltx_Math" display="inline" id="Thmclaim3.p1.m1" intent=":literal"><semantics><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D\in R^{n,d}</annotation></semantics></math>, <math alttext="E\in R^{d,n}" class="ltx_Math" display="inline" id="Thmclaim3.p1.m2" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d,n}</annotation></semantics></math>, there exist neuron-neuron interaction graphs <math alttext="G^{\mathfrak{e}},G^{\mathfrak{i}}\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="Thmclaim3.p1.m3" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>,</mo><msup><mi>G</mi><mi>𝔦</mi></msup></mrow><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}},G^{\mathfrak{i}}\in\mathcal{G}^{2}(n,m)</annotation></semantics></math>, such that <math alttext="G^{\mathfrak{e}}-G^{\mathfrak{i}}=DE" class="ltx_Math" display="inline" id="Thmclaim3.p1.m4" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>−</mo><msup><mi>G</mi><mi>𝔦</mi></msup></mrow><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}-G^{\mathfrak{i}}=DE</annotation></semantics></math>, with <math alttext="m=O(nd)" class="ltx_Math" display="inline" id="Thmclaim3.p1.m5" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">m=O(nd)</annotation></semantics></math>.
In consequence, for the same asymptotic number of parameters <math alttext="O(nd)" class="ltx_Math" display="inline" id="Thmclaim3.p1.m6" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math>, graph-based feed-forward mechanisms of BDH are strictly more expressive than corresponding mechanisms in the tensor-based implementation, BDH-Normfree.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S3.SS4.SSS1.p8">
<p class="ltx_p">The short proof of the Claim is deferred to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS3" title="C.3 Proof of Claim 3 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">C.3</span></a>.
∎</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p9">
<p class="ltx_p">We note that the converse implication does not hold: an arbitrary graph <math alttext="G^{\mathfrak{e}}\in\mathcal{G}^{2}(n,m)" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p9.m1" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>∈</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒢</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}\in\mathcal{G}^{2}(n,m)</annotation></semantics></math> does not admit an exact low-rank decomposition <math alttext="G^{\mathfrak{e}}=DE" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p9.m2" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}=DE</annotation></semantics></math>. Indeed, in general any low-rank decomposition introduces a form of noise whose implications we discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a>: if <math alttext="G^{\mathfrak{e}}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p9.m3" intent=":literal"><semantics><msup><mi>G</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">G^{\mathfrak{e}}</annotation></semantics></math> has a modular (cluster) structure, the low-rank approximation <math alttext="G^{\mathfrak{e}}\approx DE" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p9.m4" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>≈</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}\approx DE</annotation></semantics></math> still allows a form of in-cluster propagation dynamics.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Expressing BDH-GPU attention on graphs: sparsification and trainability of <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>
</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
<p class="ltx_p">We recall that by Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation4" title="Observation 4 (BDH-Normfree is a special case of the BDH graph model). ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>, the equivalence between the attention state <math alttext="{\boldsymbol{\sigma}}_{t,l}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.m1" intent=":literal"><semantics><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t,l}</annotation></semantics></math> in BDH and in tensor-based implementation holds for the case of the complete directed graph, <math alttext="{G_{s}}=\mathbf{1}^{n\times n}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.m2" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>s</mi></msub><mo>=</mo><msup><mn>𝟏</mn><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{G_{s}}=\mathbf{1}^{n\times n}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p">This means two things: first, in BDH, graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> can be trainable, while in BDH-GPU it is not. This acts to the potential advantage of BDH for expressiveness.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p">Second, in BDH, the graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> obtained through the direct correspondence is dense: with <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons, BDH would need <math alttext="n^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m3" intent=":literal"><semantics><msup><mi>n</mi><mn>2</mn></msup><annotation encoding="application/x-tex">n^{2}</annotation></semantics></math> synapses to precisely reflect BDH-Normfree. This aspect is more of a technical nuisance than an actual difference: the expressiveness of the attention mechanism of BDH, equipped with a sparse graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m4" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>, is sufficient to represent the attention operation as used in BDH-Normfree. Indeed, in the tensor-based BDH-GPU dynamics, the attention operation is immediately followed by a low-rank operation, <math alttext="{\boldsymbol{\rho}}_{t,l}=E{\boldsymbol{\sigma}}_{t,l}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m5" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>=</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t,l}=E{\boldsymbol{\sigma}}_{t,l}</annotation></semantics></math>, where <math alttext="{\boldsymbol{\rho}}_{t,l}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m6" intent=":literal"><semantics><msub><mi>𝝆</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{t,l}</annotation></semantics></math> has <math alttext="nd" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m7" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">nd</annotation></semantics></math> parameters. Graph models can instead rely on a sparse graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m8" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> to achieve the same form of state compression through sparsification.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 4</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The attention block of BDH-Normfree can be expressed using the attention block of BDH with a graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="Thmclaim4.p1.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> having <math alttext="O(nd)" class="ltx_Math" display="inline" id="Thmclaim4.p1.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> edges, subject to a natural preparation of attention values entering the attention block of BDH (directly before this attention block).</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S3.SS4.SSS2.p4">
<p class="ltx_p">The formal statement of the Claim and its proof are deferred to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS4" title="C.4 Formal statement of Claim 4 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">C.4</span></a>.
∎</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p5">
<p class="ltx_p">Going beyond the formal equivalence between BDH and BDH-GPU from Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation4" title="Observation 4 (BDH-Normfree is a special case of the BDH graph model). ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>, the above Claim, combined with Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim3" title="Claim 3. ‣ 3.4.1 Expressing matrices 𝐷_𝑥,𝐷_𝑦,𝐸 as graphs 𝐺_𝑥,𝐺_𝑦 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>, shows that BDH has at least the same expressiveness as BDH-GPU even for the same number of parameters <math alttext="O(nd)" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p5.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> and the same size of state <math alttext="O(nd)" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p5.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> per layer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p6">
<p class="ltx_p">Independent of graph-based models, in the subsequent analysis of the feed-forward network and attention mechanisms of BDH-GPU, we will show that the matrices <math alttext="{D_{x}}E,{D_{y}}E,\sigma\in R^{n\times n}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p6.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><mo>,</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><mo>,</mo><mi>σ</mi></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{D_{x}}E,{D_{y}}E,\sigma\in R^{n\times n}</annotation></semantics></math> of BDH-GPU admit a natural interpretation as <math alttext="n" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p6.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-node directed graphs (once appropriate threshold functions are applied). For example, the visualizations in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F11" title="Figure 11 ‣ 6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">11</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F10" title="Figure 10 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">10</span></a> correspond to graph representations of matrices <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p6.m3" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> and <math alttext="{G_{x}}:={D_{x}}E" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p6.m4" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>x</mi></msub><mo>:=</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">{G_{x}}:={D_{x}}E</annotation></semantics></math> of BDH-GPU, respectively, after thresholding. This graph interpretation of matrices in BDH-GPU <em class="ltx_emph ltx_font_italic">also</em> defines the neuron-neuron communication graph of the underlying BDH model, given by the equivalence from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E9" title="In Observation 4 (BDH-Normfree is a special case of the BDH graph model). ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Implementation and scaling laws</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">A code framework for BDH-GPU, representing the architecture from Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>, is made available in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>. In this Section, we present some guidelines on choice of hyperparameters, and an empirical study of models implemented in the BDH-GPU architecture, as well as a comparison to the Transformer and other language model architectures.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation characteristics of BDH-GPU</h3>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="191" id="S4.F6.g1" src="x5.png" width="475"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Diagram of one layer of the BDH-GPU architecture, following Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>). Layer inputs are <math alttext="x_{l-1},y_{l-1}\in R^{n}" class="ltx_Math" display="inline" id="S4.F6.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x_{l-1},y_{l-1}\in R^{n}</annotation></semantics></math>, layer outputs are <math alttext="x_{l},y_{l}\in R^{n}" class="ltx_Math" display="inline" id="S4.F6.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>,</mo><msub><mi>y</mi><mi>l</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x_{l},y_{l}\in R^{n}</annotation></semantics></math>. Model parameters are contained in the <math alttext="E\in R^{n\times d}" class="ltx_Math" display="inline" id="S4.F6.m8" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{n\times d}</annotation></semantics></math> and <math alttext="{D_{x}},{D_{y}}\in R^{d\times n}" class="ltx_Math" display="inline" id="S4.F6.m9" intent=":literal"><semantics><mrow><mrow><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{D_{x}},{D_{y}}\in R^{d\times n}</annotation></semantics></math>, and shared across all layers. Each layer has a state <math alttext="{\boldsymbol{\rho}}_{l}\in R^{n\times d}" class="ltx_Math" display="inline" id="S4.F6.m10" intent=":literal"><semantics><mrow><msub><mi>𝝆</mi><mi>l</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}_{l}\in R^{n\times d}</annotation></semantics></math> which is used in the Linear Attention block and persisted over time. PyTorch code implementing the model is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a></figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Model scaling in neuron dimension <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p">The architecture <math alttext="\textrm{BDH-GPU}(n,d)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mtext>BDH-GPU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{BDH-GPU}(n,d)</annotation></semantics></math> has two main dimensions, <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> which is the dimension of its concept (<em class="ltx_emph ltx_font_italic">neuronal</em>) space, and <math alttext="d\ll n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mi>d</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">d\ll n</annotation></semantics></math>, which is its low-rank (<em class="ltx_emph ltx_font_italic">synaptic</em>) dimension. The model scales primarily with the number of neurons <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. Almost all of the weights of the model are contained in three <math alttext="n\times d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m5" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n\times d</annotation></semantics></math> parameter matrices called <math alttext="E,{D_{x}},{D_{y}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m6" intent=":literal"><semantics><mrow><mi>E</mi><mo>,</mo><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">E,{D_{x}},{D_{y}}</annotation></semantics></math>; thus, the number of parameters is precisely <math alttext="(3+o(1))nd" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m7" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">(3+o(1))nd</annotation></semantics></math>.
The ratio between the dimensions <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m8" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m9" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> increases rapidly (“asymptotically”) with model size; already for a 25M-parameter model, a sound choice of dimensions is: <math alttext="d=256" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m10" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math>, <math alttext="n=32768" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m11" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>32768</mn></mrow><annotation encoding="application/x-tex">n=32768</annotation></semantics></math>, read as <math alttext="32768" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m12" intent=":literal"><semantics><mn>32768</mn><annotation encoding="application/x-tex">32768</annotation></semantics></math> neurons, each characterized by a total of <math alttext="3d=3\cdot 256=768" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.m13" intent=":literal"><semantics><mrow><mrow><mn>3</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>256</mn></mrow><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">3d=3\cdot 256=768</annotation></semantics></math> scalar parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Layers and heads.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p">The architecture has <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> layers (e.g., <math alttext="L=10" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">L=10</annotation></semantics></math>). As in the Universal Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib27" title="">2019</a>)</cite>, all layers use the same set of weights for each of the parameter matrices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p">The architecture may be equipped with several heads <math alttext="h" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.m1" intent=":literal"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>, subdividing dimension <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. The role of heads is limited to a single parameter-free LayerNorm, normalizing outcomes of linear attention separately for each head. The optimal number of heads is typically smaller than in the Transformer (e.g., <math alttext="h=4" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.m3" intent=":literal"><semantics><mrow><mi>h</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">h=4</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Linear attention with state aligned to neurons.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p">The state space of the model is fixed and large. It has the macro-interpretation of associative memory (like KV-cache, but organized differently), and is used to perform linear attention. For each layer, the state space is independent and has a fixed dimension <math alttext="n\times d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n\times d</annotation></semantics></math>, the same as the model weight matrices. Thus, a portion of <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m2" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> parameters of a state is directly associated with each of the <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons. With each token processed, a fraction of the model’s state space is updated. Sharing of state between the <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.m4" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> layers is not performed in the vanilla architecture. As usual with SSM’s, there is no notion of a context window.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px3.p2">
<p class="ltx_p">Similarly to BDH, BDH-GPU maintains a large recurrent state comparable in size with its total number of parameters (c.f. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.F4" title="Figure 4 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>). This stems from the fact that both the recurrent state matrix, and parameter matrices are expressed as low rank <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.m1" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> factorizations of <math alttext="n\times n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.m2" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> graph transition matrices. We believe that this helps the model with generalization with respect to RNNs which have <math alttext="O(N^{2})" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.m3" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N^{2})</annotation></semantics></math> parameters which manipulate a state of size <math alttext="O(N)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.m4" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Sparse positive activation.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p">The architecture relies on a length-<math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vector <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m2" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math> passed to the <math alttext="l" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m3" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>-th layer for the <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-th token processed, which can be assimilated to the vector giving rise to keys, values, and queries in the Transformer, but operating in higher dimension. As a crucial design assumption, this vector has all non-negative elements (<math alttext="x_{t,l}\geq 0" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.m5" intent=":literal"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x_{t,l}\geq 0</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px4.p2">
<p class="ltx_p">An empirically observed fact is that the activation pattern of <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.m1" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> rapidly becomes sparse (in a typical training run, only <math alttext="\rho\approx 5\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.m2" intent=":literal"><semantics><mrow><mi>ρ</mi><mo>≈</mo><mrow><mn>5</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\rho\approx 5\%</annotation></semantics></math> of the <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> entries of vector <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.m4" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> are non-zero). This corresponds to the fraction of the state space read and updated for each token.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison of BDH-GPU to GPT2-like Transformers</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Architecture differences.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p">BDH-GPU in its vanilla form can be compared to the GPT2 architecture <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib89" title="">2019</a>)</cite> with RoPE attention. In this comparison, BDH-GPU retains or strengthens the key advantages of the Transformer (parallel trainability, attention mechanism, scaling laws for loss versus parameter count, learning rate per token) on tests and benchmarks at the model scales we tested (1B parameters), across tasks such as language and translation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p2">
<p class="ltx_p">The architecture of a single layer of BDH-GPU is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.F6" title="Figure 6 ‣ 4.1 Implementation characteristics of BDH-GPU ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>. The most evident architecture differences between BDH-GPU and the Transformer include the following:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix1.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p">BDH-GPU has fewer parameter matrices, allowing for more compact interpretation and analysis.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix2.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p">BDH-GPU scales for parameters (and context length) almost exclusively in a single neuronal dimension, <math alttext="n" class="ltx_Math" display="inline" id="S4.I1.ix2.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix3.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p">Key-value state and parameter matrices have matching dimensions and are highly localized together with state, with portions of these matrices attributable to individual neurons.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix4.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.ix4.p1">
<p class="ltx_p">There is no notion of context length in BDH-GPU, and consequently no hard bound on it.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix5.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para" id="S4.I1.ix5.p1">
<p class="ltx_p">Attention of BDH-GPU is linear, but happens in the model’s large neuronal dimension.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="-" class="ltx_Math" display="inline" id="S4.I1.ix6.m1" intent=":literal"><semantics><mo>−</mo><annotation encoding="application/x-tex">-</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S4.I1.ix6.p1">
<p class="ltx_p">Activation vectors <math alttext="x,y" class="ltx_Math" display="inline" id="S4.I1.ix6.p1.m1" intent=":literal"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x,y</annotation></semantics></math> of BDH-GPU are positive (after passing through ReLU gates), and vectors <math alttext="y" class="ltx_Math" display="inline" id="S4.I1.ix6.p1.m2" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> are observed to be extremely sparse in practice.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Transformer-like scaling laws.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p">We have experimentally validated the scaling laws of BDH-GPU, expressing loss as a function of parameter count, for next-token-prediction tasks. At the same parameter scale, BDH-GPU generally compares favorably to the Transformer even on relatively short-context tasks requiring use of attention, such as translation, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.F7" title="Figure 7 ‣ Transformer-like scaling laws. ‣ 4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a>. In general, on next-token prediction tasks, BDH-GPU appears to show improvement of loss reduction per token of data than the Transformer, i.e., <em class="ltx_emph ltx_font_italic">learns faster per data token</em>, both for the natural tasks we tested (see e.g. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.F7" title="Figure 7 ‣ Transformer-like scaling laws. ‣ 4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a>) and on synthetic puzzles.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S4.F7.g1" src="bdh_scaling.png" width="333"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance of BDH-GPU and GPTXL versus model size on a translation task. We have tested all models under the same training and evaluation regimes. All models show improved performance with scale. BDH-GPU uses exactly the formulation provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>, while BDH-GPU’ extends conditional gating of states and logits. All models are trained with truncated backpropagation through time on sequences 2048 characters long, and carry their state (<math alttext="{\boldsymbol{\rho}}" class="ltx_Math" display="inline" id="S4.F7.m3" intent=":literal"><semantics><mi>𝝆</mi><annotation encoding="application/x-tex">{\boldsymbol{\rho}}</annotation></semantics></math> matrix for BDH models and a buffer of last 4096 KV-Cache entries <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib26" title="">2019</a>)</cite> for GPTXL) between minibatches. BDH models are scaled only by varying the number of neurons <math alttext="n" class="ltx_Math" display="inline" id="S4.F7.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> and keep all other hyperparameters fixed, making them easy to scale. On the other hand, GPTXL were scaled in both the embedding dimension and the number of layers and required Dropout <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib99" title="">2014</a>)</cite> tuning for optimal performance. We observe that BDH-GPU’ matches the GPT Transformer at all model sizes we have evaluated.
<br class="ltx_break"/>Details on model hyperparameters and training setup are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS2" title="B.2 BDH Scaling Experimental Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.2</span></a></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p">The BDH-GPU architecture appears to be a preferred choice for training setups where: (1) models need to learn from scarce data, or (2) training workloads need to be optimized for makespan. For the first setting, the training rate per token is the decisive factor. For the second setting, BDH-GPU can be used differently than the Transformer in distributed training and distributed inference setups because of the way it scales its dimensions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">FLOPS counts.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p">The theoretical count of arithmetic operations per token of BDH-GPU during inference is bounded by <math alttext="O(ndL)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m1" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(ndL)</annotation></semantics></math>. Each parameter is accessed <math alttext="O(L)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(L)</annotation></semantics></math> times per token (with the typical sufficient number of layers being smaller than in the Transformer), and each element of state is accessed <math alttext="O(1)" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.m3" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math> times per token, with small hidden constants. These are rough bounds for a simple implementation, and do not take advantage of activation sparsity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p2">
<p class="ltx_p">For short contexts BDH-GPU is amenable to parallel training with a causal self-attention kernel. The simple code template provided in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a> is sufficient to reproduce the empirical results presented in this paper on a single GPU node. For longer contexts (typically above 4096 tokens for <math alttext="d=256" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p2.m1" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math>), a state-space kernel for linear attention is faster and more space-efficient.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison of BDH-GPU to other sequence processing architectures</h3>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Transformers with Linear Attention.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p">Linear attention works well when used in high dimension, subject to appropriate preparation of key vectors (as we discuss in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1" title="6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.1</span></a>). An elegant way to eliminate non-linearity of attention, by applying preparation of key vectors through tensor product, was proposed in <cite class="ltx_cite ltx_citemacro_citep">(Buckman et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib17" title="">2024</a>)</cite>. We use a completely different approach to achieve attention in high dimension.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p">A much broader line of work on linear attention for the Transformer, initiated by <cite class="ltx_cite ltx_citemacro_citep">(Katharopoulos et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib54" title="">2020</a>)</cite> concerns applying linear attention in low dimension after appropriate preparation of keys and values. This is effectively a technique for SSM state compression, and it is not clear whether it relates favorably to other SSM state compression techniques. An empirical study of the amount of information recoverable from SSM’s with compressed state can be found in <cite class="ltx_cite ltx_citemacro_citep">(Ben-Kish et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib11" title="">2025</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib66" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p3">
<p class="ltx_p">A general theoretical framework for analyzing the expressiveness of Linear Attention models with attention working with positive vectors can be found in the context of the FAVOR+ framework of the Performer <cite class="ltx_cite ltx_citemacro_citep">(Choromanski et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib22" title="">2021</a>)</cite>. Finally, a general state-space formalism for Transformer models admitting Linear Attention was considered in <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib101" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib66" title="">2025</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Other types of Transformers.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p">Variants of the Transformer with identical parameters in all layers, like the Universal Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib27" title="">2019</a>)</cite>, have a number of desirable features, notably in terms of explainability and ease of defining metrics. The downside of sharing parameters between layers in the Universal Transformer is a slight time overhead for the feed-forward network operations, when measured in FLOPS per parameter. The situation is similar in BDH-GPU.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px2.p2">
<p class="ltx_p">BDH-GPU has sufficient expressiveness to prepare keys and queries for the attention operation, so that the outcome of attention captures a similarity measure between keys and queries corresponding to the outcome of a class of Locality Sensitive Hashing (LSH) functions with a very large number of buckets (cf. Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS1" title="6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.1</span></a>). The study of LSH-based KV-cache for the Transformer was initiated with the Reformer <cite class="ltx_cite ltx_citemacro_citep">(Kitaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib56" title="">2020</a>)</cite>, and the LSH Transformer architecture introduced in the same work. Generally, the LSH Transformer is shown to rapidly approach Transformer baseline behavior in practice as the number of buckets increases. The class of LSH functions considered is not the same, but some intuitions gained in the study of LSH attention may carry over to BDH-GPU.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px2.p3">
<p class="ltx_p">Finally, several lines of work have been devoted to making the Transformer work with longer context windows. Two distinct approaches, which work notably well, are the soft-rolling context window of the TransformerXL <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib26" title="">2019</a>)</cite>, and hierarchical attention <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib112" title="">2016</a>)</cite>. The BDH-GPU architecture is, generally, amenable to some of these extensions to the Transformer’s attention mechanism, while also providing new ways to extend context length in a more uniform manner.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Networks with sparse activation.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p">The use of the ReLU gate as a systematic way to achieve sparse activation was, to our knowledge, first exhibited in <cite class="ltx_cite ltx_citemacro_citep">(Haziza et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib36" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px3.p2">
<p class="ltx_p">A recent variant of the Transformer called Spark Transformer <cite class="ltx_cite ltx_citemacro_citep">(You et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib113" title="">2025</a>)</cite> relies on a combination of top-k operations and soft thresholding to provide a reduction in both attention and feed forward network activations compared to the Transformer, achieving neuron sparse activation of 8%. Compared to our work, the method used therein to achieve activation sparsity effects is completely different (and rather involved). Beyond the question of sparsity, BDH-GPU is not more similar to the Spark Transformer than to the Transformer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Oscillatory SSM’s.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p1">
<p class="ltx_p">BDH admits an interpretation at the micro-level as an oscillatory state-space network, as we outlined in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS4" title="2.4 Interpretation of BDH as an oscillator network toy-model ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.4</span></a>. The concept of Oscillatory State Space Models has recently been applied to time series analysis <cite class="ltx_cite ltx_citemacro_citep">(Rusch and Rus, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib93" title="">2025</a>)</cite>, with the LinOSS model showing encouraging performance relative to other SSM’s (such as Mamba and LSTM’s) on tasks of long-horizon forecasting and time-series classification. Other than this, the use of SSM’s with the form of an oscillator network has been limited to smaller scale studies. We are not aware of any successful application of oscillatory SSM’s to the area of language models and reasoning in language, nor of oscillator network SSM’s at scale whatsoever, prior to BDH.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p2">
<p class="ltx_p">BDH unifies multiple lines of intuition found across existing models, offering a coherent framework in which the components naturally align. The result is a biologically plausible reasoning model with an interpretable structure and state-of-the-art performance that has been experimentally verified.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis: emergence of modularity and scale-free structure</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">Large-scale reasoning systems appear to benefit from hierarchical structuring into sub-modules. In Machine Learning, the usual approach has been to design such a modular structure, by way of assigning roles and scales to different sub-modules explicitly. Many works have postulated modules capable of representing hierarchical relationships between features of objects, e.g., capsule networks <cite class="ltx_cite ltx_citemacro_citep">(Sabour et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib94" title="">2017</a>)</cite>. Some models have attempted to capture intelligence by recreating elements of structure recognized in brain study, going so far as to try to map functional sub-networks of the brain with empirically identified function into specific sub-modules in the design of a larger ML system, cf. <cite class="ltx_cite ltx_citemacro_citep">(LeCun, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib61" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p">In this work, we propose a learnable system which ends up with modularity. We show how scale-free modular structure emerges naturally when the model is implemented by a network with local graph dynamics.
In this Section, we discuss the emergence of the structure of inter-neuron connections of BDH during training, while in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6" title="6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a> we look at its temporal activation patterns during reasoning inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p">The rest of this section is organized as follows. In Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS1" title="5.1 Background: modularity and scale-free property of systems ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we introduce basic concepts related to modularity and scale-free behavior of networks. We then look at the expressiveness of feedforward networks of BDH-GPU and their usefulness as a signal propagation dynamics in Subsections <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS2" title="5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a>. In Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS4" title="5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.4</span></a>, we show theoretically how modular graph structure, with appropriate community voting mechanisms, emerges as a plausibly necessary element for the feed-forward networks <math alttext="{D_{x}}E" class="ltx_Math" display="inline" id="S5.p3.m1" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{x}}E</annotation></semantics></math> and <math alttext="{D_{y}}E" class="ltx_Math" display="inline" id="S5.p3.m2" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{y}}E</annotation></semantics></math> to function correctly. In Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.5</span></a>, we look at the corresponding empirical properties of these matrices, and the scale-free and modularity properties of the corresponding graphs <math alttext="{G_{x}}^{\mathfrak{e}}" class="ltx_Math" display="inline" id="S5.p3.m3" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mi>x</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{G_{x}}^{\mathfrak{e}}</annotation></semantics></math> and <math alttext="{G_{y}}^{\mathfrak{e}}" class="ltx_Math" display="inline" id="S5.p3.m4" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{G_{y}}^{\mathfrak{e}}</annotation></semantics></math> of the underlying BDH graph dynamics.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Background: modularity and scale-free property of systems</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Importance of modularity for information propagation.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p">Graph systems serving a function related to information propagation tend to achieve modular graph structure, and rely on it to obtain the most desirable tradeoff between efficiency and accuracy of the system dynamics. Such emergence of “hidden structure” may be observed e.g. through topic specialization of system regions, or through the coordinated voting behavior among nodes which organize themselves into communities, admitting higher local density. This type of graph community self-organization has two main advantages over a system with an explicit partition into subsystems. First, it allows nodes to belong to multiple communities, and to act as bridges between them. Second, it allows the scale and relationship between communities to evolve over time, as their relative importance changes or new connections emerge.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p">Historically, the crucial role of emergent modular structure for systems tasked with efficient knowledge retrieval at scale was first observed in the context of the World Wide Web before the year 2000, notably in the transition from catalogue-based systems (DMOZ Open Directory Project, craigslist, etc.) to naturally evolving systems based on webs of knowledge (Wikipedia, etc.), interlinked topic-based communities (reddit, etc.), and reliance on evolving network link structure for assigning expert weights to nodes in a voting process (Google PageRank, etc.). Formalization of modular properties followed soon after, with the mostly commonly used definition of modularity being proposed by Newman in <cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib79" title="">2006</a></cite>. The main theoretical reference for studies of modularity is the Stochastic Block Model <cite class="ltx_cite ltx_citemacro_citep">(Holland et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib46" title="">1983</a>)</cite> and its later generalizations, e.g., to hierarchical settings. While the definition of Newman modularity is not (efficiently) constructive, it can in practice be closely approximated by greedy algorithms <cite class="ltx_cite ltx_citemacro_citep">(Blondel et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib13" title="">2008</a>)</cite> or spectral approaches <cite class="ltx_cite ltx_citemacro_citep">(Massoulié, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib69" title="">2014</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Scale-free property.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p">The scale-free property of natural systems dealing with information processing is generally accepted as a manifestation of their operation at criticality. This refers to operation within a regime where they are both sufficiently stable to enable efficient information retrieval in the short-term, and sufficiently adaptable to be able change their behavior abruptly as new knowledge inputs become available, invalidating previous paths of reasoning or knowledge retrieval. The generally accepted definition of scale-free behavior of such a dynamical system assumes that the likelihood of a new piece of information (or other localized innovation to the system) to affect <math alttext="n^{\prime}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><msup><mi>n</mi><mo>′</mo></msup><annotation encoding="application/x-tex">n^{\prime}</annotation></semantics></math> nodes of the system, for any <math alttext="n^{\prime}&lt;n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><msup><mi>n</mi><mo>′</mo></msup><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n^{\prime}&lt;n</annotation></semantics></math>, should by polynomially large in <math alttext="1/n^{\prime}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><msup><mi>n</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">1/n^{\prime}</annotation></semantics></math>. For most information propagation dynamics, under certain uniformity assumptions, e.g., that the new piece of information arrives at a uniformly random node of the system, a usual necessary (not sufficient) condition for scale-free property is for the distribution of node degrees to follow a power-law distribution.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p">In the practice of applied sciences studying real-world network phenomena, and in the absence of the possibility to perform more in-depth analysis, power-law degree distributions are sometimes equated with scale-free behavior. One notable research application involves modeling of extreme events: understanding scale-free behavior allows researchers to make predictions about rare, large events from data on smaller, more common ones.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p3">
<p class="ltx_p">For systems with known local graph dynamics, like those considered here, more refined analysis of scale-free properties are possible. We nonetheless also report heavy-tailed degree behavior as the most obvious lithmus test indicator of scale-free operation of the system.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p">Low-rank matrices have been considered in multiple contexts of Machine Learning, from preference vectors to Internet latency estimation. In the setting of the Transformer, low-rank matrices form the basis of weight-matrix approximations such as LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib47" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p">The ReLU-lowrank block of BDH-GPU captures different properties than the above settings. Its most important effects for BDH-GPU are related to noise reduction, and faithful representation of a certain class of affinity functions on sparse positive vectors. This makes it suitable for use in combination with Linear Attention blocks. We discuss this point further in this Section.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Definition of ReLU-lowrank.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p">The parameters of BDH-GPU are concentrated in three matrices <math alttext="E,{D_{x}},{D_{y}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>E</mi><mo>,</mo><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">E,{D_{x}},{D_{y}}</annotation></semantics></math>. The encoder matrix <math alttext="E" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m2" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> transforms length-<math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vectors in the neuronal layer into length-<math alttext="d" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m4" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> vectors in the hidden layer. The two decoder matrices <math alttext="D\in\{{D_{x}},{D_{y}}\}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m5" intent=":literal"><semantics><mrow><mi>D</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><msub><mi>D</mi><mi>y</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">D\in\{{D_{x}},{D_{y}}\}</annotation></semantics></math> transform length-<math alttext="d" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.m6" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> vectors in the hidden layer back to the neuronal layer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px1.p2">
<p class="ltx_p">We consider the <em class="ltx_emph ltx_font_italic">ReLU-lowrank</em> operation of passing through the encoder, one of the decoders, and a ReLU gate (cf. Eq. (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:eq:bdhnoln</span>)), mapping vectors <math alttext="z\in R^{n}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p2.m1" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in R^{n}</annotation></semantics></math> into <math alttext="f_{DE}(z)\in R^{n}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">f_{DE}(z)\in R^{n}</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z\mapsto f_{DE}(z):=\left(DEz\right)^{+}." class="ltx_Math" display="block" id="S5.E10.m1" intent=":literal"><semantics><mrow><mrow><mi>z</mi><mo stretchy="false">↦</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><msup><mrow><mo>(</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">z\mapsto f_{DE}(z):=\left(DEz\right)^{+}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We note that the output <math alttext="f_{DE}(z)\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">f_{DE}(z)\in(R^{+})^{n}</annotation></semantics></math> always, and that in BDH-GPU we also always have <math alttext="z\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in(R^{+})^{n}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Expressiveness of ReLU-lowrank in BDH-GPU and MLP in the Transformer.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p">A single ReLU-lowrank block can be compared to a single MLP block of the Transformer. A different comparison provides closer matching of dimensions and structure of nonlinearities, by considering a single ReLU-lowrank with respect to a portion of the Transformer corresponding to the second MLP layer in an MLP block, i.e., starting with the hidden layer of neurons of the MLP in some layer <math alttext="l" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, skipping attention blocks, and followed by the ‘first’ linear layer of the MLP in layer <math alttext="l+1" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l+1</annotation></semantics></math>, finally followed by the non-linearity (typically GeLU) applied in the hidden layer of neurons in layer <math alttext="l+1" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.m3" intent=":literal"><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l+1</annotation></semantics></math>. Either approach to expressiveness is valid to the extent where we analyze similarities between one architecture with <math alttext="L" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> layers and the other with “<math alttext="O(L)" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.m5" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(L)</annotation></semantics></math>” layers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p2">
<p class="ltx_p">In the spirit of universal approximation theorem frameworks, a (deep) layer-<math alttext="L" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> stacking of Transformer’s MLP block with ReLU activation, for Transformer latent dimension <math alttext="D" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m2" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> and MLP hidden layer dimension <math alttext="cD" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m3" intent=":literal"><semantics><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">cD</annotation></semantics></math> (e.g., for <math alttext="c=4" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m4" intent=":literal"><semantics><mrow><mi>c</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">c=4</annotation></semantics></math>), is eventually (i.e., for <math alttext="L\to+\infty" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m5" intent=":literal"><semantics><mrow><mi>L</mi><mo stretchy="false">→</mo><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">L\to+\infty</annotation></semantics></math>) a universal approximator for all vector functions up to dimension <math alttext="D-O(1)" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m6" intent=":literal"><semantics><mrow><mi>D</mi><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">D-O(1)</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib97" title="">2022</a>)</cite>. A similar universal approximation result eventually (i.e., for <math alttext="L\to+\infty" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m7" intent=":literal"><semantics><mrow><mi>L</mi><mo stretchy="false">→</mo><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow></mrow><annotation encoding="application/x-tex">L\to+\infty</annotation></semantics></math>) holds up to function dimension <math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m8" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> for residual ReLU-lowrank networks <cite class="ltx_cite ltx_citemacro_citep">(Lin and Jegelka, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib65" title="">2018</a>)</cite>, however the convergence rate in <math alttext="L" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m9" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is slower due to the smaller size of the hidden layer. These results translate directly to BDH-GPU architecture which also relies on ReLU with residual connections between layers. To summarize informally, for a Transformer with latent dimension <math alttext="D" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m10" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> and BDH-GPU with hidden dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m11" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>, we expect their feed-forward networks to be comparably expressive (though usually without strict mathematical equivalence) as function approximators for functions up to some dimension <math alttext="d^{\prime},d&lt;d^{\prime}&lt;D" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m12" intent=":literal"><semantics><mrow><mrow><msup><mi>d</mi><mo>′</mo></msup><mo>,</mo><mi>d</mi></mrow><mo>&lt;</mo><msup><mi>d</mi><mo>′</mo></msup><mo>&lt;</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">d^{\prime},d&lt;d^{\prime}&lt;D</annotation></semantics></math>, between <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m13" intent=":literal"><semantics><msup><mi>d</mi><mo>′</mo></msup><annotation encoding="application/x-tex">d^{\prime}</annotation></semantics></math> and <math alttext="D" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m14" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> the Transformer can express a richer class of functions, and between <math alttext="D" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m15" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> and <math alttext="n" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p2.m16" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, BDH-GPU can approximate some functions, whereas the Transformer does not use such high dimension in its vector representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p3">
<p class="ltx_p">We remark that in all cases, regardless of expressiveness of feed-forward mechanisms, BDH-GPU is set up so that it is only using inputs and producing outputs within the positive orthant, <math alttext="(R^{+})^{n}\mapsto(R^{+})^{n}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p3.m1" intent=":literal"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo stretchy="false">↦</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">(R^{+})^{n}\mapsto(R^{+})^{n}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p4">
<p class="ltx_p">The main point to consider is: <em class="ltx_emph ltx_font_italic">what classes of useful high-dimensional functions in the positive orthant does ReLU-lowrank naturally express?</em></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>ReLU-lowrank as a signal propagation dynamics</h3>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Error of low-rank approximation (without ReLU).</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p">Consider <math alttext="R^{n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> as a space spanned by a fixed set of <math alttext="n" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> orthogonal unit basis vectors <math alttext="V=\{v_{1},\ldots,v_{n}\}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.m3" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>v</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">V=\{v_{1},\ldots,v_{n}\}</annotation></semantics></math>, called <em class="ltx_emph ltx_font_italic">nodes</em>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p2">
<p class="ltx_p">The low-rank operation can be used to approximate affinities between pairs of nodes, in the following sense. For a given matrix <math alttext="G^{\prime}\in R^{n\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p2.m1" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">G^{\prime}\in R^{n\times n}</annotation></semantics></math>, consider low-rank matrices <math alttext="D\in R^{n\times d},E\in R^{d\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">D\in R^{n\times d},E\in R^{d\times n}</annotation></semantics></math>, such that <math alttext="G:=DE" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p2.m3" intent=":literal"><semantics><mrow><mi>G</mi><mo>:=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G:=DE</annotation></semantics></math> approximates <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p2.m4" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> pointwise. <span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Elements of <math alttext="G" class="ltx_Math" display="inline" id="footnote13.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> can be computed pointwise by each pair of nodes: <math alttext="V\times V\ni(v_{1},v_{2})\mapsto G:={v_{1}}^{T}DE{v_{2}}\in R" class="ltx_Math" display="inline" id="footnote13.m2" intent=":literal"><semantics><mrow><mrow><mi>V</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>V</mi></mrow><mo>∋</mo><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">↦</mo><mi>G</mi><mo>:=</mo><mrow><mmultiscripts><mi>v</mi><mn>1</mn><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mn>2</mn></msub></mrow><mo>∈</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">V\times V\ni(v_{1},v_{2})\mapsto G:={v_{1}}^{T}DE{v_{2}}\in R</annotation></semantics></math>.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p3">
<p class="ltx_p">Assume <math alttext="\|G^{\prime}\|_{1,\infty}\leq 1" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m1" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><msup><mi>G</mi><mo>′</mo></msup><mo stretchy="false">‖</mo></mrow><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">∞</mi></mrow></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|G^{\prime}\|_{1,\infty}\leq 1</annotation></semantics></math>. An application of the Johnson-Lindenstrauss lemma shows that the following bound holds in the infinity norm: <math alttext="\|G^{\prime}-G\|_{\max}=O(\sqrt{\log n\ /\ d})" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m2" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>−</mo><mi>G</mi></mrow><mo stretchy="false">‖</mo></mrow><mi>max</mi></msub><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\|G^{\prime}-G\|_{\max}=O(\sqrt{\log n\ /\ d})</annotation></semantics></math> (cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Udell and Townsend, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib103" title="">2019</a>; Budzinskiy, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib18" title="">2025</a>)</cite>). Then, for <math alttext="z\in R^{n}=R^{|V|}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m3" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup><mo>=</mo><msup><mi>R</mi><mrow><mo stretchy="false">|</mo><mi>V</mi><mo stretchy="false">|</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z\in R^{n}=R^{|V|}</annotation></semantics></math> with <math alttext="\|z\|_{1}\leq 1" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m4" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>z</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|z\|_{1}\leq 1</annotation></semantics></math>, we have:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\|G^{\prime}z-Gz\|_{+\infty}=O(\sqrt{\log n\ /\ d})" class="ltx_Math" display="block" id="S5.E11.m1" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>−</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow></msub><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\|G^{\prime}z-Gz\|_{+\infty}=O(\sqrt{\log n\ /\ d})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, no similar bound holds for <math alttext="\|G^{\prime}z-Gz\|_{2}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m5" intent=":literal"><semantics><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>−</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|G^{\prime}z-Gz\|_{2}</annotation></semantics></math>. Even for ‘simple’ scenarios like the identity transformation <math alttext="G^{\prime}=I_{n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m6" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>=</mo><msub><mi>I</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">G^{\prime}=I_{n}</annotation></semantics></math>, the best low-rank approximation admits O(1) additive error in the L2-norm for almost all inputs, and even greater distortion (approaching <math alttext="\sqrt{n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p3.m7" intent=":literal"><semantics><msqrt><mi>n</mi></msqrt><annotation encoding="application/x-tex">\sqrt{n}</annotation></semantics></math>) may appear in the L1-norm.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p4">
<p class="ltx_p">This makes the low-rank operation useful for determining affinity of pairs of coordinates in dimension <math alttext="n" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p4.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, but more problematic as a vector transformation function. However, the ReLU-lowrank mechanism (Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E10" title="In Definition of ReLU-lowrank. ‣ 5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">10</span></a>) is able to suppress a part of the noise of the linear low-rank map, allowing to approximate a sufficiently broad class of non-linear operations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Expressiveness of ReLU-lowrank for Markov chain propagation.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p">We will consider positive inputs <math alttext="z\in R^{+n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.m1" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mrow><mo>+</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">z\in R^{+n}</annotation></semantics></math>, focusing on sparse vectors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p2">
<p class="ltx_p">One important case concerns approximating a Markov chain transformation <math alttext="z\mapsto G^{\prime}z" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m1" intent=":literal"><semantics><mrow><mi>z</mi><mo stretchy="false">↦</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><annotation encoding="application/x-tex">z\mapsto G^{\prime}z</annotation></semantics></math>, for some <math alttext="G^{\prime}\in R+^{n\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m2" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>∈</mo><mrow><mi>R</mi><msup><mo>+</mo><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">G^{\prime}\in R+^{n\times n}</annotation></semantics></math>. For such a transformation in the positive orthant, adding the ReLU operation to the linear map does not change anything directly, <math alttext="G^{\prime}z=\left(G^{\prime}z\right)^{+}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m3" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">G^{\prime}z=\left(G^{\prime}z\right)^{+}</annotation></semantics></math>. However, when considering a low-rank matrix <math alttext="G" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, the non-linear transformation <math alttext="\left(Gz\right)^{+}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m5" intent=":literal"><semantics><msup><mrow><mo>(</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup><annotation encoding="application/x-tex">\left(Gz\right)^{+}</annotation></semantics></math> can provide a closer approximation of <math alttext="G^{\prime}z" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m6" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">G^{\prime}z</annotation></semantics></math> for some classes of input vectors <math alttext="z" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m7" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>, than the low-rank linear operation <math alttext="Gz" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p2.m8" intent=":literal"><semantics><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">Gz</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p3">
<p class="ltx_p">We start with the following illustrative example.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 5</span></span><span class="ltx_text ltx_font_bold"> </span>(propagating a Markov chain)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="Thmclaim5.p1.m1" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> be the random walk matrix of a directed graph with out-degree <math alttext="r" class="ltx_Math" display="inline" id="Thmclaim5.p1.m2" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> (i.e., a stochastic matrix with <math alttext="r" class="ltx_Math" display="inline" id="Thmclaim5.p1.m3" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> non-zero entries of <math alttext="1/r" class="ltx_Math" display="inline" id="Thmclaim5.p1.m4" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">1/r</annotation></semantics></math> in each row), and let <math alttext="v\in V" class="ltx_Math" display="inline" id="Thmclaim5.p1.m5" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v\in V</annotation></semantics></math> be a node (basis vector), <math alttext="\|v\|_{1}=\|v\|_{2}=1" class="ltx_Math" display="inline" id="Thmclaim5.p1.m6" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>v</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>=</mo><msub><mrow><mo stretchy="false">‖</mo><mi>v</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|v\|_{1}=\|v\|_{2}=1</annotation></semantics></math>. Then, for any <math alttext="\varepsilon&gt;0" class="ltx_Math" display="inline" id="Thmclaim5.p1.m7" intent=":literal"><semantics><mrow><mi>ε</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\varepsilon&gt;0</annotation></semantics></math>, there exists <math alttext="d=O(r^{3}\log n/\varepsilon)" class="ltx_Math" display="inline" id="Thmclaim5.p1.m8" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>r</mi><mn>3</mn></msup><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo>/</mo><mi>ε</mi></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">d=O(r^{3}\log n/\varepsilon)</annotation></semantics></math> such that for some matrices <math alttext="D\in R^{n\times d},E\in R^{d\times n}" class="ltx_Math" display="inline" id="Thmclaim5.p1.m9" intent=":literal"><semantics><mrow><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">D\in R^{n\times d},E\in R^{d\times n}</annotation></semantics></math>, we have <math alttext="\|G^{\prime}v-f_{DE}(v)\|_{1}=O(\varepsilon)" class="ltx_Math" display="inline" id="Thmclaim5.p1.m10" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>−</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\|G^{\prime}v-f_{DE}(v)\|_{1}=O(\varepsilon)</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof (sketch).</h6>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p4">
<p class="ltx_p">Let <math alttext="D^{*}\in R^{n\times(d-1)}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m1" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">D^{*}\in R^{n\times(d-1)}</annotation></semantics></math>, <math alttext="E^{*}\in R^{(d-1)\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m2" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{*}\in R^{(d-1)\times n}</annotation></semantics></math> denote matrices <math alttext="D" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m3" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, <math alttext="E" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m4" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> restricted to all but the last coordinate in dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m5" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>. Pick <math alttext="D,E" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m6" intent=":literal"><semantics><mrow><mi>D</mi><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">D,E</annotation></semantics></math> so that <math alttext="\|G^{\prime}v-D^{*}E^{*}v\|_{\infty}&lt;\varepsilon^{*}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m7" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>−</mo><mrow><msup><mi>D</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi mathvariant="normal">∞</mi></msub><mo>&lt;</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\|G^{\prime}v-D^{*}E^{*}v\|_{\infty}&lt;\varepsilon^{*}</annotation></semantics></math>, where <math alttext="\varepsilon^{*}=\varepsilon/r" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m8" intent=":literal"><semantics><mrow><msup><mi>ε</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>ε</mi><mo>/</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">\varepsilon^{*}=\varepsilon/r</annotation></semantics></math>, following Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E11" title="In Error of low-rank approximation (without ReLU). ‣ 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">11</span></a>) (we have <math alttext="\|G^{\prime}\|_{1,\infty}\leq 1" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m9" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><msup><mi>G</mi><mo>′</mo></msup><mo stretchy="false">‖</mo></mrow><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">∞</mi></mrow></msub><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\|G^{\prime}\|_{1,\infty}\leq 1</annotation></semantics></math> by stochasticity of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m10" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>). Further, set a fixed bias, placing <math alttext="1" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m11" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> on all entries of the last coordinate in dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m12" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> of <math alttext="D" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m13" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, and <math alttext="-\varepsilon^{*}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m14" intent=":literal"><semantics><mrow><mo>−</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">-\varepsilon^{*}</annotation></semantics></math> on all entries of the corresponding last coordinate in dimension <math alttext="d" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m15" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> of <math alttext="E" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m16" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math>. Taking into account this bias, we now have
<math alttext="\|(G^{\prime}v-\varepsilon^{*}\mathbf{1})-DEv\|_{\infty}&lt;\varepsilon^{*}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p4.m17" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>−</mo><mrow><msup><mi>ε</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mn>𝟏</mn></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>−</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></mrow><mo stretchy="false">‖</mo></mrow><mi mathvariant="normal">∞</mi></msub><mo>&lt;</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\|(G^{\prime}v-\varepsilon^{*}\mathbf{1})-DEv\|_{\infty}&lt;\varepsilon^{*}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p5">
<p class="ltx_p">For all coordinates <math alttext="v_{j}\in V" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m1" intent=":literal"><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v_{j}\in V</annotation></semantics></math> such that <math alttext="{v_{j}}^{T}G^{\prime}v=0" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m2" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{v_{j}}^{T}G^{\prime}v=0</annotation></semantics></math>, we now have <math alttext="{v_{j}}^{T}DEv&lt;0" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m3" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{v_{j}}^{T}DEv&lt;0</annotation></semantics></math>, hence also <math alttext="{v_{j}}^{T}f_{DE}(v)=0" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m4" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{v_{j}}^{T}f_{DE}(v)=0</annotation></semantics></math>. For all other coordinates <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m5" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math>, we have <math alttext="{v_{j}}^{T}G^{\prime}v=1/r" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m6" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">{v_{j}}^{T}G^{\prime}v=1/r</annotation></semantics></math>, and <math alttext="1/r-2\varepsilon^{*}&lt;{v_{j}}^{T}f_{DE}(v)\leq 1/r" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m7" intent=":literal"><semantics><mrow><mrow><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow></mrow><mo>&lt;</mo><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">1/r-2\varepsilon^{*}&lt;{v_{j}}^{T}f_{DE}(v)\leq 1/r</annotation></semantics></math>. Thus, <math alttext="\|G^{\prime}v-f_{DE}(v)\|_{1}\leq 2\varepsilon^{*}r" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p5.m8" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo>−</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>≤</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>ε</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">\|G^{\prime}v-f_{DE}(v)\|_{1}\leq 2\varepsilon^{*}r</annotation></semantics></math>, and the claim follows.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>As a point of elegance, we note that in this proof, <math alttext="{v_{j}}^{T}f_{DE}(v)\leq 1/r" class="ltx_Math" display="inline" id="footnote14.m1" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>v</mi><mi>j</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">{v_{j}}^{T}f_{DE}(v)\leq 1/r</annotation></semantics></math>, so <math alttext="f_{DE}(v)" class="ltx_Math" display="inline" id="footnote14.m2" intent=":literal"><semantics><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{DE}(v)</annotation></semantics></math> was not an <em class="ltx_emph ltx_font_italic">unbiased</em> estimator of <math alttext="G^{\prime}v" class="ltx_Math" display="inline" id="footnote14.m3" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">G^{\prime}v</annotation></semantics></math>. This is easily fixed in the first-order by introducing a global multiplicative bias of <math alttext="(1+\varepsilon^{*})" class="ltx_Math" display="inline" id="footnote14.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1+\varepsilon^{*})</annotation></semantics></math> to the approximation, for example, substituting: <math alttext="(1+\varepsilon^{*})D\mapsto D" class="ltx_Math" display="inline" id="footnote14.m5" intent=":literal"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msup><mi>ε</mi><mo>∗</mo></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow><mo stretchy="false">↦</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">(1+\varepsilon^{*})D\mapsto D</annotation></semantics></math>.</span></span></span>
∎</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p6">
<p class="ltx_p">The above observation shows how ReLU-lowrank deals with one specific class of graph affinity functions (random walks of adjacency of sparse graphs), for transformations of vectors which are nodes in our distinguished basis. We use this example as it is the simplest case which exhibits the benefit of threshold non-linearity: for basis vectors, the operation <math alttext="f_{DE}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p6.m1" intent=":literal"><semantics><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><annotation encoding="application/x-tex">f_{DE}</annotation></semantics></math> captures a basic propagation effect which is well known (in general) to require a <em class="ltx_emph ltx_font_italic">full-rank</em> matrix <math alttext="G^{\prime}\in R^{n\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p6.m2" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">G^{\prime}\in R^{n\times n}</annotation></semantics></math> if relying only on linear operations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Propagation and reinforcement of signal.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p">The same thresholding approach, as discussed for Markov chains, turns out to be applicable to a wider class of signal propagation dynamics. It consists in first obtaining a positive-valued signal with heavy random noise, then applying a negative bias, and finally using the ReLU gate to act as a noise threshold.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p2">
<p class="ltx_p">Any linear function <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m1" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> can be represented with a hidden layer of <math alttext="s\leq n^{2}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m2" intent=":literal"><semantics><mrow><mi>s</mi><mo>≤</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">s\leq n^{2}</annotation></semantics></math> nodes, through two matrices <math alttext="D^{\prime}\in R^{+n\times s}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m3" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mo>+</mo><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>s</mi></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">D^{\prime}\in R^{+n\times s}</annotation></semantics></math> and <math alttext="E^{\prime}\in R^{+n\times s}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m4" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mo>+</mo><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>s</mi></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">E^{\prime}\in R^{+n\times s}</annotation></semantics></math>, such that:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex26">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G^{\prime}=D^{\prime}E^{\prime}." class="ltx_Math" display="block" id="S5.Ex26.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">G^{\prime}=D^{\prime}E^{\prime}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The above holds in general, and we will refer to such a representation of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m5" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> as having a sparse hidden (synaptic) layer. We will consider now the question of expressing non-negative functions, <math alttext="G^{\prime}\in(R^{+})^{n\times n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m6" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">G^{\prime}\in(R^{+})^{n\times n}</annotation></semantics></math>. An example of a valid representation of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m7" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is given through a node-edge incidence representation, <math alttext="D^{\prime}_{i,(i-1)n+j}=E^{\prime}_{(i-1)n+j,j}=\sqrt{G^{\prime}_{ij}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m8" intent=":literal"><semantics><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo>+</mo><mi>j</mi></mrow></mrow><mo>′</mo></msubsup><mo>=</mo><msubsup><mi>E</mi><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo>+</mo><mi>j</mi></mrow><mo>,</mo><mi>j</mi></mrow><mo>′</mo></msubsup><mo>=</mo><msqrt><msubsup><mi>G</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mo>′</mo></msubsup></msqrt></mrow><annotation encoding="application/x-tex">D^{\prime}_{i,(i-1)n+j}=E^{\prime}_{(i-1)n+j,j}=\sqrt{G^{\prime}_{ij}}</annotation></semantics></math>, but usually this representation is not optimal in terms of the number of non-zero entries of <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m9" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math> and <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p2.m10" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">E^{\prime}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p3">
<p class="ltx_p">In general, any low-rank approximation of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> can be equivalently expressed as <math alttext="G\approx D^{\prime}P_{D}P_{E}^{T}E^{\prime}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m2" intent=":literal"><semantics><mrow><mi>G</mi><mo>≈</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>P</mi><mi>D</mi></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>P</mi><mi>E</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow></mrow><annotation encoding="application/x-tex">G\approx D^{\prime}P_{D}P_{E}^{T}E^{\prime}</annotation></semantics></math>, for some two matrices, <math alttext="P_{D},P_{E}\in R^{s\times d}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m3" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mi>D</mi></msub><mo>,</mo><msub><mi>P</mi><mi>E</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">P_{D},P_{E}\in R^{s\times d}</annotation></semantics></math>. We will consider the most common class of low-rank approximations obtained by taking <math alttext="P_{D}=P_{E}=P\sim\mathcal{N}(0,1)^{s\times d}/\sqrt{d}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m4" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>D</mi></msub><mo>=</mo><msub><mi>P</mi><mi>E</mi></msub><mo>=</mo><mi>P</mi><mo>∼</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><mo>/</mo><msqrt><mi>d</mi></msqrt></mrow></mrow><annotation encoding="application/x-tex">P_{D}=P_{E}=P\sim\mathcal{N}(0,1)^{s\times d}/\sqrt{d}</annotation></semantics></math>. Consider a vector <math alttext="z" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m5" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> passing through the ReLU-lowrank operation, and the following vectors <math alttext="u\in R^{s}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m6" intent=":literal"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>R</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">u\in R^{s}</annotation></semantics></math>, <math alttext="w\in R^{n}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m7" intent=":literal"><semantics><mrow><mi>w</mi><mo>∈</mo><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">w\in R^{n}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx17">
<tbody id="S5.Ex27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle u" class="ltx_Math" display="inline" id="S5.Ex27.m1" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">\displaystyle u</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle:=E^{\prime}z" class="ltx_Math" display="inline" id="S5.Ex27.m2" intent=":literal"><semantics><mrow><mi></mi><mo>:=</mo><mrow><msup><mi>E</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><annotation encoding="application/x-tex">\displaystyle:=E^{\prime}z</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S5.Ex28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle w" class="ltx_Math" display="inline" id="S5.Ex28.m1" intent=":literal"><semantics><mi>w</mi><annotation encoding="application/x-tex">\displaystyle w</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle:=D^{\prime}PP^{T}u" class="ltx_Math" display="inline" id="S5.Ex28.m2" intent=":literal"><semantics><mrow><mi></mi><mo>:=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>u</mi></mrow></mrow><annotation encoding="application/x-tex">\displaystyle:=D^{\prime}PP^{T}u</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">If <math alttext="v_{i}^{T}z" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m8" intent=":literal"><semantics><mrow><msubsup><mi>v</mi><mi>i</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">v_{i}^{T}z</annotation></semantics></math> has the interpretation of a signal being sent by node <math alttext="v_{i}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m9" intent=":literal"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding="application/x-tex">v_{i}</annotation></semantics></math>, then <math alttext="u" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m10" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is the encoded message being passed through the hidden layer of the network, and <math alttext="v_{j}^{T}w" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m11" intent=":literal"><semantics><mrow><msubsup><mi>v</mi><mi>j</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">v_{j}^{T}w</annotation></semantics></math> is the message received by node <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p3.m12" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Modularity in BDH-GPU signal propagation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p">We are now ready to capture the essence of the signal propagation and reinforcement capability of the ReLU-lowrank system. To describe the conditions under which a neuron is able to decide whether it should, or should not activate. By a standard analysis of independent Gaussians, we have the following probabilistic statement, under random choice of <math alttext="P" class="ltx_Math" display="inline" id="S5.SS4.p1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 6</span></span><span class="ltx_text ltx_font_bold"> </span>(selective neuron activation)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Suppose that the signal of <math alttext="u" class="ltx_Math" display="inline" id="Thmclaim6.p1.m1" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is uniformly concentrated on a set of nodes <math alttext="A" class="ltx_Math" display="inline" id="Thmclaim6.p1.m2" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> of the hidden layer, i.e., for some subset <math alttext="A" class="ltx_Math" display="inline" id="Thmclaim6.p1.m3" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> of indexes of the hidden layer, we have <math alttext="u_{\bar{\alpha}}=0" class="ltx_Math" display="inline" id="Thmclaim6.p1.m4" intent=":literal"><semantics><mrow><msub><mi>u</mi><mover accent="true"><mi>α</mi><mo>¯</mo></mover></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">u_{\bar{\alpha}}=0</annotation></semantics></math> for <math alttext="\bar{\alpha}\not\in A" class="ltx_Math" display="inline" id="Thmclaim6.p1.m5" intent=":literal"><semantics><mrow><mover accent="true"><mi>α</mi><mo>¯</mo></mover><mo>∉</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\bar{\alpha}\not\in A</annotation></semantics></math>, and <math alttext="u_{\bar{\alpha}}\in[\frac{1-\kappa}{\sqrt{|A|}},\frac{1+\kappa}{\sqrt{|A|}}]" class="ltx_Math" display="inline" id="Thmclaim6.p1.m6" intent=":literal"><semantics><mrow><msub><mi>u</mi><mover accent="true"><mi>α</mi><mo>¯</mo></mover></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><msqrt><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo></mrow></msqrt></mfrac><mo>,</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><msqrt><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo></mrow></msqrt></mfrac><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">u_{\bar{\alpha}}\in[\frac{1-\kappa}{\sqrt{|A|}},\frac{1+\kappa}{\sqrt{|A|}}]</annotation></semantics></math> for <math alttext="\bar{\alpha}\in A" class="ltx_Math" display="inline" id="Thmclaim6.p1.m7" intent=":literal"><semantics><mrow><mover accent="true"><mi>α</mi><mo>¯</mo></mover><mo>∈</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\bar{\alpha}\in A</annotation></semantics></math>, so that <math alttext="\|u\|_{2}\in[1-\kappa,1+\kappa]" class="ltx_Math" display="inline" id="Thmclaim6.p1.m8" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mi>u</mi><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><mo>,</mo><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\|u\|_{2}\in[1-\kappa,1+\kappa]</annotation></semantics></math> for some small constant <math alttext="\kappa\geq 0" class="ltx_Math" display="inline" id="Thmclaim6.p1.m9" intent=":literal"><semantics><mrow><mi>κ</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\kappa\geq 0</annotation></semantics></math>. Suppose each node <math alttext="v_{j}\in V" class="ltx_Math" display="inline" id="Thmclaim6.p1.m10" intent=":literal"><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v_{j}\in V</annotation></semantics></math> is connected in <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m11" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math> to some set of nodes <math alttext="B_{j}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m12" intent=":literal"><semantics><msub><mi>B</mi><mi>j</mi></msub><annotation encoding="application/x-tex">B_{j}</annotation></semantics></math> in the hidden layer, <math alttext="B_{j}=\{\bar{\beta}:D^{\prime}_{j,\bar{\beta}}\neq 0\}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m13" intent=":literal"><semantics><mrow><msub><mi>B</mi><mi>j</mi></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mover accent="true"><mi>β</mi><mo>¯</mo></mover><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mi>D</mi><mrow><mi>j</mi><mo>,</mo><mover accent="true"><mi>β</mi><mo>¯</mo></mover></mrow><mo>′</mo></msubsup><mo>≠</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">B_{j}=\{\bar{\beta}:D^{\prime}_{j,\bar{\beta}}\neq 0\}</annotation></semantics></math>, and let these connections weight be drawn uniformly <math alttext="D^{\prime}_{j,\bar{\beta}}\in[\frac{1-\kappa}{\sqrt{|B_{j}|}},\frac{1+\kappa}{\sqrt{|B_{j}|}}]" class="ltx_Math" display="inline" id="Thmclaim6.p1.m14" intent=":literal"><semantics><mrow><msubsup><mi>D</mi><mrow><mi>j</mi><mo>,</mo><mover accent="true"><mi>β</mi><mo>¯</mo></mover></mrow><mo>′</mo></msubsup><mo>∈</mo><mrow><mo stretchy="false">[</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><msqrt><mrow><mo stretchy="false">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></msqrt></mfrac><mo>,</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><msqrt><mrow><mo stretchy="false">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></msqrt></mfrac><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">D^{\prime}_{j,\bar{\beta}}\in[\frac{1-\kappa}{\sqrt{|B_{j}|}},\frac{1+\kappa}{\sqrt{|B_{j}|}}]</annotation></semantics></math> for <math alttext="\bar{\beta}\in B_{j}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m15" intent=":literal"><semantics><mrow><mover accent="true"><mi>β</mi><mo>¯</mo></mover><mo>∈</mo><msub><mi>B</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\bar{\beta}\in B_{j}</annotation></semantics></math>. Let <math alttext="C_{j}=A\cap B_{j}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m16" intent=":literal"><semantics><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>A</mi><mo>∩</mo><msub><mi>B</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">C_{j}=A\cap B_{j}</annotation></semantics></math>. Define the ratio:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex29">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\varrho:=\left.\sqrt{\frac{|C_{j}|}{|A|}\cdot\frac{|C_{j}|}{|B_{j}|}}.\right." class="ltx_Math" display="block" id="S5.Ex29.m1" intent=":literal"><semantics><mrow><mrow><mi>ϱ</mi><mo>:=</mo><msqrt><mrow><mfrac><mrow><mo stretchy="false">|</mo><msub><mi>C</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo></mrow></mfrac><mo lspace="0.222em" rspace="0.222em">⋅</mo><mfrac><mrow><mo stretchy="false">|</mo><msub><mi>C</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></msqrt></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\varrho:=\left.\sqrt{\frac{|C_{j}|}{|A|}\cdot\frac{|C_{j}|}{|B_{j}|}}.\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Then, there exists an absolute constant <math alttext="c&gt;0" class="ltx_Math" display="inline" id="Thmclaim6.p1.m17" intent=":literal"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c&gt;0</annotation></semantics></math>, such that for any value of <math alttext="w_{j}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m18" intent=":literal"><semantics><msub><mi>w</mi><mi>j</mi></msub><annotation encoding="application/x-tex">w_{j}</annotation></semantics></math> (where we recall that <math alttext="w:=D^{\prime}PP^{T}u" class="ltx_Math" display="inline" id="Thmclaim6.p1.m19" intent=":literal"><semantics><mrow><mi>w</mi><mo>:=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>u</mi></mrow></mrow><annotation encoding="application/x-tex">w:=D^{\prime}PP^{T}u</annotation></semantics></math>), we have:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S5.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Pr\left[w_{j}\geq(1-\kappa)^{2}\varrho-c\sqrt{\log n\ /\ d}\right]=1-O(1/n)\quad\textrm{and}\quad\Pr\left[w_{j}\leq(1+\kappa)^{2}\varrho+c\sqrt{\log n\ /\ d}\right]=1-O(1/n)." class="ltx_Math" display="block" id="S5.E12.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo>[</mo><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>≥</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>ϱ</mi></mrow><mo>−</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mn>1</mn><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mtext class="ltx_mathvariant_italic">and</mtext></mrow></mrow><mspace style="width:1.167em;" width="1.167em"></mspace><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo>[</mo><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>≤</mo><mrow><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>ϱ</mi></mrow><mo>+</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Pr\left[w_{j}\geq(1-\kappa)^{2}\varrho-c\sqrt{\log n\ /\ d}\right]=1-O(1/n)\quad\textrm{and}\quad\Pr\left[w_{j}\leq(1+\kappa)^{2}\varrho+c\sqrt{\log n\ /\ d}\right]=1-O(1/n).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Thus, the value of <math alttext="w_{j}" class="ltx_Math" display="inline" id="Thmclaim6.p1.m20" intent=":literal"><semantics><msub><mi>w</mi><mi>j</mi></msub><annotation encoding="application/x-tex">w_{j}</annotation></semantics></math> can be used by a neuron to obtain an estimation of <math alttext="\varrho" class="ltx_Math" display="inline" id="Thmclaim6.p1.m21" intent=":literal"><semantics><mi>ϱ</mi><annotation encoding="application/x-tex">\varrho</annotation></semantics></math>, and apply a threshold to activate accordingly.
∎</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p">Observe that <math alttext="w_{j}={(P^{T}D^{\prime}_{j,\cdot})}^{T}{(P^{T}u)}" class="ltx_Math" display="inline" id="S5.SS4.p2.m1" intent=":literal"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><msup><mi>P</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>D</mi><mrow><mi>j</mi><mo rspace="0em">,</mo><mo lspace="0em">⋅</mo></mrow><mo>′</mo></msubsup></mrow><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>P</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>u</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">w_{j}={(P^{T}D^{\prime}_{j,\cdot})}^{T}{(P^{T}u)}</annotation></semantics></math>. As <math alttext="\|D^{\prime}_{j,\cdot}\|_{2}\in[1-\kappa,1+\kappa]" class="ltx_Math" display="inline" id="S5.SS4.p2.m2" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>D</mi><mrow><mi>j</mi><mo rspace="0em">,</mo><mo lspace="0em">⋅</mo></mrow><mo>′</mo></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>∈</mo><mrow><mo stretchy="false">[</mo><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><mo>,</mo><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\|D^{\prime}_{j,\cdot}\|_{2}\in[1-\kappa,1+\kappa]</annotation></semantics></math>, a standard application of Johnson-Lindenstrauss to vector inner products gives
<math alttext="\Pr\left[|w_{j}-{D^{\prime}_{j,\cdot}}^{T}{u}|\leq c\sqrt{\log n\ /\ d}\right]=1-O(1/n)" class="ltx_Math" display="inline" id="S5.SS4.p2.m3" intent=":literal"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mo stretchy="false">|</mo><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>−</mo><mrow><mmultiscripts><mi>D</mi><mrow><mi>j</mi><mo rspace="0em">,</mo><mo lspace="0em">⋅</mo></mrow><mo>′</mo><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>u</mi></mrow></mrow><mo stretchy="false">|</mo></mrow><mo>≤</mo><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pr\left[|w_{j}-{D^{\prime}_{j,\cdot}}^{T}{u}|\leq c\sqrt{\log n\ /\ d}\right]=1-O(1/n)</annotation></semantics></math> for <math alttext="c" class="ltx_Math" display="inline" id="S5.SS4.p2.m4" intent=":literal"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math> large enough. Since <math alttext="{D^{\prime}_{j,\cdot}}^{T}{u}\in[(1-\kappa)^{2}\rho,(1+\kappa)^{2}\rho]" class="ltx_Math" display="inline" id="S5.SS4.p2.m5" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>D</mi><mrow><mi>j</mi><mo rspace="0em">,</mo><mo lspace="0em">⋅</mo></mrow><mo>′</mo><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>u</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>κ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>ρ</mi></mrow><mo>,</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mi>κ</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>ρ</mi></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">{D^{\prime}_{j,\cdot}}^{T}{u}\in[(1-\kappa)^{2}\rho,(1+\kappa)^{2}\rho]</annotation></semantics></math>, the claim follows.
∎</p>
</div>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S5.F8.g1" src="x6.png" width="475"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The ReLU-lowrank feedforward network of BDH-GPU allows neurons to activate when triggered by activation signals in its own community. (a) Illustration of the selective neuron activation pattern in the proof of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim6" title="Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>, showing the activation decision of node <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.F8.m9" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math> (left) based on active set <math alttext="A" class="ltx_Math" display="inline" id="S5.F8.m10" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> in the sparse hidden layer. (b) Illustration of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E12" title="In Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">12</span></a>) showing the relationship between sizes of sets in the sparse hidden layer: active set <math alttext="A" class="ltx_Math" display="inline" id="S5.F8.m11" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, set <math alttext="B_{j}" class="ltx_Math" display="inline" id="S5.F8.m12" intent=":literal"><semantics><msub><mi>B</mi><mi>j</mi></msub><annotation encoding="application/x-tex">B_{j}</annotation></semantics></math> connected to neuron <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.F8.m13" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math>, and the intersection <math alttext="C_{j}=A\cap B_{j}" class="ltx_Math" display="inline" id="S5.F8.m14" intent=":literal"><semantics><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>A</mi><mo>∩</mo><msub><mi>B</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">C_{j}=A\cap B_{j}</annotation></semantics></math>: neuron <math alttext="v_{j_{1}}" class="ltx_Math" display="inline" id="S5.F8.m15" intent=":literal"><semantics><msub><mi>v</mi><msub><mi>j</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">v_{j_{1}}</annotation></semantics></math> becomes active, but neuron <math alttext="v_{j_{2}}" class="ltx_Math" display="inline" id="S5.F8.m16" intent=":literal"><semantics><msub><mi>v</mi><msub><mi>j</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">v_{j_{2}}</annotation></semantics></math> does not.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p">The ReLU-lowrank operation <math alttext="f_{DE}" class="ltx_Math" display="inline" id="S5.SS4.p3.m1" intent=":literal"><semantics><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><annotation encoding="application/x-tex">f_{DE}</annotation></semantics></math>, after adding appropriate negative bias, can thus be used to propagate positive affinity functions <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p3.m2" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> on input vectors, performing the following form of thresholding: neurons <math alttext="j" class="ltx_Math" display="inline" id="S5.SS4.p3.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> in the output layer individually compute a form of local “F-score” <math alttext="\varrho" class="ltx_Math" display="inline" id="S5.SS4.p3.m4" intent=":literal"><semantics><mi>ϱ</mi><annotation encoding="application/x-tex">\varrho</annotation></semantics></math> given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E12" title="In Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">12</span></a>) of the activation of the positive sparse hidden layer, and decide based on it whether they are good match for the output activation; if the threshold condition on <math alttext="\varrho" class="ltx_Math" display="inline" id="S5.SS4.p3.m5" intent=":literal"><semantics><mi>ϱ</mi><annotation encoding="application/x-tex">\varrho</annotation></semantics></math> is not met, the neuron <math alttext="j" class="ltx_Math" display="inline" id="S5.SS4.p3.m6" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> does not activate in the output vector (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F8" title="Figure 8 ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a> for an illustration).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p4">
<p class="ltx_p">Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E12" title="In Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">12</span></a>) naturally coincides with a pattern of communication within network graphs <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p4.m1" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> admitting positive Newman modularity <cite class="ltx_cite ltx_citemacro_citep">(Newman, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib79" title="">2006</a>)</cite>, allowing nodes <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS4.p4.m2" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math> to correctly receive messages <math alttext="u" class="ltx_Math" display="inline" id="S5.SS4.p4.m3" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> which in the hidden layer primarily reached a denser cluster of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p4.m4" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> containing <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS4.p4.m5" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math>. For a specific illustration, let <math alttext="H" class="ltx_Math" display="inline" id="S5.SS4.p4.m6" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> be an undirected <math alttext="k" class="ltx_Math" display="inline" id="S5.SS4.p4.m7" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-block stochastic block model (SBM) network <cite class="ltx_cite ltx_citemacro_citep">(Karrer and Newman, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib53" title="">2011</a>)</cite> with <math alttext="k\in\mathbb{N}" class="ltx_Math" display="inline" id="S5.SS4.p4.m8" intent=":literal"><semantics><mrow><mi>k</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">k\in\mathbb{N}</annotation></semantics></math> blocks of <math alttext="n/k" class="ltx_Math" display="inline" id="S5.SS4.p4.m9" intent=":literal"><semantics><mrow><mi>n</mi><mo>/</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n/k</annotation></semantics></math> nodes each, in-block edge density <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p4.m10" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and out-of-block edge density <math alttext="q&lt;p" class="ltx_Math" display="inline" id="S5.SS4.p4.m11" intent=":literal"><semantics><mrow><mi>q</mi><mo>&lt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q&lt;p</annotation></semantics></math>. We put <math alttext="G^{\prime}:=D^{\prime}E^{\prime}=H^{2}" class="ltx_Math" display="inline" id="S5.SS4.p4.m12" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>:=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><mo>=</mo><msup><mi>H</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">G^{\prime}:=D^{\prime}E^{\prime}=H^{2}</annotation></semantics></math>, i.e., the first connection layer of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p4.m13" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is <math alttext="E^{\prime}=H" class="ltx_Math" display="inline" id="S5.SS4.p4.m14" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">E^{\prime}=H</annotation></semantics></math> and the second connection layer is also <math alttext="D^{\prime}=H" class="ltx_Math" display="inline" id="S5.SS4.p4.m15" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">D^{\prime}=H</annotation></semantics></math>. Suppose that <math alttext="H" class="ltx_Math" display="inline" id="S5.SS4.p4.m16" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> is a random SBM graph with positive Newman modularity separated from <math alttext="0" class="ltx_Math" display="inline" id="S5.SS4.p4.m17" intent=":literal"><mn>0</mn></math>, i.e., let <math alttext="\mu=\frac{k-1}{k}\frac{p-q}{p+(k-1)q}&gt;0" class="ltx_Math" display="inline" id="S5.SS4.p4.m18" intent=":literal"><semantics><mrow><mi>μ</mi><mo>=</mo><mrow><mfrac><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><mi>k</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mi>p</mi><mo>−</mo><mi>q</mi></mrow><mrow><mi>p</mi><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>q</mi></mrow></mrow></mfrac></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu=\frac{k-1}{k}\frac{p-q}{p+(k-1)q}&gt;0</annotation></semantics></math>. Following Claim (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim6" title="Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>) with <math alttext="\kappa=0" class="ltx_Math" display="inline" id="S5.SS4.p4.m19" intent=":literal"><semantics><mrow><mi>κ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\kappa=0</annotation></semantics></math>, we can find a ReLU-lowrank representation <math alttext="G" class="ltx_Math" display="inline" id="S5.SS4.p4.m20" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> to achieve a communication scheme on <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p4.m21" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>, such that a message sent from one node <math alttext="z=v_{i}" class="ltx_Math" display="inline" id="S5.SS4.p4.m22" intent=":literal"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z=v_{i}</annotation></semantics></math> activates a node <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS4.p4.m23" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math> when <math alttext="i" class="ltx_Math" display="inline" id="S5.SS4.p4.m24" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S5.SS4.p4.m25" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> are in the same block with probability <math alttext="1-O(1/n)" class="ltx_Math" display="inline" id="S5.SS4.p4.m26" intent=":literal"><semantics><mrow><mn>1</mn><mo>−</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">1-O(1/n)</annotation></semantics></math>, and with probability <math alttext="O(1/n)" class="ltx_Math" display="inline" id="S5.SS4.p4.m27" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1/n)</annotation></semantics></math> otherwise, when <math alttext="\mu&gt;\frac{1}{p}\sqrt{\log n\ /\ d}" class="ltx_Math" display="inline" id="S5.SS4.p4.m28" intent=":literal"><semantics><mrow><mi>μ</mi><mo>&gt;</mo><mrow><mfrac><mn>1</mn><mi>p</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt></mrow></mrow><annotation encoding="application/x-tex">\mu&gt;\frac{1}{p}\sqrt{\log n\ /\ d}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<p class="ltx_p">We thus make the following intuitive observation.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation6">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 6</span></span><span class="ltx_text ltx_font_bold"> </span>(in-cluster signal reinforcement)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The ReLU-lowrank representation of <math alttext="\textrm{BDH-GPU}(n,d)" class="ltx_Math" display="inline" id="Thmobservation6.p1.m1" intent=":literal"><semantics><mrow><mtext class="ltx_mathvariant_italic">BDH-GPU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{BDH-GPU}(n,d)</annotation></semantics></math> is sufficient to represent in-cluster information spreading dynamics in models of graphs with constant in-cluster density and arbitrarily small positive modularity (such as the <math alttext="k" class="ltx_Math" display="inline" id="Thmobservation6.p1.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-cluster Stochastic Block Model) when <math alttext="d/\log n=\omega(1)" class="ltx_Math" display="inline" id="Thmobservation6.p1.m3" intent=":literal"><semantics><mrow><mrow><mi>d</mi><mo>/</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow><mo>=</mo><mrow><mi>ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">d/\log n=\omega(1)</annotation></semantics></math> is an arbitrarily slowly growing function.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p6">
<p class="ltx_p">While Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim6" title="Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a> and Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation6" title="Observation 6 (in-cluster signal reinforcement). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a> are made with reference to an almost-uniform distribution of signal <math alttext="u" class="ltx_Math" display="inline" id="S5.SS4.p6.m1" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> on the set of nodes of the middle layer, <math alttext="u" class="ltx_Math" display="inline" id="S5.SS4.p6.m2" intent=":literal"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> can have (and in practice does have) a distribution of density which is non-uniform, e.g., going across <math alttext="a=O(\log n)" class="ltx_Math" display="inline" id="S5.SS4.p6.m3" intent=":literal"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">a=O(\log n)</annotation></semantics></math> different clustering scales, with a <math alttext="(1/a)" class="ltx_Math" display="inline" id="S5.SS4.p6.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1/a)</annotation></semantics></math>-fraction of the signal represented at each scale. This allows neurons in the output layer to combine a smaller number of strong signals in its local cluster, with a larger number of weaker ones spread more globally. Such an approach coincides with the observed structure of the graph <math alttext="D^{\prime}E^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.p6.m5" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">D^{\prime}E^{\prime}</annotation></semantics></math>, discussed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Supermodularity on input perturbation.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px1.p1">
<p class="ltx_p">We clarify how the properties of function <math alttext="f_{DE}:(R^{+})^{n}\to(R^{+})^{n}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo stretchy="false">→</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f_{DE}:(R^{+})^{n}\to(R^{+})^{n}</annotation></semantics></math> relate to the previously discussed ability to make an input signal resonate “within a module” in a graph with hidden modular structure. First, note that <math alttext="f_{DE}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p1.m2" intent=":literal"><semantics><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><annotation encoding="application/x-tex">f_{DE}</annotation></semantics></math> <em class="ltx_emph ltx_font_italic">is a subadditive function, but is not submodular in general</em> with respect to the set of <math alttext="n" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> coordinates of its input vector. In some of the regimes in which it appears to be operating, locally <math alttext="f_{DE}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p1.m4" intent=":literal"><semantics><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><annotation encoding="application/x-tex">f_{DE}</annotation></semantics></math> exhibits a form of behavior opposite to submodularity, referred to as ‘supermodularity’, or ‘increasing returns’ of adding new coordinates to the input vector. This is already implicitly captured by Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim6" title="Claim 6 (selective neuron activation). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>, but we can consider a simpler example.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px1.p2">
<p class="ltx_p">Take a variant of the setting from Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim5" title="Claim 5 (propagating a Markov chain). ‣ Expressiveness of ReLU-lowrank for Markov chain propagation. ‣ 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a> with the same choice of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m1" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>, and let <math alttext="z\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">z\in(R^{+})^{n}</annotation></semantics></math> and biases of <math alttext="D" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m3" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> be chosen so that all coordinates of <math alttext="DEz" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">DEz</annotation></semantics></math> are approximately equal to <math alttext="-1.5/r\pm o(1)" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m5" intent=":literal"><semantics><mrow><mrow><mo>−</mo><mrow><mn>1.5</mn><mo>/</mo><mi>r</mi></mrow></mrow><mo>±</mo><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">-1.5/r\pm o(1)</annotation></semantics></math> (this can be done by choosing e.g. <math alttext="z_{j}=1/n" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m6" intent=":literal"><semantics><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">z_{j}=1/n</annotation></semantics></math>). Then, <math alttext="f_{DE}(z)=0" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f_{DE}(z)=0</annotation></semantics></math>, and for any <math alttext="v_{i},v_{j}\in V" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m8" intent=":literal"><semantics><mrow><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>j</mi></msub></mrow><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v_{i},v_{j}\in V</annotation></semantics></math>, <math alttext="f_{DE}(z+v_{i})=0" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m9" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo>+</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f_{DE}(z+v_{i})=0</annotation></semantics></math> a.s., <math alttext="f_{DE}(z+v_{j})=0" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m10" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo>+</mo><msub><mi>v</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f_{DE}(z+v_{j})=0</annotation></semantics></math> a.s., but <math alttext="f_{DE}(z+v_{i}+v_{j})" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m11" intent=":literal"><semantics><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo>+</mo><msub><mi>v</mi><mi>i</mi></msub><mo>+</mo><msub><mi>v</mi><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{DE}(z+v_{i}+v_{j})</annotation></semantics></math> has non-zero coordinates a.s. with values approximately <math alttext="1/{2r}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m12" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">1/{2r}</annotation></semantics></math>, for all nodes <math alttext="v_{k}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m13" intent=":literal"><semantics><msub><mi>v</mi><mi>k</mi></msub><annotation encoding="application/x-tex">v_{k}</annotation></semantics></math> which are common out-neighbor nodes of <math alttext="v_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m14" intent=":literal"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding="application/x-tex">v_{i}</annotation></semantics></math> and <math alttext="v_{j}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m15" intent=":literal"><semantics><msub><mi>v</mi><mi>j</mi></msub><annotation encoding="application/x-tex">v_{j}</annotation></semantics></math>, i.e., for all <math alttext="k" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m16" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> such that <math alttext="G^{\prime}(v_{i},v_{k})=G^{\prime}(v_{j},v_{k})=1/r" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m17" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo>,</mo><msub><mi>v</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">G^{\prime}(v_{i},v_{k})=G^{\prime}(v_{j},v_{k})=1/r</annotation></semantics></math>. This mechanism generalizes to finding common neighborhoods which have many connections to two given subsets of nodes, <math alttext="V_{a}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m18" intent=":literal"><semantics><msub><mi>V</mi><mi>a</mi></msub><annotation encoding="application/x-tex">V_{a}</annotation></semantics></math> and <math alttext="V_{b}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m19" intent=":literal"><semantics><msub><mi>V</mi><mi>b</mi></msub><annotation encoding="application/x-tex">V_{b}</annotation></semantics></math>. In a setting where the considered affinity <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m20" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is bi-directional (e.g., a symmetric matrix), this corresponds to finding shortcut nodes, allowing to go from <math alttext="V_{a}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m21" intent=":literal"><semantics><msub><mi>V</mi><mi>a</mi></msub><annotation encoding="application/x-tex">V_{a}</annotation></semantics></math> to <math alttext="V_{b}" class="ltx_Math" display="inline" id="S5.SS4.SSS0.Px1.p2.m22" intent=":literal"><semantics><msub><mi>V</mi><mi>b</mi></msub><annotation encoding="application/x-tex">V_{b}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS0.Px1.p3">
<p class="ltx_p">It follows that the neighborhood-reinforcing nature of the threshold dynamics of BDH-GPU, which plausibly follows from the logic of its role in inference and from the needs for an efficient computational process, is starkly different from the more often studied submodular behavior of threshold and cascade dynamics on real-world networks <cite class="ltx_cite ltx_citemacro_citep">(Kempe et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib55" title="">2003</a>)</cite>, and plausibly, much less smooth when considered as a dynamical process.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Empirical findings: parameter distribution in ReLU-lowrank matrix products</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p">We consider the <math alttext="D" class="ltx_Math" display="inline" id="S5.SS5.p1.m1" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> matrices (in the same way <math alttext="{D_{y}}" class="ltx_Math" display="inline" id="S5.SS5.p1.m2" intent=":literal"><semantics><msub><mi>D</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{D_{y}}</annotation></semantics></math> and <math alttext="{D_{x}}" class="ltx_Math" display="inline" id="S5.SS5.p1.m3" intent=":literal"><semantics><msub><mi>D</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{D_{x}}</annotation></semantics></math>) and <math alttext="E" class="ltx_Math" display="inline" id="S5.SS5.p1.m4" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> matrix obtained after training of BDH-GPU models, and used in the ReLU-lowrank operation Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E10" title="In Definition of ReLU-lowrank. ‣ 5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">10</span></a>), <math alttext="f_{DE}(z)=\left(DEz\right)^{+}" class="ltx_Math" display="inline" id="S5.SS5.p1.m5" intent=":literal"><semantics><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">f_{DE}(z)=\left(DEz\right)^{+}</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Choice of prior of matrix parameter distributions.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p1">
<p class="ltx_p">Following the discussion in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS4" title="5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.4</span></a>, we expect matrix <math alttext="G:=DE" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo>:=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G:=DE</annotation></semantics></math> to reflect the clustering (modularity) structure of the neuron-neuron communication graph. Any plausible parameter distribution of matrix <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p1.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> must therefore allow heavy-tailed distribution of entries. At the same time, a Gaussian noise term is inherent to low-rank matrix representation, and needs to be taken into account together with this heavy-tailed distribution.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p2">
<p class="ltx_p">We now provide a somewhat more fine-grained explanation, which leads to the prior on the structure of matrix <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> as given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>). Consider a training set-up in which the ReLU-lowrank operation described by matrix <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> is treated as an approximation of the same operation, governed by a high-rank matrix <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m3" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>, with <math alttext="f^{\prime}(z):=\left(G^{\prime}z\right)^{+}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m4" intent=":literal"><semantics><mrow><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><msup><mrow><mo>(</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">f^{\prime}(z):=\left(G^{\prime}z\right)^{+}</annotation></semantics></math>. Considering this block in isolation from the rest of the training system, the training of matrices <math alttext="D" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m5" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, <math alttext="E" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m6" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> goal corresponds to learning an approximation of <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m7" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math>, with <math alttext="D\in R^{n,d},E\in R^{n,d}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m8" intent=":literal"><semantics><mrow><mrow><mi>D</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow><mo>,</mo><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">D\in R^{n,d},E\in R^{n,d}</annotation></semantics></math>, such that <math alttext="f(z)\approx f^{\prime}(z)" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m9" intent=":literal"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f(z)\approx f^{\prime}(z)</annotation></semantics></math> holds for some class of vectors <math alttext="z" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p2.m10" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p3">
<p class="ltx_p">For the rest of this analysis, we will consider the function <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p3.m1" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math> as a ground truth reference for the intended operation of the ReLU-lowrank block. This type of analysis can be seen as plausible over short time spans in later phases of training of a BDH-GPU model, i.e., once individual neurons in <math alttext="R^{n}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p3.m2" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> have started to admit semantic or functional meaning, and so when function <math alttext="D^{\prime}E^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p3.m3" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">D^{\prime}E^{\prime}</annotation></semantics></math> describes a property of the problem being solved in a (frozen) concept space, and not a co-learning process between the representation of the concept space in <math alttext="R^{n}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p3.m4" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> and the functions applied to it.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p4">
<p class="ltx_p">We can represent <math alttext="G^{\prime}:=D^{\prime}E^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m1" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>:=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow></mrow><annotation encoding="application/x-tex">G^{\prime}:=D^{\prime}E^{\prime}</annotation></semantics></math>, where <math alttext="D^{\prime}\in R^{n,s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m2" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo>,</mo><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D^{\prime}\in R^{n,s}</annotation></semantics></math>, <math alttext="E^{\prime}\in R^{s,n}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m3" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>s</mi><mo>,</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{\prime}\in R^{s,n}</annotation></semantics></math>, with <math alttext="s=O(n^{2})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m4" intent=":literal"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">s=O(n^{2})</annotation></semantics></math>, are in general matrices of rank <math alttext="n" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>; we have <math alttext="f^{\prime}(z):=\left(D^{\prime}E^{\prime}z\right)^{+}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m6" intent=":literal"><semantics><mrow><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><msup><mrow><mo>(</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">f^{\prime}(z):=\left(D^{\prime}E^{\prime}z\right)^{+}</annotation></semantics></math>. Without loss of generality, we can choose from among the possible representations one with the following distribution of positive and negative elements: <math alttext="D^{\prime}\in(R^{+})^{n,s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m7" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo>,</mo><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D^{\prime}\in(R^{+})^{n,s}</annotation></semantics></math>, <math alttext="E^{\prime}={E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m8" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>=</mo><mrow><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow></mrow><annotation encoding="application/x-tex">E^{\prime}={E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}}</annotation></semantics></math>, with <math alttext="{E^{\prime}}^{\mathfrak{e}},{E^{\prime}}^{\mathfrak{i}}\in(R^{+})^{s,n}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m9" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mi>s</mi><mo>,</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{E^{\prime}}^{\mathfrak{e}},{E^{\prime}}^{\mathfrak{i}}\in(R^{+})^{s,n}</annotation></semantics></math>. We will write: <math alttext="G^{\prime}={G^{\prime}}^{\mathfrak{e}}-{G^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m10" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>=</mo><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow></mrow><annotation encoding="application/x-tex">G^{\prime}={G^{\prime}}^{\mathfrak{e}}-{G^{\prime}}^{\mathfrak{i}}</annotation></semantics></math>, where <math alttext="{G^{\prime}}^{\mathfrak{e}}=D^{\prime}{E^{\prime}}^{\mathfrak{e}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m11" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts></mrow></mrow><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{e}}=D^{\prime}{E^{\prime}}^{\mathfrak{e}}</annotation></semantics></math>, and <math alttext="{G^{\prime}}^{\mathfrak{i}}=D^{\prime}{E^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m12" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow></mrow><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{i}}=D^{\prime}{E^{\prime}}^{\mathfrak{i}}</annotation></semantics></math>. The main purpose of the chosen representation <math alttext="G^{\prime}=D^{\prime}E^{\prime}=D^{\prime}({E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m13" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>′</mo></msup><mo>=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><mo>=</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\prime}=D^{\prime}E^{\prime}=D^{\prime}({E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}})</annotation></semantics></math> is to have matrices <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m14" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math>, <math alttext="{E^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m15" intent=":literal"><semantics><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts><annotation encoding="application/x-tex">{E^{\prime}}^{\mathfrak{i}}</annotation></semantics></math>, <math alttext="{E^{\prime}}^{\mathfrak{e}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m16" intent=":literal"><semantics><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{E^{\prime}}^{\mathfrak{e}}</annotation></semantics></math> with much smaller outlying elements compared to matrix <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p4.m17" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>, which leads to more justified conclusions about the uniform nature of the noise introduced by the low-rank decomposition.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>For a specific example, one very broad class of matrices <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="footnote15.m1" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is given by the product of sparse matrices <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="footnote15.m2" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math>, <math alttext="{E^{\prime}}" class="ltx_Math" display="inline" id="footnote15.m3" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">{E^{\prime}}</annotation></semantics></math>, in which each <math alttext="s" class="ltx_Math" display="inline" id="footnote15.m4" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>-element row of <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="footnote15.m5" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math> (column of <math alttext="{E^{\prime}}" class="ltx_Math" display="inline" id="footnote15.m6" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">{E^{\prime}}</annotation></semantics></math>) has at most <math alttext="\Delta\ll n" class="ltx_Math" display="inline" id="footnote15.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">\Delta\ll n</annotation></semantics></math> non-zero elements, each with value bounded by <math alttext="O(1/\sqrt{\Delta})" class="ltx_Math" display="inline" id="footnote15.m8" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><msqrt><mi mathvariant="normal">Δ</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1/\sqrt{\Delta})</annotation></semantics></math>, and all remaining <math alttext="s-\Delta" class="ltx_Math" display="inline" id="footnote15.m9" intent=":literal"><semantics><mrow><mi>s</mi><mo>−</mo><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">s-\Delta</annotation></semantics></math> elements of these matrices are equal to <math alttext="0" class="ltx_Math" display="inline" id="footnote15.m10" intent=":literal"><mn>0</mn></math>. The resulting elements, <math alttext="{G^{\prime}}_{i}j=\sum_{\alpha}D^{\prime}_{i,\alpha}\{E^{\prime}\}_{\alpha,j}" class="ltx_Math" display="inline" id="footnote15.m11" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mi>i</mi><mrow></mrow></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mo rspace="0.111em">=</mo><mrow><msub><mo>∑</mo><mi>α</mi></msub><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow><mo>′</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">{</mo><msup><mi>E</mi><mo>′</mo></msup><mo stretchy="false">}</mo></mrow><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">{G^{\prime}}_{i}j=\sum_{\alpha}D^{\prime}_{i,\alpha}\{E^{\prime}\}_{\alpha,j}</annotation></semantics></math>, may be much less uniform, only satisfying <math alttext="{G^{\prime}}_{i}j=O(1)" class="ltx_Math" display="inline" id="footnote15.m12" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mi>i</mi><mrow></mrow></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{G^{\prime}}_{i}j=O(1)</annotation></semantics></math>. This type of scenario captures the expressiveness of set intersection for “bag-of-words” models for language, or expressiveness of “hub label” representations for a measure of node proximity in a directed graph.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p5">
<p class="ltx_p">Assume now that we learn to approximate function <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m1" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math> with <math alttext="f_{DE}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m2" intent=":literal"><semantics><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><annotation encoding="application/x-tex">f_{DE}</annotation></semantics></math> by trainable matrices <math alttext="D,E" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m3" intent=":literal"><semantics><mrow><mi>D</mi><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">D,E</annotation></semantics></math> through the following low-rank scheme:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex30">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G=DE:=(B_{D}+D^{\prime}P)(P^{T}E^{\prime}+B_{E}^{T})," class="ltx_Math" display="block" id="S5.Ex30.m1" intent=":literal"><semantics><mrow><mrow><mi>G</mi><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><mo>:=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>B</mi><mi>D</mi></msub><mo>+</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>P</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>P</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><mo>+</mo><msubsup><mi>B</mi><mi>E</mi><mi>T</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">G=DE:=(B_{D}+D^{\prime}P)(P^{T}E^{\prime}+B_{E}^{T}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="P\in R^{s,d}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m4" intent=":literal"><semantics><mrow><mi>P</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>s</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">P\in R^{s,d}</annotation></semantics></math> is <em class="ltx_emph ltx_font_italic">non-parametric</em> and the result of random sampling an almost-orthonormal random projection so that <math alttext="PP^{T}\approx I_{s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m5" intent=":literal"><semantics><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>T</mi></msup></mrow><mo>≈</mo><msub><mi>I</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">PP^{T}\approx I_{s}</annotation></semantics></math> (e.g. <math alttext="P\sim\mathcal{N}(0,1/\sqrt{d})^{s,d}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m6" intent=":literal"><semantics><mrow><mi>P</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mrow><mn>1</mn><mo>/</mo><msqrt><mi>d</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>s</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">P\sim\mathcal{N}(0,1/\sqrt{d})^{s,d}</annotation></semantics></math>), and <math alttext="B_{D},B_{E}\in R^{n,d}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>B</mi><mi>D</mi></msub><mo>,</mo><msub><mi>B</mi><mi>E</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B_{D},B_{E}\in R^{n,d}</annotation></semantics></math> represent <em class="ltx_emph ltx_font_italic">trainable</em> additional terms for compensating error or introducing bias, with the goal of minimizing some loss function <math alttext="\mathcal{L}(f^{\prime},f_{DE})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m8" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>f</mi><mo>′</mo></msup><mo>,</mo><msub><mi>f</mi><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}(f^{\prime},f_{DE})</annotation></semantics></math>. The terms <math alttext="B_{D},B_{E}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m9" intent=":literal"><semantics><mrow><msub><mi>B</mi><mi>D</mi></msub><mo>,</mo><msub><mi>B</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">B_{D},B_{E}</annotation></semantics></math> compensate the error introduced by the approximation <math alttext="PP^{T}\approx I_{s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p5.m10" intent=":literal"><semantics><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>T</mi></msup></mrow><mo>≈</mo><msub><mi>I</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">PP^{T}\approx I_{s}</annotation></semantics></math>, after the ReLU operation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p6">
<p class="ltx_p">Let <math alttext="Q:=PP^{T}=I_{s}+\delta_{I}+\delta_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m1" intent=":literal"><semantics><mrow><mi>Q</mi><mo>:=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>P</mi><mi>T</mi></msup></mrow><mo>=</mo><mrow><msub><mi>I</mi><mi>s</mi></msub><mo>+</mo><msub><mi>δ</mi><mi>I</mi></msub><mo>+</mo><msub><mi>δ</mi><mi>Q</mi></msub></mrow></mrow><annotation encoding="application/x-tex">Q:=PP^{T}=I_{s}+\delta_{I}+\delta_{Q}</annotation></semantics></math>, where <math alttext="\delta_{I}\in R^{s\times s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m2" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>I</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\delta_{I}\in R^{s\times s}</annotation></semantics></math> is a diagonal error matrix, and <math alttext="\delta_{Q}\in R^{s\times s}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m3" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>Q</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\delta_{Q}\in R^{s\times s}</annotation></semantics></math> is a non-diagonal (hollow) matrix. We have:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex31">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G=DE=({G^{\prime}}^{\mathfrak{e}}-{G^{\prime}}^{\mathfrak{i}})+D^{\prime}\delta_{I}({E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}})+\underbrace{D^{\prime}\delta_{Q}E^{\prime}}_{\varepsilon_{Q}}+\underbrace{(B_{D}E+DB_{E}^{T})}_{\varepsilon_{B}}." class="ltx_Math" display="block" id="S5.Ex31.m1" intent=":literal"><semantics><mrow><mrow><mi>G</mi><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo stretchy="false">)</mo></mrow><mo>+</mo><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>δ</mi><mi>I</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><munder><munder accentunder="true"><mrow><msup><mi>D</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>δ</mi><mi>Q</mi></msub><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><mo stretchy="true">⏟</mo></munder><msub><mi>ε</mi><mi>Q</mi></msub></munder><mo>+</mo><munder><munder accentunder="true"><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>B</mi><mi>D</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><mo>+</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>B</mi><mi>E</mi><mi>T</mi></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo stretchy="true">⏟</mo></munder><msub><mi>ε</mi><mi>B</mi></msub></munder></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">G=DE=({G^{\prime}}^{\mathfrak{e}}-{G^{\prime}}^{\mathfrak{i}})+D^{\prime}\delta_{I}({E^{\prime}}^{\mathfrak{e}}-{E^{\prime}}^{\mathfrak{i}})+\underbrace{D^{\prime}\delta_{Q}E^{\prime}}_{\varepsilon_{Q}}+\underbrace{(B_{D}E+DB_{E}^{T})}_{\varepsilon_{B}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Since all elements of <math alttext="D^{\prime},{E^{\prime}}^{\mathfrak{e}},{E^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m4" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>,</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>,</mo><mmultiscripts><mi>E</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><annotation encoding="application/x-tex">D^{\prime},{E^{\prime}}^{\mathfrak{e}},{E^{\prime}}^{\mathfrak{i}}</annotation></semantics></math> are non-negative and <math alttext="I_{\delta}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m5" intent=":literal"><semantics><msub><mi>I</mi><mi>δ</mi></msub><annotation encoding="application/x-tex">I_{\delta}</annotation></semantics></math> is diagonal, we can represent elements <math alttext="G_{ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m6" intent=":literal"><semantics><msub><mi>G</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">G_{ij}</annotation></semantics></math>, for <math alttext="i,j\in 1,\ldots,n" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m7" intent=":literal"><semantics><mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">i,j\in 1,\ldots,n</annotation></semantics></math>, as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="G_{ij}=(1+\varepsilon_{\delta\,ij}^{\mathfrak{e}}){G^{\prime}}^{\mathfrak{e}}_{ij}-(1+\varepsilon_{\delta\,ij}^{\mathfrak{i}}){G^{\prime}}^{\mathfrak{i}}_{ij}+\varepsilon_{Q\,ij}+\varepsilon_{B\,ij}" class="ltx_Math" display="block" id="S5.E13.m1" intent=":literal"><semantics><mrow><msub><mi>G</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msubsup><mi>ε</mi><mrow><mi>δ</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts></mrow><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><msubsup><mi>ε</mi><mrow><mi>δ</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔦</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔦</mi></mmultiscripts></mrow></mrow><mo>+</mo><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>ε</mi><mrow><mi>B</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">G_{ij}=(1+\varepsilon_{\delta\,ij}^{\mathfrak{e}}){G^{\prime}}^{\mathfrak{e}}_{ij}-(1+\varepsilon_{\delta\,ij}^{\mathfrak{i}}){G^{\prime}}^{\mathfrak{i}}_{ij}+\varepsilon_{Q\,ij}+\varepsilon_{B\,ij}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="|\varepsilon_{\delta\,ij}^{\mathfrak{e}}|=O(\sqrt{\log n\ /\ d})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m8" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><msubsup><mi>ε</mi><mrow><mi>δ</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></msubsup><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">|\varepsilon_{\delta\,ij}^{\mathfrak{e}}|=O(\sqrt{\log n\ /\ d})</annotation></semantics></math> and <math alttext="|\varepsilon_{\delta\,ij}^{\mathfrak{i}}|=O(\sqrt{\log n\ /\ d})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p6.m9" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><msubsup><mi>ε</mi><mrow><mi>δ</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔦</mi></msubsup><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">|\varepsilon_{\delta\,ij}^{\mathfrak{i}}|=O(\sqrt{\log n\ /\ d})</annotation></semantics></math> have the interpretation of small multiplicative distortion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p7">
<p class="ltx_p">Following (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>), we expect the elements of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p7.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> to be distributed as the sum of four different distributions.
The term <math alttext="{G^{\prime}}^{\mathfrak{e}}_{ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p7.m2" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{e}}_{ij}</annotation></semantics></math> has the interpretation of positive ground truth elements of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p7.m3" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>. The term <math alttext="-{G^{\prime}}^{\mathfrak{i}}_{ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p7.m4" intent=":literal"><semantics><mrow><mo>−</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔦</mi></mmultiscripts></mrow><annotation encoding="application/x-tex">-{G^{\prime}}^{\mathfrak{i}}_{ij}</annotation></semantics></math> has the interpretation of negative ground truth elements of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p7.m5" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>; its use in combination with the ReLU mechanism can be interpreted as inhibitory action. Both of these terms are subject to slight multiplicative distortion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p8">
<p class="ltx_p">The term <math alttext="\varepsilon_{Q\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m1" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{Q\,ij}</annotation></semantics></math> has the interpretation of non-trainable noise (which depends only on <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m2" intent=":literal"><semantics><msup><mi>D</mi><mo>′</mo></msup><annotation encoding="application/x-tex">D^{\prime}</annotation></semantics></math>, <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m3" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">E^{\prime}</annotation></semantics></math> and the random choice of <math alttext="P" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m4" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>). Under reasonable assumptions on outlying elements of <math alttext="D^{\prime},E^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m5" intent=":literal"><semantics><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>,</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">D^{\prime},E^{\prime}</annotation></semantics></math>, it is a form of almost-Gaussian symmetric noise inherent to the considered class of low-rank projections, <math alttext="\varepsilon_{Q\,ij}\rightarrow N(0,\sigma_{Q})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m6" intent=":literal"><semantics><mrow><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo stretchy="false">→</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msub><mi>σ</mi><mi>Q</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\varepsilon_{Q\,ij}\rightarrow N(0,\sigma_{Q})</annotation></semantics></math>, for some <math alttext="\sigma_{Q}\in R^{+}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m7" intent=":literal"><semantics><mrow><msub><mi>σ</mi><mi>Q</mi></msub><mo>∈</mo><msup><mi>R</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">\sigma_{Q}\in R^{+}</annotation></semantics></math>, and the expected value of this noise is typically very close to <math alttext="0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m8" intent=":literal"><mn>0</mn></math>, even when considering the expectation of <math alttext="\varepsilon_{Q\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m9" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{Q\,ij}</annotation></semantics></math> conditioned on known values of <math alttext="\varepsilon_{Q\,i^{\prime}j^{\prime}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m10" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><msup><mi>i</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{Q\,i^{\prime}j^{\prime}}</annotation></semantics></math> for a small number of indexes <math alttext="(i^{\prime},j^{\prime})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p8.m11" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>′</mo></msup><mo>,</mo><msup><mi>j</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i^{\prime},j^{\prime})</annotation></semantics></math> in the matrix.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p9">
<p class="ltx_p">Finally, <math alttext="\varepsilon_{B\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p9.m1" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>B</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{B\,ij}</annotation></semantics></math> is a trainable term, whose norm tends to <math alttext="0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p9.m2" intent=":literal"><mn>0</mn></math> as <math alttext="d" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p9.m3" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> increases. We expect it to have the interpretation of bias used to offset the low-rank Gaussian noise and perform denoising in the ReLU-gate, as previously discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a>. Because of the action of the ReLU gate, we plausibly expect the distribution of <math alttext="\varepsilon_{B\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p9.m4" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>B</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{B\,ij}</annotation></semantics></math> to be skewed towards negative numbers, with <math alttext="0&gt;\mathbb{E}\varepsilon_{B\,ij}\gg\sigma_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p9.m5" intent=":literal"><semantics><mrow><mn>0</mn><mo>&gt;</mo><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>ε</mi><mrow><mi>B</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow><mo>≫</mo><msub><mi>σ</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">0&gt;\mathbb{E}\varepsilon_{B\,ij}\gg\sigma_{Q}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p10">
<p class="ltx_p">From the above discussion of the four terms of the sum in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>), we see that only one of these terms, <math alttext="{G^{\prime}}^{\mathfrak{e}}_{ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p10.m1" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{e}}_{ij}</annotation></semantics></math>, is expected to take values much larger than <math alttext="\sigma_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p10.m2" intent=":literal"><semantics><msub><mi>σ</mi><mi>Q</mi></msub><annotation encoding="application/x-tex">\sigma_{Q}</annotation></semantics></math> with non-negligible probability. We reach the conclusion that a part of the relevant signal of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p10.m3" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is concentrated in the right tail of large positive matrix entries of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p10.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_hypothesis" id="Thmhypothesis1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Hypothesis 1</span></span><span class="ltx_text ltx_font_bold"> </span>(right tail contains signal)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmhypothesis1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Consider the interpretation that the ReLU-lowrank transformation <math alttext="z\mapsto\left(Gz\right)^{+}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m1" intent=":literal"><semantics><mrow><mi>z</mi><mo stretchy="false">↦</mo><msup><mrow><mo>(</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">z\mapsto\left(Gz\right)^{+}</annotation></semantics></math>, with <math alttext="G=DE" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m2" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G=DE</annotation></semantics></math>, has learned to act as an approximation of some other operation <math alttext="z\mapsto\left(G^{\prime}z\right)^{+}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m3" intent=":literal"><semantics><mrow><mi>z</mi><mo stretchy="false">↦</mo><msup><mrow><mo>(</mo><mrow><msup><mi>G</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">z\mapsto\left(G^{\prime}z\right)^{+}</annotation></semantics></math>, where <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m4" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> has no low-rank constraint imposed on it. Then the right tail of the distribution of matrix elements of <math alttext="G" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> corresponds to the right tail of the distribution of elements of <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m6" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math>, starting from some positive threshold value <math alttext="\sigma_{Q}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m7" intent=":literal"><semantics><msub><mi>σ</mi><mi>Q</mi></msub><annotation encoding="application/x-tex">\sigma_{Q}</annotation></semantics></math>, associated with the noise of the low-rank decomposition. Formally, for almost all pairs of indices <math alttext="i,j\in 1,\ldots,n" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m8" intent=":literal"><semantics><mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">i,j\in 1,\ldots,n</annotation></semantics></math> such that <math alttext="G_{ij}\gg\sigma_{Q}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m9" intent=":literal"><semantics><mrow><msub><mi>G</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>≫</mo><msub><mi>σ</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">G_{ij}\gg\sigma_{Q}</annotation></semantics></math>, we also have <math alttext="{G^{\prime}}^{\mathfrak{e}}_{ij}\gg\sigma_{Q}" class="ltx_Math" display="inline" id="Thmhypothesis1.p1.m10" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts><mo>≫</mo><msub><mi>σ</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{e}}_{ij}\gg\sigma_{Q}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p11">
<p class="ltx_p">The converse implication, that <math alttext="{G^{\prime}}^{\mathfrak{e}}_{ij}\gg\sigma_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p11.m1" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts><mo>≫</mo><msub><mi>σ</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{e}}_{ij}\gg\sigma_{Q}</annotation></semantics></math> implies <math alttext="{G}_{ij}\gg\sigma_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p11.m2" intent=":literal"><semantics><mrow><msub><mi>G</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>≫</mo><msub><mi>σ</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">{G}_{ij}\gg\sigma_{Q}</annotation></semantics></math>, also plausibly holds under some stronger assumptions on the form of biases <math alttext="\varepsilon_{B\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p11.m3" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>B</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{B\,ij}</annotation></semantics></math> which may follow from minimizing training error for the specific inference task considered.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p12">
<p class="ltx_p">This direct method of decoding <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> from <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m2" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> does not extend from the right tail towards the center of the distribution. For the choices of <math alttext="n,d" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m3" intent=":literal"><semantics><mrow><mi>n</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n,d</annotation></semantics></math> we make, we expect the term dominating most elements of matrix <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> to be <math alttext="\varepsilon_{Q\,ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m5" intent=":literal"><semantics><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">\varepsilon_{Q\,ij}</annotation></semantics></math>. For example, when <math alttext="G^{\prime}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m6" intent=":literal"><semantics><msup><mi>G</mi><mo>′</mo></msup><annotation encoding="application/x-tex">G^{\prime}</annotation></semantics></math> is a stochastic matrix, we expect to have <math alttext="\sigma_{Q}=O(1/\sqrt{d})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m7" intent=":literal"><semantics><mrow><msub><mi>σ</mi><mi>Q</mi></msub><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><msqrt><mi>d</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\sigma_{Q}=O(1/\sqrt{d})</annotation></semantics></math> (cf. Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E11" title="In Error of low-rank approximation (without ReLU). ‣ 5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">11</span></a>) for the corresponding infinity-norm bound, <math alttext="|\varepsilon_{Q\,ij}|=O(\sqrt{\log n\ /\ d})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m8" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><msub><mi>ε</mi><mrow><mi>Q</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo lspace="0.722em" rspace="0.722em">/</mo><mi>d</mi></mrow></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">|\varepsilon_{Q\,ij}|=O(\sqrt{\log n\ /\ d})</annotation></semantics></math>). With <math alttext="\sum_{i,j}|{G^{\prime}}^{\mathfrak{e}}_{i,j}|=n" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m9" intent=":literal"><semantics><mrow><mrow><msub><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mrow><mo lspace="0em" stretchy="false">|</mo><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>𝔢</mi></mmultiscripts><mo stretchy="false">|</mo></mrow></mrow><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">\sum_{i,j}|{G^{\prime}}^{\mathfrak{e}}_{i,j}|=n</annotation></semantics></math> for a stochastic matrix,
we expect the right heavy tail of the element distribution of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m10" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> to have <math alttext="\Omega(n\sqrt{d})" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m11" intent=":literal"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><msqrt><mi>d</mi></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega(n\sqrt{d})</annotation></semantics></math> elements (out of the <math alttext="n^{2}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m12" intent=":literal"><semantics><msup><mi>n</mi><mn>2</mn></msup><annotation encoding="application/x-tex">n^{2}</annotation></semantics></math> matrix elements of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p12.m13" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>) which are clearly separated from the Gaussian noise.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px1.p13">
<p class="ltx_p">We confirm empirically that the right tail of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p13.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, defined as above with respect to threshold <math alttext="\sigma_{Q}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p13.m2" intent=":literal"><semantics><msub><mi>σ</mi><mi>Q</mi></msub><annotation encoding="application/x-tex">\sigma_{Q}</annotation></semantics></math>, turns out to contain a non-negligible portion of the parameter capacity of matrices <math alttext="D" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p13.m3" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, <math alttext="E" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px1.p13.m4" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math>, even for very small models (10M to 100M parameters).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Experimental setup.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px2.p1">
<p class="ltx_p">We prepared parameter matrices of a 24M-parameter BDH-GPU model configured with <math alttext="h=4" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m1" intent=":literal"><semantics><mrow><mi>h</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">h=4</annotation></semantics></math> heads and <math alttext="L=8" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">L=8</annotation></semantics></math> layers, <math alttext="n=h\cdot 2^{13}=2^{15}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m3" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>13</mn></msup></mrow><mo>=</mo><msup><mn>2</mn><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">n=h\cdot 2^{13}=2^{15}</annotation></semantics></math> neurons, and hidden low-rank dimension <math alttext="d=256" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m4" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math>. We considered the weighted neuron-neuron interaction graph, having the encoder-decoder matrix pair <math alttext="G={D_{x}}E" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m5" intent=":literal"><semantics><mrow><mi>G</mi><mo>=</mo><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G={D_{x}}E</annotation></semantics></math> as its node adjacency matrix on the set of neurons <math alttext="V=1,\ldots,n" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m6" intent=":literal"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow><annotation encoding="application/x-tex">V=1,\ldots,n</annotation></semantics></math>. For uniformity, we subsampled <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m7" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> by picking node subsets <math alttext="V^{(a)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m8" intent=":literal"><semantics><msup><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">V^{(a)}</annotation></semantics></math>, <math alttext="a\in\{1,2,3,4\}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m9" intent=":literal"><semantics><mrow><mi>a</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">a\in\{1,2,3,4\}</annotation></semantics></math>, associated with each head, and considered the weighted subgraphs <math alttext="G^{(ab)}=\{V,\{(u,v,G_{uv}):u\in V^{(a)},v\in V^{(b)}\}\}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m10" intent=":literal"><semantics><mrow><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>V</mi><mo>,</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><msub><mi>G</mi><mrow><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow></msub><mo rspace="0.278em" stretchy="false">)</mo></mrow><mo rspace="0.278em">:</mo><mrow><mrow><mi>u</mi><mo>∈</mo><msup><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>∈</mo><msup><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo stretchy="false">}</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">G^{(ab)}=\{V,\{(u,v,G_{uv}):u\in V^{(a)},v\in V^{(b)}\}\}</annotation></semantics></math>, with <math alttext="G^{(ab)}\in R^{n^{*}\times n^{*}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m11" intent=":literal"><semantics><mrow><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>R</mi><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>n</mi><mo>∗</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">G^{(ab)}\in R^{n^{*}\times n^{*}}</annotation></semantics></math> where <math alttext="n^{*}=n/h=2^{13}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m12" intent=":literal"><semantics><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>n</mi><mo>/</mo><mi>h</mi></mrow><mo>=</mo><msup><mn>2</mn><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">n^{*}=n/h=2^{13}</annotation></semantics></math>, each having <math alttext="(n^{*})^{2}=2^{26}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p1.m13" intent=":literal"><semantics><mrow><msup><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><mn>2</mn></msup><mo>=</mo><msup><mn>2</mn><mn>26</mn></msup></mrow><annotation encoding="application/x-tex">(n^{*})^{2}=2^{26}</annotation></semantics></math> weighted edges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px2.p2">
<p class="ltx_p">We repeated the experiment <math alttext="5" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px2.p2.m1" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math> times using models pretrained with different random seeds.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Findings.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p1">
<p class="ltx_p">For all of the <math alttext="5" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m1" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math> models we pretrained for this purpose, exactly <math alttext="3" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m2" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math> out of the <math alttext="4" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m3" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> encoder heads and all decoder heads adhered to the prior on parameter distribution given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>), showing a good correspondence for <math alttext="12" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m4" intent=":literal"><semantics><mn>12</mn><annotation encoding="application/x-tex">12</annotation></semantics></math> out of <math alttext="16" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m5" intent=":literal"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math> of their parameter sub-matrices <math alttext="G^{(ab)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p1.m6" intent=":literal"><semantics><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">G^{(ab)}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p2">
<p class="ltx_p">We continue the discussion in this Section for one specific matrix <math alttext="G^{(ab)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m1" intent=":literal"><semantics><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">G^{(ab)}</annotation></semantics></math> of one specific pretrained models, which was chosen as representative. The example we choose has <math alttext="a=b" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m2" intent=":literal"><semantics><mrow><mi>a</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a=b</annotation></semantics></math>; and so the matrix <math alttext="G^{(ab)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m3" intent=":literal"><semantics><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">G^{(ab)}</annotation></semantics></math> has an interpretation as <math alttext="G[V_{a}]" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m4" intent=":literal"><semantics><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>V</mi><mi>a</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">G[V_{a}]</annotation></semantics></math>, i.e., the subgraph of <math alttext="G" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> induced by vertex set <math alttext="V_{a}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m6" intent=":literal"><semantics><msub><mi>V</mi><mi>a</mi></msub><annotation encoding="application/x-tex">V_{a}</annotation></semantics></math>, which enables us to visualize the graph <math alttext="G^{(aa)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m7" intent=":literal"><semantics><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">G^{(aa)}</annotation></semantics></math> more easily on its vertex set <math alttext="V_{a}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p2.m8" intent=":literal"><semantics><msub><mi>V</mi><mi>a</mi></msub><annotation encoding="application/x-tex">V_{a}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p3">
<p class="ltx_p">We refer to the representative object of our study, i.e., to the matrix <math alttext="G^{(aa)}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m1" intent=":literal"><semantics><msup><mi>G</mi><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">G^{(aa)}</annotation></semantics></math> of the selected model, as <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m2" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math>. For any matrix <math alttext="A" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m3" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math alttext="\beta\geq 0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m4" intent=":literal"><semantics><mrow><mi>β</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta\geq 0</annotation></semantics></math>, we denote by <math alttext="A_{\geq\beta}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m5" intent=":literal"><semantics><msub><mi>A</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow></msub><annotation encoding="application/x-tex">A_{\geq\beta}</annotation></semantics></math> the matrix <math alttext="A" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m6" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> cut off at threshold <math alttext="\beta" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m7" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, i.e., <math alttext="{A_{\geq\beta}}_{ij}=A_{ij}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m8" intent=":literal"><semantics><mrow><mmultiscripts><mi>A</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mrow></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mrow></mrow></mmultiscripts><mo>=</mo><msub><mi>A</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{A_{\geq\beta}}_{ij}=A_{ij}</annotation></semantics></math> if <math alttext="A_{ij}\geq\beta" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m9" intent=":literal"><semantics><mrow><msub><mi>A</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>≥</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">A_{ij}\geq\beta</annotation></semantics></math>, and <math alttext="{A_{\geq\beta}}_{ij}=0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p3.m10" intent=":literal"><semantics><mrow><mmultiscripts><mi>A</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mrow></mrow><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mrow></mrow></mmultiscripts><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{A_{\geq\beta}}_{ij}=0</annotation></semantics></math> otherwise.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS0.Px3.p4">
<p class="ltx_p">The distribution of elements <math alttext="G^{*}_{i,j}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p4.m1" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{i,j}</annotation></semantics></math> is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F9" title="Figure 9 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a> (a).</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="165" id="S5.F9.g1" src="fig4.png" width="476"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>(a) Heavy-tailed element distribution and modularity analysis of the excitatory neuron-neuron connection graph contained the encoder-decoder matrix <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.F9.m17" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math>. Distribution of elements of the encoder-decoder matrix <math alttext="G^{*}\in R^{n^{*}\times n^{*}}" class="ltx_Math" display="inline" id="S5.F9.m18" intent=":literal"><semantics><mrow><msup><mi>G</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>n</mi><mo>∗</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">G^{*}\in R^{n^{*}\times n^{*}}</annotation></semantics></math> of a BDH-GPU model with <math alttext="n^{*}=8192" class="ltx_Math" display="inline" id="S5.F9.m19" intent=":literal"><semantics><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo>=</mo><mn>8192</mn></mrow><annotation encoding="application/x-tex">n^{*}=8192</annotation></semantics></math> neurons and <math alttext="d=256" class="ltx_Math" display="inline" id="S5.F9.m20" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math>: histogram <math alttext="\mathrm{freq_{G^{*}}}(x)" class="ltx_Math" display="inline" id="S5.F9.m21" intent=":literal"><semantics><mrow><msub><mi>freq</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{freq_{G^{*}}}(x)</annotation></semantics></math>, its symmetric part <math alttext="\mathrm{freq-symmetric_{G^{*}}}(x):=\min\{\mathrm{freq_{G^{*}}}(x),\mathrm{freq_{G^{*}}}(-x)\}" class="ltx_Math" display="inline" id="S5.F9.m22" intent=":literal"><semantics><mrow><mrow><mi>freq</mi><mo>−</mo><mrow><msub><mi>symmetric</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>:=</mo><mrow><mi>min</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>freq</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>freq</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{freq-symmetric_{G^{*}}}(x):=\min\{\mathrm{freq_{G^{*}}}(x),\mathrm{freq_{G^{*}}}(-x)\}</annotation></semantics></math>, and distribution skew <math alttext="\mathrm{freq-skew_{G^{*}}}(x):=\mathrm{freq_{G^{*}}}(x)-\mathrm{freq-symmetric_{G^{*}}}(x)" class="ltx_Math" display="inline" id="S5.F9.m23" intent=":literal"><semantics><mrow><mrow><mi>freq</mi><mo>−</mo><mrow><msub><mi>skew</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>:=</mo><mrow><mrow><msub><mi>freq</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>freq</mi><mo>−</mo><mrow><msub><mi>symmetric</mi><msup><mi mathvariant="normal">G</mi><mo>∗</mo></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{freq-skew_{G^{*}}}(x):=\mathrm{freq_{G^{*}}}(x)-\mathrm{freq-symmetric_{G^{*}}}(x)</annotation></semantics></math>. <math alttext="\diamond" class="ltx_Math" display="inline" id="S5.F9.m24" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> (b) Estimate (lower bound) of Newman modularity of matrix <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F9.m25" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math> for different values of <math alttext="\beta" class="ltx_Math" display="inline" id="S5.F9.m26" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, plotted as a function of the number of non-zero elements (edges) of <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F9.m27" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>. Modularity of random graph baselines are provided for reference, for the <math alttext="G(n^{*},m)" class="ltx_Math" display="inline" id="S5.F9.m28" intent=":literal"><semantics><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mo>∗</mo></msup><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(n^{*},m)</annotation></semantics></math> model with the same number of edges as <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F9.m29" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>, and for a matrix <math alttext="(P_{1}P_{2}^{T})_{\geq\beta^{\prime}}" class="ltx_Math" display="inline" id="S5.F9.m30" intent=":literal"><semantics><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>P</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>P</mi><mn>2</mn><mi>T</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mrow><mi></mi><mo>≥</mo><msup><mi>β</mi><mo>′</mo></msup></mrow></msub><annotation encoding="application/x-tex">(P_{1}P_{2}^{T})_{\geq\beta^{\prime}}</annotation></semantics></math> with the same number of edges as <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F9.m31" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>, where <math alttext="P_{1},P_{2}\sim\mathcal{N}(0,1)^{n^{*}\times d}" class="ltx_Math" display="inline" id="S5.F9.m32" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><msub><mi>P</mi><mn>2</mn></msub></mrow><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">P_{1},P_{2}\sim\mathcal{N}(0,1)^{n^{*}\times d}</annotation></semantics></math>. The modularity estimates were obtained using the community structures returned by the Louvain algorithm, in the best of 5 clustering runs with different random seeds.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p5">
<p class="ltx_p">We find that the observed distribution corresponds well to the prior expected of it by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>). We determine the threshold value <math alttext="\beta\geq 0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m1" intent=":literal"><semantics><mrow><mi>β</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta\geq 0</annotation></semantics></math> at which we expect to capture signal, <math alttext="{G^{*}_{\geq\beta}}\approx{G^{\prime}_{\geq\beta}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m2" intent=":literal"><semantics><mrow><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><mo>≈</mo><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>′</mo></msubsup></mrow><annotation encoding="application/x-tex">{G^{*}_{\geq\beta}}\approx{G^{\prime}_{\geq\beta}}</annotation></semantics></math>, following Hypothesis <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmhypothesis1" title="Hypothesis 1 (right tail contains signal). ‣ Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>. We find (from Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F9" title="Figure 9 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a>(a)) that the separation from noise happens for this specific matrix <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m3" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math> at <math alttext="\beta_{1}\approx 1.2" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m4" intent=":literal"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>≈</mo><mn>1.2</mn></mrow><annotation encoding="application/x-tex">\beta_{1}\approx 1.2</annotation></semantics></math>, at which point the right heavy tail begins to dominate. However, already for much smaller values of <math alttext="\beta" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m5" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> we find that <math alttext="{G^{*}_{\geq\beta}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m6" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">{G^{*}_{\geq\beta}}</annotation></semantics></math> has high modularity, and this actually increases as more non-zero values are added to <math alttext="{G^{*}_{\geq\beta}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m7" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">{G^{*}_{\geq\beta}}</annotation></semantics></math> for smaller <math alttext="\beta" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m8" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, up to a maximum at <math alttext="\beta_{2}\approx 1.0" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m9" intent=":literal"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>≈</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">\beta_{2}\approx 1.0</annotation></semantics></math> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F9" title="Figure 9 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a>(b)). Even for much smaller values of <math alttext="\beta" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m10" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, the modularity of <math alttext="{G^{*}_{\geq\beta}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m11" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">{G^{*}_{\geq\beta}}</annotation></semantics></math> remains almost constant up to well above <math alttext="2^{20}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m12" intent=":literal"><semantics><msup><mn>2</mn><mn>20</mn></msup><annotation encoding="application/x-tex">2^{20}</annotation></semantics></math> non-zero matrix entries on the <math alttext="n^{*}=2^{13}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m13" intent=":literal"><semantics><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo>=</mo><msup><mn>2</mn><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">n^{*}=2^{13}</annotation></semantics></math> nodes considered. The modularity of the baselines, of random graphs or random low-rank matrix products, quickly drops to 0 in this regime. This should be compared to the total number of parameters of the matrices <math alttext="{D_{x}},E" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m14" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>x</mi></msub><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{x}},E</annotation></semantics></math> corresponding to <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m15" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math>, i.e., <math alttext="2\cdot 2^{13}\cdot 2^{8}=2^{22}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m16" intent=":literal"><semantics><mrow><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>13</mn></msup><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>8</mn></msup></mrow><mo>=</mo><msup><mn>2</mn><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">2\cdot 2^{13}\cdot 2^{8}=2^{22}</annotation></semantics></math> parameters. A complementary analysis of the inhibitory signal, for a similarly defined matrix <math alttext="|G^{*}_{\leq-\beta}|" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p5.m17" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><msubsup><mi>G</mi><mrow><mi></mi><mo>≤</mo><mrow><mo>−</mo><mi>β</mi></mrow></mrow><mo>∗</mo></msubsup><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|G^{*}_{\leq-\beta}|</annotation></semantics></math>, also finds that this structure has high modularity.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS0.Px3.p6">
<p class="ltx_p">In auxiliary experiments, we looked at basic graph parameters of matrix <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p6.m1" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>, treated as a directed graph on its set of nodes. We set <math alttext="\beta=1.2" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p6.m2" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1.2</mn></mrow><annotation encoding="application/x-tex">\beta=1.2</annotation></semantics></math>, obtaining <math alttext="m=46820" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p6.m3" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mn>46820</mn></mrow><annotation encoding="application/x-tex">m=46820</annotation></semantics></math> non-zero entries (edges) in <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p6.m4" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>.
We found that <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p6.m5" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math> has a heavy-tailed, power-law-like degree distribution, with generally more concentrated out-degree than in-degree (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F10" title="Figure 10 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">10</span></a>(a)).</p>
</div>
<figure class="ltx_figure" id="S5.F10">
<p class="ltx_p">(a)
<br class="ltx_break"/><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S5.F10.g1" src="fig5.png" width="476"/>
<br class="ltx_break"/>(b)
<br class="ltx_break"/>     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="174" id="S5.F10.g2" src="fig6.png" width="381"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>(a) Unweighted in-degree and out-degree distribution for the <math alttext="n^{*}=8192" class="ltx_Math" display="inline" id="S5.F10.m6" intent=":literal"><semantics><mrow><msup><mi>n</mi><mo>∗</mo></msup><mo>=</mo><mn>8192</mn></mrow><annotation encoding="application/x-tex">n^{*}=8192</annotation></semantics></math> neuron nodes and <math alttext="m=46820" class="ltx_Math" display="inline" id="S5.F10.m7" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mn>46820</mn></mrow><annotation encoding="application/x-tex">m=46820</annotation></semantics></math> edges of matrix <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F10.m8" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math> with <math alttext="\beta=1.2" class="ltx_Math" display="inline" id="S5.F10.m9" intent=":literal"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1.2</mn></mrow><annotation encoding="application/x-tex">\beta=1.2</annotation></semantics></math>. The distributions exhibit power law distributions, with different exponents, the out-degree distribution being more concentrated. (b) Visualization of graph <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.F10.m10" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math>, hinting at its core-periphery structure.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p7">
<p class="ltx_p">Generally, this finding is consistent with expectations as to the structure of a network with positive modularity. The difference of in- and out-degree distributions, while plausible and prevalent in real-world information dissemination networks, was not considered in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p8">
<p class="ltx_p">Finally, a visualization of <math alttext="G^{*}_{\geq\beta}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p8.m1" intent=":literal"><semantics><msubsup><mi>G</mi><mrow><mi></mi><mo>≥</mo><mi>β</mi></mrow><mo>∗</mo></msubsup><annotation encoding="application/x-tex">G^{*}_{\geq\beta}</annotation></semantics></math> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.F10" title="Figure 10 ‣ Findings. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">10</span></a>(b)) exhibits a core-periphery structure. This is again consistent with the expected modular structure.</p>
</div>
<div class="ltx_theorem ltx_theorem_finding" id="Thmfinding3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Empirical Finding 3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmfinding3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">We confirmed that during training, <em class="ltx_emph ltx_font_upright">a graph structure with positive modularity</em> appears in BDH-GPU model parameter matrices <math alttext="{D_{x}}E" class="ltx_Math" display="inline" id="Thmfinding3.p1.m1" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>x</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{x}}E</annotation></semantics></math> and <math alttext="{D_{y}}E" class="ltx_Math" display="inline" id="Thmfinding3.p1.m2" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{y}}E</annotation></semantics></math>. This modular structure plausibly follows from the network’s inference function, and specifically from the cluster-aware information propagation dynamics supported by the ReLU-lowrank mechanism (Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation6" title="Observation 6 (in-cluster signal reinforcement). ‣ 5.4 Modularity in BDH-GPU signal propagation ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>).</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.SSS0.Px3.p9">
<p class="ltx_p">We also observed that for all of the studied models with <math alttext="h=4" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m1" intent=":literal"><semantics><mrow><mi>h</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">h=4</annotation></semantics></math> heads, <math alttext="1" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m2" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> encoder sub-matrix out of <math alttext="4" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m3" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> has no heavy positive tail, and generally appears to capture a form of inhibitory structure <math alttext="{G^{\prime}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m4" intent=":literal"><semantics><mmultiscripts><mi>G</mi><mrow></mrow><mo>′</mo><mrow></mrow><mi>𝔦</mi></mmultiscripts><annotation encoding="application/x-tex">{G^{\prime}}^{\mathfrak{i}}</annotation></semantics></math> from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.E13" title="In Choice of prior of matrix parameter distributions. ‣ 5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>). Since we have not provided convincing mechanisms for isolating negative signals in <math alttext="G^{*}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m5" intent=":literal"><semantics><msup><mi>G</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">G^{*}</annotation></semantics></math> and these are easily confounded with the bias term <math alttext="\varepsilon_{B}" class="ltx_Math" display="inline" id="S5.SS5.SSS0.Px3.p9.m6" intent=":literal"><semantics><msub><mi>ε</mi><mi>B</mi></msub><annotation encoding="application/x-tex">\varepsilon_{B}</annotation></semantics></math>, we omit this case from discussion. We remark that the apparent need for passing activations through such a separate “inhibitory circuit” is one of the most evident explanations for why introducing (a small number of) heads to BDH-GPU provides an improvement in model quality.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis: linear attention, sparse positive activation, and monosemanticity</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Macro-expressiveness of attention in BDH-GPU</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p">The attention mechanism of BDH-GPU can be described at a coarse-grained level as a transformation mechanism for key-query-value vectors, similar to that in the Transformer. This description is complementary to the interpretation of the BDH-GPU attention mechanism at the micro-level of correlations between neuron pairs, which we defer to Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.2</span></a>, which provides more insight into the way activation vectors used by BDH-GPU relate to the concept space of the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p">We compare the attention mechanism of BDH-GPU with the attention mechanism of the Transformer, describing both as reflections of a general attention mechanism. Specifically, we explain why, and up to what context length, the linear attention mechanism of BDH-GPU plausibly fits into macro-expressiveness frameworks of attention designed for the Transformer (based on RASP).</p>
</div>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Basic properties of BDH-GPU attention.</h5>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p">The key-query space for BDH-GPU is <math alttext="R^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math>, the same as its neuron dimension, rather than the small dense dimension used by the Transformer. The keys and queries used by BDH-GPU are given by positive vectors, in <math alttext="(R^{+})^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m2" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><annotation encoding="application/x-tex">(R^{+})^{n}</annotation></semantics></math>, and are expressed by the same vector <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m3" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math> (noting that at time <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m5" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math> is used as a query, and only <math alttext="x_{\tau,l}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m6" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{\tau,l}</annotation></semantics></math>, for <math alttext="\tau\leq t-1" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.m7" intent=":literal"><semantics><mrow><mi>τ</mi><mo>≤</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\tau\leq t-1</annotation></semantics></math>, are used as keys).</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px1.p2">
<p class="ltx_p">‘Value’ vectors of BDH-GPU remain in the small dimension, <math alttext="R^{d}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p2.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>d</mi></msup><annotation encoding="application/x-tex">R^{d}</annotation></semantics></math>, which at some model scales is comparable to the dimension used for attention ‘values’ in common configurations of the Transformer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px1.p3">
<p class="ltx_p">The relationship between softmax-based attention of the Transformer, regarded as a low-dimensional kernel for general linear attention, and linear attention for vectors in the positive orthant, was considered in a framework called FAVOR+ <cite class="ltx_cite ltx_citemacro_citep">(Choromanski et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib22" title="">2021</a>)</cite>. Here, we provide a few complementary (simpler) observations, sufficient to grasp the main effects of the ability of Linear Attention to distinguish facts in context.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">State capacity vs. distinction capacity.</h5>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p">The matrix <math alttext="{\boldsymbol{\rho}}\in R^{n\times d}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p1.m1" intent=":literal"><semantics><mrow><mi>𝝆</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">{\boldsymbol{\rho}}\in R^{n\times d}</annotation></semantics></math>, which is used to represent state for each layer of BDH-GPU, should theoretically have sufficient capacity to store <math alttext="O(n)" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p1.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math> ‘value’ vectors in <math alttext="R^{d}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p1.m3" intent=":literal"><semantics><msup><mi>R</mi><mi>d</mi></msup><annotation encoding="application/x-tex">R^{d}</annotation></semantics></math> if considered as a lookup table for values. We now remark that its actual capability of <em class="ltx_emph ltx_font_italic">distinguishing facts</em> using the linear attention mechanism is also asymptotically close to <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p1.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px2.p2">
<p class="ltx_p">Attention is a mechanism of associative memory which, given a series of key-value pairs <math alttext="((k_{1},v_{1})\ldots,(k_{t},v_{t}))\in(\Lambda_{k}\times R^{d})^{t}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>v</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi mathvariant="normal">Λ</mi><mi>k</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><mo stretchy="false">)</mo></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">((k_{1},v_{1})\ldots,(k_{t},v_{t}))\in(\Lambda_{k}\times R^{d})^{t}</annotation></semantics></math>, a query <math alttext="q\in\Lambda_{q}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m2" intent=":literal"><semantics><mrow><mi>q</mi><mo>∈</mo><msub><mi mathvariant="normal">Λ</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">q\in\Lambda_{q}</annotation></semantics></math> and an affinity function <math alttext="\phi(\cdot,\cdot):\Lambda_{q}\times\Lambda_{k}\to[0,1]" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m3" intent=":literal"><semantics><mrow><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><msub><mi mathvariant="normal">Λ</mi><mi>q</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi mathvariant="normal">Λ</mi><mi>k</mi></msub></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\phi(\cdot,\cdot):\Lambda_{q}\times\Lambda_{k}\to[0,1]</annotation></semantics></math> between the space of queries and keys, returns the attention value:
<math alttext="a_{t}=\sum_{\tau=1}^{t-1}\phi(q,k_{\tau})v_{\tau}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m4" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mi>τ</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">a_{t}=\sum_{\tau=1}^{t-1}\phi(q,k_{\tau})v_{\tau}</annotation></semantics></math>
(or a normalization thereof). With BDH-GPU, we consider ‘value’ vectors <math alttext="v\in R^{d}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m5" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">v\in R^{d}</annotation></semantics></math>, where <math alttext="d" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m6" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is small. The spaces of keys and queries may be assumed to coincide as <math alttext="\Lambda=\Lambda_{k}=\Lambda_{q}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">Λ</mi><mo>=</mo><msub><mi mathvariant="normal">Λ</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="normal">Λ</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\Lambda=\Lambda_{k}=\Lambda_{q}</annotation></semantics></math>, and we consider in general a single key-query sequence, given by <math alttext="(k_{t})_{t\in\mathbb{N}}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m8" intent=":literal"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow></msub><annotation encoding="application/x-tex">(k_{t})_{t\in\mathbb{N}}</annotation></semantics></math>:<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>This assumption is known to have moderate practical implications for trainability. In this specific discussion, it is ‘without loss of generality’, since one can consider <math alttext="\Lambda=\Lambda_{1}\otimes\Lambda_{2}\otimes\ldots\otimes\Lambda_{t}" class="ltx_Math" display="inline" id="footnote16.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Λ</mi><mo>=</mo><mrow><msub><mi mathvariant="normal">Λ</mi><mn>1</mn></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><msub><mi mathvariant="normal">Λ</mi><mn>2</mn></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><mi mathvariant="normal">…</mi><mo lspace="0.222em" rspace="0.222em">⊗</mo><msub><mi mathvariant="normal">Λ</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\Lambda=\Lambda_{1}\otimes\Lambda_{2}\otimes\ldots\otimes\Lambda_{t}</annotation></semantics></math>, and consider each <math alttext="k_{i}" class="ltx_Math" display="inline" id="footnote16.m2" intent=":literal"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding="application/x-tex">k_{i}</annotation></semantics></math> as chosen from <math alttext="\Lambda_{i}" class="ltx_Math" display="inline" id="footnote16.m3" intent=":literal"><semantics><msub><mi mathvariant="normal">Λ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\Lambda_{i}</annotation></semantics></math>, defining affinity <math alttext="\phi(k_{t},k_{\tau}):\Lambda_{t}\times\Lambda_{\tau}\to[0,1]" class="ltx_Math" display="inline" id="footnote16.m4" intent=":literal"><semantics><mrow><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo rspace="0.278em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.278em">:</mo><mrow><mrow><msub><mi mathvariant="normal">Λ</mi><mi>t</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi mathvariant="normal">Λ</mi><mi>τ</mi></msub></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\phi(k_{t},k_{\tau}):\Lambda_{t}\times\Lambda_{\tau}\to[0,1]</annotation></semantics></math> appropriately to handle successive keys and queries (effectively describing a general form of positional embedding).</span></span></span></p>
<table class="ltx_equation ltx_eqn_table" id="S6.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="a_{t}=\sum_{\tau=1}^{t-1}\phi(k_{t},k_{\tau})v_{\tau}" class="ltx_Math" display="block" id="S6.E14.m1" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mi>τ</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">a_{t}=\sum_{\tau=1}^{t-1}\phi(k_{t},k_{\tau})v_{\tau}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This key-query space <math alttext="\Lambda" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m9" intent=":literal"><semantics><mi mathvariant="normal">Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> may be considered as an abstract space, and represented in any way which is convenient, for as long as the affinity function <math alttext="\phi(k_{t},k_{\tau})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m10" intent=":literal"><semantics><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi(k_{t},k_{\tau})</annotation></semantics></math> is preserved. For example, when the keys and queries are sampled from a finite (though possibly extremely large) set, there also exists some vector space dimension <math alttext="\nu" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m11" intent=":literal"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math> (possibly extremely large) and a function mapping <math alttext="f:\Lambda\to S^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m12" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi mathvariant="normal">Λ</mi><mo stretchy="false">→</mo><msup><mi>S</mi><mi>ν</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f:\Lambda\to S^{\nu}</annotation></semantics></math>, where <math alttext="S^{\nu}=\{z\in R^{\nu}:\|z\|=1\}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m13" intent=":literal"><semantics><mrow><msup><mi>S</mi><mi>ν</mi></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mi>ν</mi></msup></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">‖</mo><mi>z</mi><mo stretchy="false">‖</mo></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S^{\nu}=\{z\in R^{\nu}:\|z\|=1\}</annotation></semantics></math> is the unit sphere, such that the scalar (dot, cosine) product in <math alttext="S^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m14" intent=":literal"><semantics><msup><mi>S</mi><mi>ν</mi></msup><annotation encoding="application/x-tex">S^{\nu}</annotation></semantics></math> satisfies <math alttext="f(k_{t})\cdot f(k_{\tau})=\phi(k_{t},k_{\tau})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m15" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">f(k_{t})\cdot f(k_{\tau})=\phi(k_{t},k_{\tau})</annotation></semantics></math>. In other words, any affinity function <math alttext="\phi" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m16" intent=":literal"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> becomes linear when represented in sufficiently high dimension, subject to suitable preparation of its arguments with function <math alttext="f" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m17" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. With <math alttext="\nu" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m18" intent=":literal"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math> extremely large, <math alttext="S^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p2.m19" intent=":literal"><semantics><msup><mi>S</mi><mi>ν</mi></msup><annotation encoding="application/x-tex">S^{\nu}</annotation></semantics></math> is a sort of Platonic ideal of a space in which the attention keys and queries live, with no relation to any specific model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px2.p3">
<p class="ltx_p">This type of argument, often used in considerations of Support Vector Machines, is linked to two challenges: (1) ensuring that the dimension actually considered by the network (in our case <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p3.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>) is high enough compared to the hypothetical dimension <math alttext="(\nu)" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p3.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>ν</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\nu)</annotation></semantics></math>, and (2) ensuring that a suitable preparation function <math alttext="f" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p3.m3" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> exists and is easy to learn for the model.<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>The Transformer can also be positioned in the same SVM framework: the Transformer’s attention represents a form of “kernel trick” for one specific affinity function <math alttext="\phi" class="ltx_Math" display="inline" id="footnote17.m1" intent=":literal"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>, with the kernel used to approximate it being the exponential function (in the case of softmax attention).</span></span></span> We now explain when the dimension <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px2.p3.m4" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> can be considered sufficient, and what types of keys can be prepared by BDH-GPU.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Expressiveness of linear attention in dimension <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</h5>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px3.p1">
<p class="ltx_p">The Linear Attention mechanism aggregates key-value correlations over time. In general, the associated rate of accumulation of noise is manageable, up to the approximate scale of between <math alttext="t=\tilde{\Omega}(\sqrt{n})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.p1.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi mathvariant="normal">Ω</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{\Omega}(\sqrt{n})</annotation></semantics></math> and <math alttext="t=\tilde{O}(n)" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi>O</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{O}(n)</annotation></semantics></math> key-value ‘facts’ stored in the attention of a given layer. We make the following statement about the Linear Attention mechanism in general.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 7</span></span><span class="ltx_text ltx_font_bold"> </span>(informal statement)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The mechanism of Linear Attention, applied in dimension <math alttext="R^{n}" class="ltx_Math" display="inline" id="Thmclaim7.p1.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math>, can approximately express an attention affinity function for up to <math alttext="t=\tilde{O}(n)" class="ltx_Math" display="inline" id="Thmclaim7.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi>O</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{O}(n)</annotation></semantics></math> ‘key-value’ pairs in context, with ‘values’ having comparable L2-norm, under moderate assumptions on weak correlation of historical keys and uniformity of the expressed affinity function. Without such assumptions, Linear Attention can compute the correct affinity up to at least <math alttext="t=\tilde{\Omega}(\sqrt{n})" class="ltx_Math" display="inline" id="Thmclaim7.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi mathvariant="normal">Ω</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{\Omega}(\sqrt{n})</annotation></semantics></math> ‘key-value’ pairs in context, except for a negligible fraction of possible inputs. Keys and queries need to be suitably prepared beforehand.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px3.p2">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">The formal statement and proof is provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS2" title="C.2 Formal statement of Claim 7 (linear attention) ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">C.2</span></a>.</em>∎</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px3.p3">
<p class="ltx_p">The above claim captures the expressiveness of Linear Attention in dimension <math alttext="R^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.p3.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math>, subject to some way of preparing keys and queries in <math alttext="R^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.p3.m2" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> by the model in blocks preceding the attention block. A model using Linear Attention has to learn its own way to prepare keys. In fact, different <em class="ltx_emph ltx_font_italic">natural</em> approaches to key preparation, for example using random projections or hashing on a set of <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px3.p3.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> vectors, lead to the same asymptotic statement of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim7" title="Claim 7 (informal statement). ‣ Expressiveness of linear attention in dimension 𝑛. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a>. (In the proof in the Appendix, we chose to use a particularly simple one.)</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px3.p4">
<p class="ltx_p">The specific way of preparing keys used (learned) by BDH-GPU for its Linear Attention is particularly interesting. Except for the effect of RoPE rotation, which introduces a negative positional effect in the affinity of keys and queries, BDH-GPU uses activation vectors (keys, queries) with only positive coordinates to represent its keys.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px3.p5">
<p class="ltx_p">We discuss some aspects of how the positive activation vectors of BDH-GPU relate to Linear Attention.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Preparation of positive keys for Linear Attention.</h5>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.p1">
<p class="ltx_p">Activation vectors of BDH-GPU belong to the positive orthant, and are often sparse. The interpretation of such vectors depends on whether we consider the positive orthant to be a “valid shape” for the latent concept space of the considered task (in this case, language and reasoning), or whether the task has to be embedded into such a space. For language, this would be a question of whether a <span class="ltx_text ltx_font_sansserif">word2vec</span>-like internal representation of the concept space by the model has an inherent advantage over a
<span class="ltx_text ltx_font_sansserif">bag-of-words</span>-like representation, especially when expressing concept affinities in attention.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.p2">
<p class="ltx_p">We note that latent representation of key and query vectors in the positive orthant is natural for any problem which is <em class="ltx_emph ltx_font_italic">amenable to attention</em>. In the discussion of general attention given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.E14" title="In State capacity vs. distinction capacity. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>), we noted that the affinity function <math alttext="\phi" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m1" intent=":literal"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> takes values in <math alttext="[0,1]" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math>, and we considered an embedding <math alttext="f" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m3" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> of a set of key vectors <math alttext="k_{1},\ldots,k_{t}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m4" intent=":literal"><semantics><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">k_{1},\ldots,k_{t}</annotation></semantics></math> into <math alttext="R^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m5" intent=":literal"><semantics><msup><mi>R</mi><mi>ν</mi></msup><annotation encoding="application/x-tex">R^{\nu}</annotation></semantics></math> such that <math alttext="f(k_{t})\cdot f(k_{\tau})=\phi(k_{t},k_{\tau})\geq 0" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m6" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo>,</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(k_{t})\cdot f(k_{\tau})=\phi(k_{t},k_{\tau})\geq 0</annotation></semantics></math>. Given this condition on non-negativity of dot product on all pairs among the <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m7" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> vectors considered, we could have, without loss of generality, used an appropriately rotated embedding <math alttext="f" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m8" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> so that <math alttext="f(k_{\tau})\in(R^{+})^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m9" intent=":literal"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>ν</mi></msup></mrow><annotation encoding="application/x-tex">f(k_{\tau})\in(R^{+})^{\nu}</annotation></semantics></math>, thus <em class="ltx_emph ltx_font_italic">directly reducing the problem of general attention to a problem of linear attention in the non-negative orthant</em>. The question which remains is a subtle one: whether this type of embedding of the latent space of language and reasoning in <math alttext="(R^{+})^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m10" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>ν</mi></msup><annotation encoding="application/x-tex">(R^{+})^{\nu}</annotation></semantics></math> is ‘natural’, i.e., preserved over long periods of time of inference and training, notably longer than the short window <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p2.m11" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> of context used for Transformer-like attention.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.p3">
<p class="ltx_p">In the rest of the paper, we are generally inclined to assume that representations in <math alttext="(R^{+})^{\nu}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.p3.m1" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>ν</mi></msup><annotation encoding="application/x-tex">(R^{+})^{\nu}</annotation></semantics></math> of concepts, combinations of concepts, and density distributions over such combinations of concepts, are universal to language and reasoning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.p4">
<p class="ltx_p">We limit ourselves to a very brief discussion of a way to represent attention keys with positive vectors for problems for which such a concept representation is not natural.</p>
</div>
<section class="ltx_subparagraph" id="S6.SS1.SSS0.Px4.SPx1">
<h6 class="ltx_title ltx_title_subparagraph">Using LSH to move key vectors into the positive orthant.</h6>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx1.p1">
<p class="ltx_p">Locality Sensitive Hashing (LSH) is one technique for converting arbitrary vectors in a lower-dimensional space <math alttext="R^{a}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m1" intent=":literal"><semantics><msup><mi>R</mi><mi>a</mi></msup><annotation encoding="application/x-tex">R^{a}</annotation></semantics></math>, for some fixed <math alttext="a\in\mathbb{N}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m2" intent=":literal"><semantics><mrow><mi>a</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">a\in\mathbb{N}</annotation></semantics></math>, into vectors in <math alttext="(R^{+})^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m3" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><annotation encoding="application/x-tex">(R^{+})^{n}</annotation></semantics></math>, in a way which can be used to describe certain ‘sharp-boundary’ affinity functions <math alttext="\phi" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m4" intent=":literal"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> in <math alttext="R^{a}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m5" intent=":literal"><semantics><msup><mi>R</mi><mi>a</mi></msup><annotation encoding="application/x-tex">R^{a}</annotation></semantics></math>. Consider an <math alttext="n\times a" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m6" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">n\times a</annotation></semantics></math> matrix represented as <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m7" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> fixed random vectors <math alttext="\lambda_{1},\ldots,\lambda_{n}\in R^{a}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m8" intent=":literal"><semantics><mrow><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>λ</mi><mi>n</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mi>a</mi></msup></mrow><annotation encoding="application/x-tex">\lambda_{1},\ldots,\lambda_{n}\in R^{a}</annotation></semantics></math>, and a corresponding sequence of <math alttext="n" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m9" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> appropriately chosen gating functions <math alttext="\gamma_{1},\ldots,\gamma_{n}:R\to R^{+}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m10" intent=":literal"><semantics><mrow><mrow><msub><mi>γ</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>γ</mi><mi>n</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>R</mi><mo stretchy="false">→</mo><msup><mi>R</mi><mo>+</mo></msup></mrow></mrow><annotation encoding="application/x-tex">\gamma_{1},\ldots,\gamma_{n}:R\to R^{+}</annotation></semantics></math>. For a vector <math alttext="v\in R^{a}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m11" intent=":literal"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mi>R</mi><mi>a</mi></msup></mrow><annotation encoding="application/x-tex">v\in R^{a}</annotation></semantics></math>, we define:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="b(v):=\gamma([\lambda_{1}\ldots\lambda_{n}]v)=(\gamma_{i}({v}^{T}{\lambda_{i}}))_{1\leq i\leq n}." class="ltx_Math" display="block" id="S6.E15.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>γ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">[</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>λ</mi><mi>n</mi></msub></mrow><mo stretchy="false">]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>v</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>γ</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>v</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>λ</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi></mrow></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">b(v):=\gamma([\lambda_{1}\ldots\lambda_{n}]v)=(\gamma_{i}({v}^{T}{\lambda_{i}}))_{1\leq i\leq n}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Each <math alttext="i" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m12" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th element of vector <math alttext="b" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m13" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> thus corresponds to the outcome of the <math alttext="i" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p1.m14" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th bucket of LSH.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx1.p2">
<p class="ltx_p">The bucketing function <math alttext="b" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m1" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> may now be used to prepare queries and keys as attention inputs. If <math alttext="\gamma_{i}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m2" intent=":literal"><semantics><msub><mi>γ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\gamma_{i}</annotation></semantics></math> is a <math alttext="\{0,1\}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0,1\}</annotation></semantics></math>-valued threshold function, then, for <math alttext="q,k_{i}\in R^{a}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m4" intent=":literal"><semantics><mrow><mrow><mi>q</mi><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow><mo>∈</mo><msup><mi>R</mi><mi>a</mi></msup></mrow><annotation encoding="application/x-tex">q,k_{i}\in R^{a}</annotation></semantics></math>, <math alttext="{b(q)}^{T}{b(k_{i})}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m5" intent=":literal"><semantics><mrow><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>b</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{b(q)}^{T}{b(k_{i})}</annotation></semantics></math> is an attention affinity function between <math alttext="q" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m6" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> and <math alttext="k_{i}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m7" intent=":literal"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding="application/x-tex">k_{i}</annotation></semantics></math>, equal to the number of LSH buckets shared between <math alttext="q" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m8" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> and <math alttext="k_{i}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p2.m9" intent=":literal"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding="application/x-tex">k_{i}</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation7">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 7</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The LSH vector affinity function <math alttext="b" class="ltx_Math" display="inline" id="Thmobservation7.p1.m1" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>, given by Equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.E15" title="In Using LSH to move key vectors into the positive orthant. ‣ Preparation of positive keys for Linear Attention. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">15</span></a>), using <math alttext="n" class="ltx_Math" display="inline" id="Thmobservation7.p1.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> buckets on vectors in <math alttext="R^{a}" class="ltx_Math" display="inline" id="Thmobservation7.p1.m3" intent=":literal"><semantics><msup><mi>R</mi><mi>a</mi></msup><annotation encoding="application/x-tex">R^{a}</annotation></semantics></math> for some <math alttext="a\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmobservation7.p1.m4" intent=":literal"><semantics><mrow><mi>a</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">a\in\mathbb{N}</annotation></semantics></math>, can be expressed through Linear Attention with attention keys in the positive orthant <math alttext="(R^{+})^{n}" class="ltx_Math" display="inline" id="Thmobservation7.p1.m5" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><annotation encoding="application/x-tex">(R^{+})^{n}</annotation></semantics></math>.∎</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx1.p3">
<p class="ltx_p">In the ReLU-based setup considered in BDH-GPU, an appropriate function <math alttext="b" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p3.m1" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> is plausibly easy to learn. LSH is a ‘sharp-boundary’ technique, well-suited for finding <math alttext="k" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx1.p3.m2" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-nearest-neighbors of a queried vector in a set of keys. Hence, the class of attention affinity functions, naturally expressible using BDH-GPU, also includes such ‘sharp’ functions.</p>
</div>
</section>
<section class="ltx_subparagraph" id="S6.SS1.SSS0.Px4.SPx2">
<h6 class="ltx_title ltx_title_subparagraph">Attention in the positive concept space of language and reasoning.</h6>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx2.p1">
<p class="ltx_p">BDH-GPU uses the positive orthant <math alttext="(R^{+})^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p1.m1" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><annotation encoding="application/x-tex">(R^{+})^{n}</annotation></semantics></math> as its latent space for representing combinations of concepts in its activation vectors. Attention keys and queries are prepared entirely in this positive orthant.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx2.p2">
<p class="ltx_p">When representing a task of reasoning or language inference in a high-dimensional space, positive activation vectors in <math alttext="(R^{+})^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p2.m1" intent=":literal"><semantics><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><annotation encoding="application/x-tex">(R^{+})^{n}</annotation></semantics></math> have a natural interpretation of convex combinations of concepts. Such convex combinations of concepts may represent both semantically connected concepts (“bags-of-concepts”), and mixed states of uncertainty between unconnected concepts. In this interpretation, a positive vector is considered as a state of certain knowledge when its L1-norm and L2-norm align closely. Note that for a (normalized) probability vector, the only vectors for which L1-norm and L2-norm coincide precisely are distributions concentrated on a single coordinate.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px4.SPx2.p3">
<p class="ltx_p">Linear Attention of BDH-GPU is capable of amplifying very small differences between keys in the L1-norm when matching queries to keys. Consider, for instance, two probability distribution vectors <math alttext="x_{1},x_{2}\in(R^{+})^{n}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x_{1},x_{2}\in(R^{+})^{n}</annotation></semantics></math>, where <math alttext="x_{1}=(\alpha,\frac{1-\alpha}{n-1},\frac{1-\alpha}{n-1},\ldots,\frac{1-\alpha}{n-1})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m2" intent=":literal"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo>,</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>,</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{1}=(\alpha,\frac{1-\alpha}{n-1},\frac{1-\alpha}{n-1},\ldots,\frac{1-\alpha}{n-1})</annotation></semantics></math> and <math alttext="x_{2}=(\frac{1-\alpha}{n-1},\alpha,\frac{1-\alpha}{n-1},\ldots,\frac{1-\alpha}{n-1})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m3" intent=":literal"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>,</mo><mi>α</mi><mo>,</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{2}=(\frac{1-\alpha}{n-1},\alpha,\frac{1-\alpha}{n-1},\ldots,\frac{1-\alpha}{n-1})</annotation></semantics></math>, for some <math alttext="0&lt;\alpha&lt;1" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m4" intent=":literal"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0&lt;\alpha&lt;1</annotation></semantics></math>. Now, vectors <math alttext="x_{1}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m5" intent=":literal"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_{1}</annotation></semantics></math> and <math alttext="x_{2}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m6" intent=":literal"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_{2}</annotation></semantics></math> almost coincide when treated as probability distributions, <math alttext="\|x_{1}-x_{2}\|_{1}=O(\alpha)=\|x_{1}-x_{2}\|_{\mathrm{TVD}}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m7" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo stretchy="false">‖</mo></mrow><mi>TVD</mi></msub></mrow><annotation encoding="application/x-tex">\|x_{1}-x_{2}\|_{1}=O(\alpha)=\|x_{1}-x_{2}\|_{\mathrm{TVD}}</annotation></semantics></math>. However, they are extremely different when considered as keys for the Linear Attention mechanism, with <math alttext="x_{1}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m8" intent=":literal"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_{1}</annotation></semantics></math> showing very weak affinity to <math alttext="x_{2}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m9" intent=":literal"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_{2}</annotation></semantics></math>: <math alttext="{x_{1}}^{T}{x_{2}}=O(\alpha^{-2}n^{-1}){x_{1}}^{T}{x_{1}}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px4.SPx2.p3.m10" intent=":literal"><semantics><mrow><mrow><mmultiscripts><mi>x</mi><mn>1</mn><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>α</mi><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>n</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mn>1</mn><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mn>1</mn></msub></mrow></mrow><annotation encoding="application/x-tex">{x_{1}}^{T}{x_{2}}=O(\alpha^{-2}n^{-1}){x_{1}}^{T}{x_{1}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 8</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In key-query matching, the Linear Attention mechanism of BDH-GPU is able to separate positive keys which are close in the L1-norm, strongly amplifying L1-norm differences of activation vectors.∎</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px4.SPx2.p4">
<p class="ltx_p">This mechanism can be treated as complementary to the propagation dynamics of positive activations in the feed-forward network, discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS3" title="5.3 ReLU-lowrank as a signal propagation dynamics ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Natural support for long context.</h5>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px5.p1">
<p class="ltx_p">There is no bound on context length in BDH-GPU, so the actual <math alttext="t=\tilde{\Omega}(\sqrt{n})" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi mathvariant="normal">Ω</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{\Omega}(\sqrt{n})</annotation></semantics></math> to <math alttext="t=\tilde{O}(n)" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mover accent="true"><mi>O</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t=\tilde{O}(n)</annotation></semantics></math> “equally important facts” that a BDH-GPU model can distinguish in each layer in view of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim7" title="Claim 7 (informal statement). ‣ Expressiveness of linear attention in dimension 𝑛. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a> do not have to correspond to the latest <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> “facts” seen in context. For example, if, for some layer <math alttext="l" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m4" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, mechanisms from lower layers deem a given entry to be irrelevant for layer <math alttext="l" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m5" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, and provide an extremely weak attention ‘value’ for this layer, and this key-value entry is effectively seen as omitted. This mechanism corresponds to weaker signals <math alttext="y" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m6" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> in a layer which needs to take no action on a given input, e.g., does not have to remember it (cf. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F14" title="Figure 14 ‣ 6.4 Empirical findings: sparse neuron activations ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>). Indeed, empirically we observe progressive de-noising of state in the higher layers, with only small fractions of input tokens requiring significant key-value state update in the middle layers across the entire spectrum of state <math alttext="{\boldsymbol{\rho}}" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p1.m7" intent=":literal"><semantics><mi>𝝆</mi><annotation encoding="application/x-tex">{\boldsymbol{\rho}}</annotation></semantics></math> of neurons.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px5.p2">
<p class="ltx_p">As a result, the middle and higher layers of BDH-GPU may, in principle, have unbounded look-back on context. Nonetheless, as context length <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px5.p2.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> increases, we find that damping of historical signals over long sequences is necessary in BDH-GPU to avoid overwhelming the model with noise from stale context. For the vanilla version of the architecture, we found that RoPE combined with ALiBi provide a sufficient remedy, and model performance improves as context length increases. More advanced techniques for BDH-GPU, related to selective forgetting, state compression, or other forms of state optimization, can also be added to the architecture.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Micro-interpretation of attention in BDH-GPU</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p">BDH maintains its state in the <math alttext="n\times n" class="ltx_Math" display="inline" id="S6.SS2.p1.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS2.p1.m2" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> that has a clear interpretation as synapse weights that connect neurons (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2" title="2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>). On the other hand, BDH-GPU’s state <math alttext="{\boldsymbol{\rho}}" class="ltx_Math" display="inline" id="S6.SS2.p1.m3" intent=":literal"><semantics><mi>𝝆</mi><annotation encoding="application/x-tex">{\boldsymbol{\rho}}</annotation></semantics></math> is a <math alttext="n\times d" class="ltx_Math" display="inline" id="S6.SS2.p1.m4" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n\times d</annotation></semantics></math> matrix. To perform the analysis for BDH-GPU, in this section we recover <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS2.p1.m5" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> from the relation:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\boldsymbol{\sigma}}_{t-1,l}=\sum_{\tau&lt;t}{y_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}" class="ltx_Math" display="block" id="S6.E16.m1" intent=":literal"><semantics><mrow><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></munder><mrow><msub><mi>y</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mi>t</mi><mo>−</mo><mi>τ</mi></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t-1,l}=\sum_{\tau&lt;t}{y_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="222" id="S6.F11.g1" src="sigma_degrees_powerlaw.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>BDH’s state <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.F11.m2" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> encodes neuron connections as a scale-free graph showing clear heavy-tailed (power-law-like) degree distribution.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p">We first analyze the neuron relationship graph encoded by matrix <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS2.p2.m1" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math>. As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.SS2" title="2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2.2</span></a>, <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS2.p2.m2" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> can be interpreted as a graph of context dependent implications between <math alttext="x" class="ltx_Math" display="inline" id="S6.SS2.p2.m3" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S6.SS2.p2.m4" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>. We compute the <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS2.p2.m5" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> matrix for 0-th head at layer 5 of an 8-th layer network trained on Europarl translation corpus <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib57" title="">2005</a>)</cite> (we provide more details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS3" title="B.3 BDH Monosemantic Synapse Experiment Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.3</span></a>). We filter out negative entries which are introduced by the RoPE positional embeddings <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib100" title="">2024</a>)</cite> and enforce a small positive threshold on remaining values to further sparsify the network structure. We plot the histograms of neuron in- and out-degrees, unraveling a scale-free network structure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p">Encouraged by the emergent network structure, we have identified a few synapses that are activated at recognizable concepts, we show examples in the next section.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Empirical findings: monosemantic synapses</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p">We have identified in the <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS3.p1.m1" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> matrix entries (synapses) that show activity whenever a currency name or country name, both frequently occurring in the Euro-parliament transcripts, is present in the processed sentence. We have identified the synapses by searching for entries in <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="S6.SS3.p1.m2" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math> that have predictive power at separating sentences containing a concept from contrast sentences. We present a few examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F12" title="Figure 12 ‣ 6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">12</span></a>. We note that the synapses strength changes abruptly after words that are related to each concept. The same synapse is activated for concepts in both French and English sentences, even when the words used are different (e.g. “livre sterling” vs “British Pound”). Synapse selectivity to a semantic context stems directly from sparsity of neuron activations as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F13" title="Figure 13 ‣ 6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="641" id="S6.F12.g1" src="money_country_synapses_enfrespt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Evolution of values set by BDH-GPU on 2 specific synapses which we have named (following their interpretation) as “currency synapse” and “country synapse”, relating to concepts naturally present in European Parliament transcripts on which the model was trained. We can notice that mentions of country or currency names result in an increase of the respective synapse value, indicating a stronger presence of the concept in the context. Moreover, the synapses consistently became activated in both French and English, confirming the (notice how it reacts both to “British Pound” and “livre sterling”).
<br class="ltx_break"/>For visual clarity, we indicate changes that clear a small threshold with the <math alttext="*" class="ltx_Math" display="inline" id="S6.F12.m2" intent=":literal"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math> character (the changes in activity when the system is processing the translation of a source sentence tend to be small).</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S6.F13.g1" src="x7.png" width="561"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Sparse updates to synapses related to meaningful concepts stem from sparse neuronal activations. BDH-GPU maintains in its recurrent state a “currency synapse” (a concept naturally present in the Europarl corpus, see also Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F12" title="Figure 12 ‣ 6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">12</span></a>). The synapse is updated using a Hebbian learning rule when activity in <math alttext="y" class="ltx_Math" display="inline" id="S6.F13.m3" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> activations at a preceding layer (4 in the example) leads to firing of neuron <math alttext="x" class="ltx_Math" display="inline" id="S6.F13.m4" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> in the next layer (5).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p">To confirm the selectivity of the synapses, we have generated, using ChatGPT, 50 sentences relating to European currencies, and another set of 50 sentences speaking about European politics, but not mentioning currencies. A one-sided Mann–Whitney U test revealed that sentences relating to currencies received significantly higher “Currency synapse” values than those without the currency concept (<math alttext="U=2368" class="ltx_Math" display="inline" id="S6.SS3.p2.m1" intent=":literal"><semantics><mrow><mi>U</mi><mo>=</mo><mn>2368</mn></mrow><annotation encoding="application/x-tex">U=2368</annotation></semantics></math> with <math alttext="U_{\textrm{opt}}=2500" class="ltx_Math" display="inline" id="S6.SS3.p2.m2" intent=":literal"><semantics><mrow><msub><mi>U</mi><mtext>opt</mtext></msub><mo>=</mo><mn>2500</mn></mrow><annotation encoding="application/x-tex">U_{\textrm{opt}}=2500</annotation></semantics></math>, <math alttext="p&lt;10^{-14}" class="ltx_Math" display="inline" id="S6.SS3.p2.m3" intent=":literal"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>14</mn></mrow></msup></mrow><annotation encoding="application/x-tex">p&lt;10^{-14}</annotation></semantics></math>). The rank-biserial correlation was <math alttext="0.86" class="ltx_Math" display="inline" id="S6.SS3.p2.m4" intent=":literal"><semantics><mn>0.86</mn><annotation encoding="application/x-tex">0.86</annotation></semantics></math>, further confirming association between Currency concept presence and synapse value.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Empirical findings: sparse neuron activations</h3>
<div class="ltx_para ltx_noindent" id="S6.SS4.p1">
<p class="ltx_p">Sparsity of signals is often a prerequisite to their interpretability. In section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS3" title="6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.3</span></a> we have shown that BDH has monosemantic synapses, selectively activated by occurrences of specific concepts. In this section, we experimentally show that neuron activity correlates with signal predictability: fewer neurons are active, or equivalently, layer activations become sparser, for more predictable input signals.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p2">
<p class="ltx_p">We have trained a BDH-GPU model with <math alttext="n=65536" class="ltx_Math" display="inline" id="S6.SS4.p2.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>65536</mn></mrow><annotation encoding="application/x-tex">n=65536</annotation></semantics></math> neurons, <math alttext="d=256" class="ltx_Math" display="inline" id="S6.SS4.p2.m2" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">d=256</annotation></semantics></math>, <math alttext="L=4" class="ltx_Math" display="inline" id="S6.SS4.p2.m3" intent=":literal"><semantics><mrow><mi>L</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">L=4</annotation></semantics></math> layers, and tokenization on letters of the Latin alphabet, to perform a single synthetic next-token prediction task. The input sequence started with a fixed <math alttext="13" class="ltx_Math" display="inline" id="S6.SS4.p2.m4" intent=":literal"><semantics><mn>13</mn><annotation encoding="application/x-tex">13</annotation></semantics></math>-letter warm-up sequence, followed by <math alttext="8" class="ltx_Math" display="inline" id="S6.SS4.p2.m5" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> repetitions of an <math alttext="8" class="ltx_Math" display="inline" id="S6.SS4.p2.m6" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math>-letter random word (“fact”), with the same pattern repeating every <math alttext="13+8\cdot 8=77" class="ltx_Math" display="inline" id="S6.SS4.p2.m7" intent=":literal"><semantics><mrow><mrow><mn>13</mn><mo>+</mo><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>8</mn></mrow></mrow><mo>=</mo><mn>77</mn></mrow><annotation encoding="application/x-tex">13+8\cdot 8=77</annotation></semantics></math> letters. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F14" title="Figure 14 ‣ 6.4 Empirical findings: sparse neuron activations ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>, we show neuron activity patterns. We can notice that neurons in higher layers are active during warm-up and fact introduction, then become quiet. We then group neurons by their RoPE frequencies and find that largest difference of activity during memorization and repetition is shown by the slow-acting neuron population.</p>
</div>
<figure class="ltx_figure" id="S6.F14">
<p class="ltx_p ltx_align_center">(a)
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="218" id="S6.F14.g1" src="fig12av2.png" width="215"/>
(b)
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="218" id="S6.F14.g2" src="fig12v2.png" width="215"/></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Neurons in BDH-GPU are less active (signal is sparser) when the input is predictable. The input sequence started with a fixed <math alttext="13" class="ltx_Math" display="inline" id="S6.F14.m18" intent=":literal"><semantics><mn>13</mn><annotation encoding="application/x-tex">13</annotation></semantics></math>-letter warm-up sequence, followed by <math alttext="8" class="ltx_Math" display="inline" id="S6.F14.m19" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> repetitions of an <math alttext="8" class="ltx_Math" display="inline" id="S6.F14.m20" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math>-letter random word (“fact”), with the same pattern repeating every <math alttext="13+8\cdot 8=77" class="ltx_Math" display="inline" id="S6.F14.m21" intent=":literal"><semantics><mrow><mrow><mn>13</mn><mo>+</mo><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>8</mn></mrow></mrow><mo>=</mo><mn>77</mn></mrow><annotation encoding="application/x-tex">13+8\cdot 8=77</annotation></semantics></math> letters. (a) Fraction of neurons with non-zero entry <math alttext="y_{t,l}" class="ltx_Math" display="inline" id="S6.F14.m22" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l}</annotation></semantics></math> in different layers <math alttext="l" class="ltx_Math" display="inline" id="S6.F14.m23" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, with fact memorization effect noted through increased activation level in layer <math alttext="2" class="ltx_Math" display="inline" id="S6.F14.m24" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>. The activation in layer <math alttext="2" class="ltx_Math" display="inline" id="S6.F14.m25" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> has <math alttext="4.0\%-7.5\%" class="ltx_Math" display="inline" id="S6.F14.m26" intent=":literal"><semantics><mrow><mrow><mn>4.0</mn><mo>%</mo></mrow><mo>−</mo><mrow><mn>7.5</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">4.0\%-7.5\%</annotation></semantics></math> non-zero entries during memorization and approximately <math alttext="2.5\%" class="ltx_Math" display="inline" id="S6.F14.m27" intent=":literal"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">2.5\%</annotation></semantics></math> non-zero entries during repetition. (b) Detailed breakup of activation sparsity in layer <math alttext="2" class="ltx_Math" display="inline" id="S6.F14.m28" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>, with neurons bucketed into equal fractions by their RoPE phase: <math alttext="\textrm{freq}0\in[1,4]" class="ltx_Math" display="inline" id="S6.F14.m29" intent=":literal"><semantics><mrow><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>0</mn></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{freq}0\in[1,4]</annotation></semantics></math>, <math alttext="\textrm{freq}1\in[4,16]" class="ltx_Math" display="inline" id="S6.F14.m30" intent=":literal"><semantics><mrow><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>1</mn></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>4</mn><mo>,</mo><mn>16</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{freq}1\in[4,16]</annotation></semantics></math>, <math alttext="\textrm{freq}2\in[16,64]" class="ltx_Math" display="inline" id="S6.F14.m31" intent=":literal"><semantics><mrow><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>2</mn></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>16</mn><mo>,</mo><mn>64</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{freq}2\in[16,64]</annotation></semantics></math>, <math alttext="\ldots" class="ltx_Math" display="inline" id="S6.F14.m32" intent=":literal"><semantics><mi mathvariant="normal">…</mi><annotation encoding="application/x-tex">\ldots</annotation></semantics></math>, <math alttext="\textrm{freq}7\in[16384,65536]" class="ltx_Math" display="inline" id="S6.F14.m33" intent=":literal"><semantics><mrow><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>7</mn></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>16384</mn><mo>,</mo><mn>65536</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{freq}7\in[16384,65536]</annotation></semantics></math>. The slow-acting half of the neuron population (<math alttext="\textrm{freq}4-\textrm{freq}7" class="ltx_Math" display="inline" id="S6.F14.m34" intent=":literal"><semantics><mrow><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>4</mn></mrow><mo>−</mo><mrow><mtext>freq</mtext><mo lspace="0em" rspace="0em">​</mo><mn>7</mn></mrow></mrow><annotation encoding="application/x-tex">\textrm{freq}4-\textrm{freq}7</annotation></semantics></math>) exhibits the largest amplitude ratio between peak activation during memorization and repetition phases.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS4.p3">
<p class="ltx_p">From a biological standpoint, sparse and surprisal-driven neuron activation lowers energy consumption — despite fluctuations in low level percepts (in the experiment tokens are changing at every timestep), neurons in higher layers are inactive and do not expand energy. From a Deep Learning perspective, it has been recently shown that input complexity is related to predictability of internal representations of Transformers <cite class="ltx_cite ltx_citemacro_citep">(Herrmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib39" title="">2025</a>)</cite>. BDH makes this very explicit and does not require a separate prediction network: the predictable steady-state consists of zero activations, and input complexity entails neuronal activity. This suggests that BDH, natively, at a neuron level, implements mechanisms reminiscent of adaptive computation time <cite class="ltx_cite ltx_citemacro_citep">(Graves, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib34" title="">2017</a>)</cite> and conditional computation <cite class="ltx_cite ltx_citemacro_citep">(Cho and
Bengio, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib21" title="">2014</a>; Shazeer et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib96" title="">2017</a>)</cite>, used in modern Transformers to lower computational effort during inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS4.p4">
<p class="ltx_p">Finally, sparse activation vectors in BDH imply that potentiation of specific synapses occurs rarely during inference. This is useful from the point of view of interpretability, noise reduction in Linear Attention state, and opens the door to simplified and compressed representations, notably for state and for gradient backpropagation DAG’s.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Playing with the Hatchling</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Model merging: concatenating two models</h3>
<div class="ltx_para ltx_noindent" id="S7.SS1.p1">
<p class="ltx_p">Updating models with up-to-date knowledge and expanding models knowledge-base will become crucial in practical applications of AI. On possible solution is model composability, potentially allowing building of larger models by assembling a number of smaller, specialized models into a larger, more powerful one. A natural hope for such a system would be the achievement of ”more is different than a sum of its part” effect. In the following experiment we are showing that doing so is relatively straight forward with BDH-GPU. This is because BDH-GPU can be scaled by varying only the number of neurons <math alttext="n" class="ltx_Math" display="inline" id="S7.SS1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. In this section we explore whether we can create larger models directly concatenating smaller models trained on disjoint subsets of data. Details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS4" title="B.4 BDH Merging Experiment Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.4</span></a>. We have experimented with the following simple model merging procedure:</p>
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p">Train a base model on a chosen language pair. In the experiment we have used English-Spanish (En-Es) translation data, and have trained a model with <math alttext="n=24576" class="ltx_Math" display="inline" id="S7.I1.i1.p1.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mn>24576</mn></mrow><annotation encoding="application/x-tex">n=24576</annotation></semantics></math> neurons (19M parameters).</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p">Clone the base model and continue training on two datasets: English-French (En-Fr) and English-Portuguese (En-Pt).</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p">We then merge the weights of the En-Fr and En-Pt models to create a new En-FrPt model with <math alttext="n=24576\cdot 2=49152" class="ltx_Math" display="inline" id="S7.I1.i3.p1.m1" intent=":literal"><semantics><mrow><mi>n</mi><mo>=</mo><mrow><mn>24576</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><mn>2</mn></mrow><mo>=</mo><mn>49152</mn></mrow><annotation encoding="application/x-tex">n=24576\cdot 2=49152</annotation></semantics></math> neurons (38M parameters).
To create the merged model we:</p>
<ol class="ltx_enumerate" id="S7.I1.i3.I1">
<li class="ltx_item" id="S7.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S7.I1.i3.I1.i1.p1">
<p class="ltx_p">concatenate all parameter tensors that have an ‘<math alttext="n" class="ltx_Math" display="inline" id="S7.I1.i3.I1.i1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>’ dimension (e.g. <math alttext="{D_{y}}" class="ltx_Math" display="inline" id="S7.I1.i3.I1.i1.p1.m2" intent=":literal"><semantics><msub><mi>D</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{D_{y}}</annotation></semantics></math>, <math alttext="{D_{x}}" class="ltx_Math" display="inline" id="S7.I1.i3.I1.i1.p1.m3" intent=":literal"><semantics><msub><mi>D</mi><mi>x</mi></msub><annotation encoding="application/x-tex">{D_{x}}</annotation></semantics></math>, <math alttext="E" class="ltx_Math" display="inline" id="S7.I1.i3.I1.i1.p1.m4" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math>, RoPE frequency buffers) along their <math alttext="n" class="ltx_Math" display="inline" id="S7.I1.i3.I1.i1.p1.m5" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> dimension,</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para ltx_noindent" id="S7.I1.i3.I1.i2.p1">
<p class="ltx_p">average all other parameters (e.g. token embeddings and token prediction weights).</p>
</div>
</li>
</ol>
<p class="ltx_p">To validate the hypothesis that direct model merging is feasible, we report all results on the merged model without any subsequent training or finetuning. However, we have verified that the merged model quickly improves when trained on all language pairs.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S7.I1.i4.p1">
<p class="ltx_p">After each stage we evaluate the models on all involved language pairs: En-Es, En-Fr, En-Pt, regardless of the data seen by the model up to this stage.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S7.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><em class="ltx_emph ltx_font_italic">Translation into English</em></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><em class="ltx_emph ltx_font_italic">Translation from English</em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><em class="ltx_emph ltx_font_italic">Model:</em></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">Es<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>En</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">Fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m2" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>En</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Pt<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m3" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>En</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m4" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>Es</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m5" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>Fr</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">En<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S7.T2.m6" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math>Pt</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1: Base En-Es</th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline">0.36</span></td>
<td class="ltx_td ltx_align_left ltx_border_t">0.77</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_framed ltx_framed_underline">0.35</span></td>
<td class="ltx_td ltx_align_left ltx_border_t">2.21</td>
<td class="ltx_td ltx_align_left ltx_border_t">2.27</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">2: Base (1) tuned on En-Fr</th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_framed ltx_framed_underline">0.58</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_framed ltx_framed_underline">0.36</span></td>
<td class="ltx_td ltx_align_left ltx_border_r">0.68</td>
<td class="ltx_td ltx_align_left">2.57</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_framed ltx_framed_underline">0.31</span></td>
<td class="ltx_td ltx_align_left">2.54</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">3: Base (1) tuned on En-Pt</th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_framed ltx_framed_underline">0.44</span></td>
<td class="ltx_td ltx_align_left">0.76</td>
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_framed ltx_framed_underline">0.34</span></td>
<td class="ltx_td ltx_align_left">1.79</td>
<td class="ltx_td ltx_align_left">2.20</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_framed ltx_framed_underline">0.33</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">4: Merged (2<math alttext="\|" class="ltx_Math" display="inline" id="S7.T2.m7" intent=":literal"><semantics><mo>∥</mo><annotation encoding="application/x-tex">\|</annotation></semantics></math>3)</th>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline">0.43</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_framed ltx_framed_underline">0.40</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span class="ltx_text ltx_framed ltx_framed_underline">0.39</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb">1.45</td>
<td class="ltx_td ltx_align_left ltx_border_bb">0.77</td>
<td class="ltx_td ltx_align_left ltx_border_bb">0.86</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Validation next token prediction losses (lower is better) of translation models trained on different language pairs and then merged. We evaluate each model on En-Es, En-Fr, En-Pt language pairs separately. We can see that the base model can translate between English and Spanish, while on En-Fr and En-Pt tasks it falls back on perplexities of an unconditional English language model (loss about <math alttext="0.65" class="ltx_Math" display="inline" id="S7.T2.m9" intent=":literal"><semantics><mn>0.65</mn><annotation encoding="application/x-tex">0.65</annotation></semantics></math>) and can’t generate proper French or Portuguese. After tuning on French or Portuguese the model learns to translate between respectively English and French or English and Portuguese, while somewhat retaining the capacity to translate Spanish to English and losing the capability to translate English to Spanish. The merged model can translate Spanish, French, and Portuguese to English, however it mixes these three languages when asked to translate from English. This is consistent with qualitative results shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.F15" title="Figure 15 ‣ 7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">15</span></a>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS1.p2">
<p class="ltx_p">We report quantitative results in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.T2" title="Table 2 ‣ 7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a> and show qualitative results of merged model operation in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.F15" title="Figure 15 ‣ 7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">15</span></a>. The merged model shows human-like degradation of operation: while it retained the capability to generate and translate into English, it has lost the ability to generate proper text in Spanish, French, or Portuguese, mixing words and grammatical constructs. We have verified that a small amount of training on all language pairs restore the model’s proficiency in Spanish, French and Portuguese. However, we decided to report on the behavior of the merged model without any subsequent tuning to highlight the possibilities of model engineering offered by the large and sparse working dimension <math alttext="n" class="ltx_Math" display="inline" id="S7.SS1.p2.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> of BDH-GPU.</p>
</div>
<figure class="ltx_figure" id="S7.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel"><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:es&gt;Esta es una afirmación clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">In this clarification, it is a clear statement.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:es&gt;Esta es una afirmación clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">It is a clear statement.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:es&gt;Esta es una afirmación clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">That is a clear affirmation.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_phantom" style="width:433.6pt;"><span style="visibility:hidden"> </span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:fr&gt;C’est une déclaration claire.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">This is a clear statement.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:fr&gt;C’est une déclaration claire.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">This is a clear declaration.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:fr&gt;C’est une déclaration claire.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">It is a clear declaration.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_phantom" style="width:433.6pt;"><span style="visibility:hidden"> </span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:pt&gt;Esta é uma afirmação clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">That is a clear statement.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:pt&gt;Esta é uma afirmação clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">This is a clear statement.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:pt&gt;Esta é uma afirmação clara.&lt;T:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">This is a clear assertion.</span></span></span>(a) Sampled translations from Spanish, French, and Portuguese to English. The vertical bar <math alttext="|" class="ltx_Math" display="inline" id="S7.F15.m1" intent=":literal"><semantics><mo fence="false" stretchy="false">|</mo><annotation encoding="application/x-tex">|</annotation></semantics></math> delimits the prompt from model output. Each translation was sampled three times to show the model’s consistency.
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel"><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:es&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Ce récent statement está clarificative.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:es&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Il se revela exact.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:es&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Constato que cette déclaration était monstruosa.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_phantom" style="width:433.6pt;"><span style="visibility:hidden"> </span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:fr&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Está de noto uma déclaration.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:fr&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Ce sont une declaration clare.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:fr&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">C’está uma declaração clare.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_phantom" style="width:433.6pt;"><span style="visibility:hidden"> </span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:pt&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Esta declaração étangling état está clara.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:pt&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Istambigna de contence.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;This is a clear statement.&lt;T:pt&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">C’estaté clarification é clara!.</span></span></span>(b) Sampled translations from English into Spanish, French, and Portuguese. The model mixes the three languages, though the meaning of the source sentence seems to have been preserved.
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel"><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:en&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">The European Convention on Human Rights has been set up in 1992, when it applied the Convention in 1</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:es&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">Naturalment, nos deputés de toda autre groupe southern Italians, notariously engaged in the discuss</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:fr&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">(ES) Mr President, aproveit de montrar ma satisfaction por a surprise de la parte de Milan, et parti</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:pt&gt;</span> | <span class="ltx_text" style="--ltx-fg-color:#0000FF;">(FI) Mr President, todos outreaches, mesures on ways and means of promoting the economic development</span></span></span>(c) Language-conditional samples. Consistently with the translation experiment in (a) and (b) above, the model properly generates English, and mixes Spanish, French, and Portuguese, sometimes slipping into English.
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel"><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:es&gt;Mi lingua project nominat Romanid esed publicat ja in may de pasat an.&lt;T:en&gt;</span> |</span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"> <span class="ltx_text" style="--ltx-fg-color:#0000FF;">A Lingue nominated Project is Romanian published in Romanian pasta year.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:fr&gt;Mi lingua project nominat Romanid esed publicat ja in may de pasat an.&lt;T:en&gt;</span> |</span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"> <span class="ltx_text" style="--ltx-fg-color:#0000FF;">My language in Project Nomina is Romani esedi publish posted peas in maybe the same year.</span></span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"><span class="ltx_text" style="--ltx-fg-color:#BF8040;">&lt;F:pt&gt;Mi lingua project nominat Romanid esed publicat ja in may de pasat an.&lt;T:en&gt;</span> |</span></span><span class="ltx_text ltx_font_typewriter ltx_inline-block" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" style="width:433.6pt;"> <span class="ltx_text" style="--ltx-fg-color:#0000FF;">I have been a Latva project nomination Romanim esede publica published in May of the postal service and the service provider.</span></span></span>d) Attempts of the model to translate a sentence in Romanid (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib91" title="">2025</a></cite>), a zonal auxiliary language for speakers of Romance languages naturalistic constructed language intended to be intuitively understandable to speakers of Romance languages. The English translation of the prompt is: “My language project called Romanid was published already in May of last year”. The model is able to pick up some of the meaning of the prompt.</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Conditional and unconditional samples generated from a English-Spanish-French-Portuguese translation model created by direct concatenation of parameters of models trained on distinct language pairs.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S7.SS1.p3">
<p class="ltx_p">The BDH-GPU model merging experiment has shown that when the model latent space promotes concept disentangling (c.f. Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.2</span></a> on monosemanticity) then it is feasible to directly compose concepts in this space, e.g. by concatenation of weights from different models. This feature of the BDH architecture allows us to see the models as composable computer programs with emergent properties.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Training without backpropagation through time</h3>
<div class="ltx_para ltx_noindent" id="S7.SS2.p1">
<p class="ltx_p">Sparsity of synapse activations in BDH opens the door to efficient approximations to backpropagation through time. The main intuition is that we only need to remember when a synapse has changed, and the <math alttext="i,j" class="ltx_Math" display="inline" id="S7.SS2.p1.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math> coordinates of the synapse implicitly encode which neurons were active and should take part in error signal backpropagation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p2">
<p class="ltx_p">In this section we report results of preliminary experiments on the impact of removal of backpropagation through time on model performance. For the PyTorch implementation in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>, this corresponds to ‘detach’-ing variables <span class="ltx_text ltx_font_typewriter">K</span> and <span class="ltx_text ltx_font_typewriter">V</span> in the implementation of the <span class="ltx_text ltx_font_sansserif">LinearAttention</span> class.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p3">
<p class="ltx_p">In particular, we found that such a model, trained without any backpropagation through time, retained some ability to model language, but lost the ability to match concepts between different languages during translation. For translation tasks like those presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.T2" title="Table 2 ‣ 7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">2</span></a>, loss values for English increased from a loss level of approximately <math alttext="0.65" class="ltx_Math" display="inline" id="S7.SS2.p3.m1" intent=":literal"><semantics><mn>0.65</mn><annotation encoding="application/x-tex">0.65</annotation></semantics></math> for an unconditional English language model (trained with backpropagation over time), to loss of approximately <math alttext="0.75-1.05" class="ltx_Math" display="inline" id="S7.SS2.p3.m2" intent=":literal"><semantics><mrow><mn>0.75</mn><mo>−</mo><mn>1.05</mn></mrow><annotation encoding="application/x-tex">0.75-1.05</annotation></semantics></math> for a model trained without backpropagation over time, depending on model variant, regardless of whether English was the source or target language in translation. No significant difficulties were encountered during training when crossing the barrier of the letter-bigram language model, at loss value <math alttext="2.4" class="ltx_Math" display="inline" id="S7.SS2.p3.m3" intent=":literal"><semantics><mn>2.4</mn><annotation encoding="application/x-tex">2.4</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p4">
<p class="ltx_p">Beyond side-effects of the general design, we did not optimize the BDH-GPU model for suitability of training without backpropagation. We consider this architecture to be a good starting point for bootstrapping further investigations in this direction.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Takeaways for model engineering</h3>
<div class="ltx_para ltx_noindent" id="S8.SS1.p1">
<p class="ltx_p">This paper leads up to a new class of language and reasoning models which eliminate architecture nonuniformities, notably in terms of scaling for model size, and handling of time scales of inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS1.p2">
<p class="ltx_p">The BDH-GPU architecture introduced in this paper opens the following opportunities:</p>
<ol class="ltx_enumerate" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="S8.I1.i1.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">New ways of scaling models for time and size.</em> BDH-GPU is a state-space model which scales for size in one large dimension <math alttext="n" class="ltx_Math" display="inline" id="S8.I1.i1.p1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> (neurons in this dimension are indexed by RoPE oscillator frequency). Subject to appropriate sharding, this also leads to a desirable form of locality: important data is located just next to the sites at which it is being processed. This minimizes communication, and eliminates the most painful of all bottlenecks for reasoning models during inference: memory-to-core bandwidth.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Faster model iteration.</em> During training and inference alike, BDH-GPU provides insight into parameter and state spaces of the model which allows for easy and direct evaluation of model health and performance, notably, through sparsity-related measures and through aggregates and statistics on the large pool of homogeneous neurons, even for relatively small models. Attention and parametric layers alike operate on the same neuron dimension (‘concept dimension’).</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S8.I1.i3.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Direct explainability of model state.</em> Elements of state of BDH-GPU are directly localized at neuron pairs, allowing for a micro-interpretation of the hidden state of the model.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="S8.I1.i4.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">New opportunities for ‘model surgery’.</em> The BDH-GPU architecture is, in principle, amenable to direct composability of model weights in a way resemblant of composability of programs. This concerns the potential both the direct composition of separately trained model parts, as well as ‘surgery’ of parameter spaces of models, by inserting fragments of manually programmed protocols into machine-learned code.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Implications for brain science</h3>
<div class="ltx_para ltx_noindent" id="S8.SS2.p1">
<p class="ltx_p">We have obtained a micro-foundational description of attention for artificial language and reasoning models, expressed in a framework of local graph dynamics. This has been found to be consistent with the effects observed for the same function of <em class="ltx_emph ltx_font_italic">attention for language and reasoning</em> in the brain. By introducing a translation layer based on similarity of function between the artificial and biological planes, for blocks of feed-forward neural networks and attention mechanisms, our work points to the following hypothesis: <em class="ltx_emph ltx_font_italic">complex systems effects which are observed in the brain, around modular scale-free network structure, synaptic plasticity, and Hebbian learning arose from its core purpose — doing reasoning — and <span class="ltx_text ltx_font_bold">not</span> from any specific longer-term training dynamics which the brain applies</em>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.p2">
<p class="ltx_p">We have exhibited how a general attention mechanism can be efficiently implemented as an artificial neuronal system with spiking neurons and synapse plasticity. More formally, we first describe the class of local interaction dynamics which any system <em class="ltx_emph ltx_font_italic">plausibly needs</em> to implement attention mechanisms. We then confirmed that the edge-reweighting rule is <em class="ltx_emph ltx_font_italic">sufficient</em> to allow a certain artificial Language Model (BDH-GPU) to operate at least at the level of the Transformer. For an artificial network, the edge-reweighting rule intuitively describes the interaction between two artificial neurons exhibiting rapid state-change behavior, and one synaptic neuron interconnection element exhibiting plasticity as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F13" title="Figure 13 ‣ 6.3 Empirical findings: monosemantic synapses ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.p3">
<p class="ltx_p">More broadly, this work may potentially serve to support efforts aiming to isolate, from among the many extremely complex electrochemical patterns and signal dynamics occurring in the brain, those that are crucial for <em class="ltx_emph ltx_font_italic">solving tasks in-context (based on attention)</em>, from those that potentially serve other purposes, such as transfer of information from short-term memory to long-term memory, or long-term improvement of brain function (learning).</p>
</div>
<section class="ltx_paragraph" id="S8.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">How this work helps with axiomatization of learning theory in the brain.</h5>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p1">
<p class="ltx_p">Attempts to understand the brain, starting from the perspective of longer time scales of training, have proved extremely challenging, defying progress. This paper pin-points attention-based reasoning at shorter time scales as ‘the other end of the string’, and hints how, from here, untangling the entire story will plausibly be easier.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p2">
<p class="ltx_p">For natural systems undergoing continuous learning, the time scales to look at are: language function and reasoning (chain-of-thought inference), then short-to-long memory transfer from state to network weights, adaptation of structure: changes to interconnections, and finally, changes to neuron nodes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p3">
<p class="ltx_p">For long time scales, this reduces the question of finding supervised training dynamics form the most general case, to a specific class of local dynamics: an interaction kernel performing ‘edge-reweighting’ rules. As these rules appear fundamental to logical inference and biochemical processes alike, its universality in processes that the brain is responsible for is plausible also beyond the realm of language-based reasoning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p4">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">From a systems perspective, we arrive at the following possible explanation.</em> The brain generally tries to be lazy in terms of energy expense, and does things as late as it can. Only reasoning needs to happen close to a critical regime, because it involves executing a real-time program which needs to be responsive, since the life and success of the biological organism depends on it. Then, for a certain time, which may be minutes for humans, the brain has enough synapses in it to represent (almost) all useful information it needs for reasoning, decision-making, etc. — all stored in short-term state, at synapses (and/or neurons). Some of the neuron activations which the brain performs at this time scale represent ‘gradients of state’ — the gradients of in-context learning, passed on to modify synapse strength, in a weight-update process. As time goes by, the system runs out of state space. Then, memory processes work to iron things out, preserving in more permanent neuron connection weights and graph structure the elements of state that have been reinforced by feedback signals. Overall, there are fewer and fewer things that need to be remembered across progressively longer time scales. However, this entire memory process is, plausibly, subsidiary to the definition of the dynamics of reasoning and the synaptic dynamics of state that we discuss in this paper. In other words, the best form of description of the relaxation from state into longer-term memory follows from the specific kernel of the reasoning dynamics, such as the edge-reweighting kernel.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p5">
<p class="ltx_p">As for the ratio of time scales (measured in tokens for language), we can estimate that the time lapse after which harmonizing state with a memory process becomes important is of about the same order of magnitude as the average time between ‘writes’ (significant transmission increases) for individual synaptic elements (see e.g. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.F14" title="Figure 14 ‣ 6.4 Empirical findings: sparse neuron activations ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>). In our models, this time is lower-bounded by the inverse of sparsity of the vector <math alttext="y" class="ltx_Math" display="inline" id="S8.SS2.SSS0.Px1.p5.m1" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>, i.e., <math alttext="1/\rho\approx 1/5\%=20" class="ltx_Math" display="inline" id="S8.SS2.SSS0.Px1.p5.m2" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mi>ρ</mi></mrow><mo>≈</mo><mrow><mn>1</mn><mo>/</mo><mrow><mn>5</mn><mo>%</mo></mrow></mrow><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">1/\rho\approx 1/5\%=20</annotation></semantics></math> tokens, but it could be much larger for larger systems; we also do not force it in any way to be sparser during training. During training with backpropagation, if the backpropagation window <math alttext="T" class="ltx_Math" display="inline" id="S8.SS2.SSS0.Px1.p5.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is short enough, <math alttext="T&lt;1/\rho" class="ltx_Math" display="inline" id="S8.SS2.SSS0.Px1.p5.m4" intent=":literal"><semantics><mrow><mi>T</mi><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mi>ρ</mi></mrow></mrow><annotation encoding="application/x-tex">T&lt;1/\rho</annotation></semantics></math> tokens, we can plausibly assume that a synapse changes state only once in that window (and is used multiple times), hence the DAG of gradient backwards propagation is much more direct to embed within the system graph. Backpropagation is then a question of ‘routing’ gradients in the neuron communication graph, and not one of disentangling them. All natural training approaches, whether based on backpropagation, or any more direct form of relaxation ‘from state into weights’, appear to bottleneck on the amount of available state space on synapses, becoming necessary at about <math alttext="T\sim 1/\rho" class="ltx_Math" display="inline" id="S8.SS2.SSS0.Px1.p5.m5" intent=":literal"><semantics><mrow><mi>T</mi><mo>∼</mo><mrow><mn>1</mn><mo>/</mo><mi>ρ</mi></mrow></mrow><annotation encoding="application/x-tex">T\sim 1/\rho</annotation></semantics></math> by a simple information-theoretic argument on state storage capacity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p6">
<p class="ltx_p">Regardless of how much of this is an accurate description, and how much an intuition, at the very least, it appears we may now have a way forward. Some part of the “global mystery” of learning in the brain can be reduced to a more “localized problem” of state-to-operator transfer for some relatively compact form of state-space dynamics (i.e., one specific local graph kernel). This change of perspective brings in both a completely new ‘problem landscape’ in which to navigate towards a complete solution, as well as a set of new methods to use for the different types of graph structure changes involved in learning, including approaches from distributed computing, evolving network theory, and graph rewiring systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS2.SSS0.Px1.p7">
<p class="ltx_p">At this point, it seems one natural next step would be to ground the current discussion more deeply in findings of brain science, to refine or simplify the <em class="ltx_emph ltx_font_italic">actual kernels</em> used by brain reasoning (which was not the objective of this paper), and potentially seek validation through experiment.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3 </span>Societal impact</h3>
<div class="ltx_para ltx_noindent" id="S8.SS3.p1">
<p class="ltx_p">This paper is a voice in favor of bringing principled understanding to reasoning in Machine Learning. Axiomatic AI provides an opportunity to reduce risks related to unpredictable behavior of AI models, and, to open or accelerate new development directions. The subject matter which we consider here serves as a direct introduction to the most crucial problem that lies ahead: controlling the behavior of autonomous AI reasoning models and AI systems as they progress across time scales, from seconds to years.</p>
</div>
<div class="ltx_table ltx_transformed_outer" id="S8.T3" style="width:234.9pt;height:345pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:345.0pt;transform:translate(-55.0pt,-55.0pt) rotate(-90deg) ;"><figure>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt"></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><span class="ltx_text ltx_font_bold">Transformer (GPT2)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><span class="ltx_text ltx_font_bold">BDH-GPU (<math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,<math alttext="d" class="ltx_Math" display="inline" id="S8.T3.m2" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><span class="ltx_text ltx_font_bold">BDH(<math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,<math alttext="\Delta" class="ltx_Math" display="inline" id="S8.T3.m4" intent=":literal"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><span class="ltx_text ltx_font_bold">Brain models (reasoning and language function)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Inference hardware</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">GPU, CPU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">GPU, CPU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">CPU, Sparse GPU kernels, Neuromorphic</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Brain and supporting systems</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Model weights (predominant location)</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="5" class="ltx_Math" display="inline" id="S8.T3.m5" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math> tensors per layer (different shapes)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="3" class="ltx_Math" display="inline" id="S8.T3.m6" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math> tensors per model (same shape <math alttext="n\times d" class="ltx_Math" display="inline" id="S8.T3.m7" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n\times d</annotation></semantics></math>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Neuron-synapse graph: connection topology, edge weights</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Neuron-synapse graph: connection topology, edge weights</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Representation of attention</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">KV-cache tensor (not localized at neurons)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="n\times d" class="ltx_Math" display="inline" id="S8.T3.m8" intent=":literal"><semantics><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n\times d</annotation></semantics></math> tensor for each layer (localized at neurons)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Memory on synapse edge weights</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" style="padding-bottom:14.22636pt;">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">State memory through synapse plasticity</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Macro-description of attention</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Key lookup data-structure, key-value map</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Key lookup data-structure, key-value correlation matrix</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Key lookup data-structure, key-value correlation matrix</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Not known</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Micro-description of attention</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">None</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Neuron-pair correlations in context (transformed)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Neuron-pair correlations in context</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Strengthened or weakened connections between neurons based on context</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Scaling for model size</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Multiple combinations of dimensions, e.g. MLP scales with <math alttext="D\times D" class="ltx_Math" display="inline" id="S8.T3.m9" intent=":literal"><semantics><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow><annotation encoding="application/x-tex">D\times D</annotation></semantics></math>, scales separately with context length</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Uniform linear array of <math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m10" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> particles in a mean-field</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m11" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-node graph model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m12" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-node graph model with evolving graph mechanisms</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Distributed system micro-architecture</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Follows from compiled matrix multiplication kernels, non-uniform</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Particles run identical local kernels, communicating <math alttext="O(d)" class="ltx_Math" display="inline" id="S8.T3.m13" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math>-size messages through mean-field, and storing local state</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">All <math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m14" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neuron nodes run identical local kernels, communicating over neuron-synapse graphs. Some synapses act as memory elements.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><math alttext="n" class="ltx_Math" display="inline" id="S8.T3.m15" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> neurons run local kernels and communicate through a network, using numerous signal patterns and coding schemes. Synapses act as memory element.</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Macro-expressiveness of programs (approximation)</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">RASP-L, C-RASP</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">RASP-L, C-RASP</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">RASP-L, C-RASP (or superset)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Unknown</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Micro-expressiveness of programs</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Unknown</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Subset of BDH</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Probabilistic rule-based local protocols. Micro-Inductive bias interpreted as reasoning system in a form of propositional logic.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Unknown</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Emergence of structure</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Partially interpretable concept layer (evidence of monosemantic neurons for important concepts)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Evidence of emergent network, oscillator dynamics</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Emergent network, oscillator dynamics</span>
</span>
</td>
<td class="ltx_td ltx_align_justify">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Emergent network; oscillatory effects; possible monosemantic “grandfather neurons”</span>
</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left"><em class="ltx_emph ltx_font_italic">Activation vectors</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Dense activation; can be subsampled or sparsified by architecture modification</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Positive vectors, sparse fresh activation vectors <math alttext="y" class="ltx_Math" display="inline" id="S8.T3.m16" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Positive vectors, sparse fresh activation vectors <math alttext="y" class="ltx_Math" display="inline" id="S8.T3.m17" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p ltx_align_left">Sparse positive activation vectors</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of properties of language and reasoning model architectures: the GPT2 Transformer, BDH-GPU, BDH, and brain models.</figcaption>
</figure></div></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="S8.SSx1">
<h3 class="ltx_title ltx_title_subsection">Acknowledgments</h3>
<div class="ltx_para ltx_noindent" id="S8.SSx1.p1">
<p class="ltx_p">The authors thank David Sussillo, Navdeep Jaitly, and Emanuele Natale for insightful discussions on reasoning and the brain, and for early feedback on this write-up.
We also thank Samy Bengio for comments on the presentation.
We kindly acknowledge the support of all of the Pathway team, notably, Paweł Podhajski for his amazing help with cluster setup, Victor Szczerba and Z Schwab for all discussions over coffee, and Kamil Piechowiak and Chris Ociepa for constructive comments on the presentation. AK thanks Christos Papadimitriou for being the direct inspiration for us to embark on this journey.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SSx2">
<h3 class="ltx_title ltx_title_subsection">Author contributions</h3>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p1">
<p class="ltx_p">AK conceived the BDH and BDH-GPU architectures, conceived most of the theory, developed most of the model source code, conceived and performed experiments on synapses, and wrote most of the paper.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p2">
<p class="ltx_p">PU contributed crucial elements of BDH-GPU architecture, contributed model and framework source code, contributed to theoretical analysis, and performed experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p3">
<p class="ltx_p">JCh led, designed, and oversaw methodology of experiments, led framework development, contributed major improvements to BDH-GPU architecture, contributed to the theory, implemented baselines, performed experiments, and substantially redacted the paper.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p4">
<p class="ltx_p">ZS conceived the project, guided research directions, introduced particle-interaction interpretation, acted as final judge in research decisions, and substantially redacted the paper.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SSx2.p5">
<p class="ltx_p">MB optimized model source code, contributed framework source code, and performed experiments.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abraham et al. (2011)</span>
<span class="ltx_bibblock">
I. Abraham, D. Delling, A. V. Goldberg, and R. F. Werneck.

</span>
<span class="ltx_bibblock">A hub-based labeling algorithm for shortest paths in road networks.

</span>
<span class="ltx_bibblock">In P. M. Pardalos and S. Rebennack, editors, <em class="ltx_emph ltx_font_italic">Experimental
Algorithms</em>, pages 230–241, Berlin, Heidelberg, 2011. Springer Berlin
Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-642-20662-7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achlioptas and Mcsherry (2007)</span>
<span class="ltx_bibblock">
D. Achlioptas and F. Mcsherry.

</span>
<span class="ltx_bibblock">Fast computation of low-rank matrix approximations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">J. ACM</em>, 54(2):9–es, Apr. 2007.

</span>
<span class="ltx_bibblock">ISSN 0004-5411.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1219092.1219097" title="">https://doi.org/10.1145/1219092.1219097</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angluin et al. (2006)</span>
<span class="ltx_bibblock">
D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta.

</span>
<span class="ltx_bibblock">Computation in networks of passively mobile finite-state sensors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Distributed Comput.</em>, 18(4):235–253, 2006.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00446-005-0138-3" title="">https://doi.org/10.1007/s00446-005-0138-3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aspnes and Ruppert (2009)</span>
<span class="ltx_bibblock">
J. Aspnes and E. Ruppert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">An Introduction to Population Protocols</em>, pages 97–120.

</span>
<span class="ltx_bibblock">Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-89707-1.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-540-89707-1_5" title="">https://doi.org/10.1007/978-3-540-89707-1_5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et al. (2016a)</span>
<span class="ltx_bibblock">
J. Ba, G. E. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu.

</span>
<span class="ltx_bibblock">Using fast weights to attend to the recent past.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 29,
2016a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et al. (2016b)</span>
<span class="ltx_bibblock">
J. L. Ba, J. R. Kiros, and G. E. Hinton.

</span>
<span class="ltx_bibblock">Layer normalization, 2016b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1607.06450" title="">https://arxiv.org/abs/1607.06450</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2015)</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock">In Y. Bengio and Y. LeCun, editors, <em class="ltx_emph ltx_font_italic">3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9, 2015, Conference Track Proceedings</em>, 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1409.0473" title="">http://arxiv.org/abs/1409.0473</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barlow (1972)</span>
<span class="ltx_bibblock">
H. B. Barlow.

</span>
<span class="ltx_bibblock">Single units and sensation: A neuron doctrine for perceptual
psychology?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Perception</em>, 1(4):371–394, 1972.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1068/p010371" title="">https://doi.org/10.1068/p010371</a>.

</span>
<span class="ltx_bibblock">PMID: 4377168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Becchetti et al. (2018)</span>
<span class="ltx_bibblock">
L. Becchetti, V. Bonifaci, and E. Natale.

</span>
<span class="ltx_bibblock">Pooling or sampling: Collective dynamics for electrical flow
estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 17th International Conference on
Autonomous Agents and MultiAgent Systems</em>, AAMAS ’18, page 1576–1584,
Richland, SC, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beck et al. (2024)</span>
<span class="ltx_bibblock">
M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp,
G. Klambauer, J. Brandstetter, and S. Hochreiter.

</span>
<span class="ltx_bibblock">xlstm: Extended long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
37:107547–107603, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-Kish et al. (2025)</span>
<span class="ltx_bibblock">
A. Ben-Kish, I. Zimerman, M. J. Mirza, J. Glass, L. Karlinsky, and R. Giryes.

</span>
<span class="ltx_bibblock">Overflow prevention enhances long-context recurrent llms, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2505.07793" title="">https://arxiv.org/abs/2505.07793</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Björner et al. (1991)</span>
<span class="ltx_bibblock">
A. Björner, L. Lovász, and P. W. Shor.

</span>
<span class="ltx_bibblock">Chip-firing games on graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">European Journal of Combinatorics</em>, 12(4):283–291, 1991.

</span>
<span class="ltx_bibblock">ISSN 0195-6698.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/S0195-6698(13)80111-4" title="">https://doi.org/10.1016/S0195-6698(13)80111-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blondel et al. (2008)</span>
<span class="ltx_bibblock">
V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre.

</span>
<span class="ltx_bibblock">Fast unfolding of communities in large networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Statistical Mechanics: Theory and Experiment</em>,
2008(10):P10008, oct 2008.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dx.doi.org/10.1088/1742-5468/2008/10/P10008" title="">https://dx.doi.org/10.1088/1742-5468/2008/10/P10008</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boczkowski et al. (2019)</span>
<span class="ltx_bibblock">
L. Boczkowski, A. Korman, and E. Natale.

</span>
<span class="ltx_bibblock">Minimizing message size in stochastic communication patterns: fast
self-stabilizing protocols with 3 bits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Distributed Comput.</em>, 32(3):173–191, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00446-018-0330-x" title="">https://doi.org/10.1007/s00446-018-0330-x</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bostrom (2014)</span>
<span class="ltx_bibblock">
N. Bostrom.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Superintelligence: Paths, Dangers, Strategies</em>.

</span>
<span class="ltx_bibblock">Oxford University Press, Inc., USA, 1st edition, 2014.

</span>
<span class="ltx_bibblock">ISBN 0199678111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunel (1996)</span>
<span class="ltx_bibblock">
N. Brunel.

</span>
<span class="ltx_bibblock">Hebbian learning of context in recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neural Computation</em>, 8(8):1677–1710, 1996.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/6796169" title="">https://ieeexplore.ieee.org/document/6796169</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buckman et al. (2024)</span>
<span class="ltx_bibblock">
J. Buckman, C. Gelada, and S. Zhang.

</span>
<span class="ltx_bibblock">Symmetric Power Transformers, 2024.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://manifestai.com/articles/symmetric-power-transformers/" title="">https://manifestai.com/articles/symmetric-power-transformers/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budzinskiy (2025)</span>
<span class="ltx_bibblock">
S. Budzinskiy.

</span>
<span class="ltx_bibblock">When big data actually are low-rank, or entrywise approximation of
certain function-generated matrices, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.03250" title="">https://arxiv.org/abs/2407.03250</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cairns (2018)</span>
<span class="ltx_bibblock">
H. Cairns.

</span>
<span class="ltx_bibblock">Some halting problems for abelian sandpiles are undecidable in
dimension three.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">SIAM Journal on Discrete Mathematics</em>, 32(4):2636–2666, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1137/16M1091964" title="">https://doi.org/10.1137/16M1091964</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2014)</span>
<span class="ltx_bibblock">
Y. Chen, D. Doty, and Soloveichik.

</span>
<span class="ltx_bibblock">Deterministic function computation with chemical reaction networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nat Comput</em>, 13:517–534, 2014.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/article/10.1007/s11047-013-9393-6" title="">https://link.springer.com/article/10.1007/s11047-013-9393-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho and
Bengio (2014)</span>
<span class="ltx_bibblock">
K. Cho and Y. Bengio.

</span>
<span class="ltx_bibblock">Exponentially increasing the capacity-to-computation ratio for
conditional computation in deep learning, 2014.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1406.7362" title="">https://arxiv.org/abs/1406.7362</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski et al. (2021)</span>
<span class="ltx_bibblock">
K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J.
Colwell, and A. Weller.

</span>
<span class="ltx_bibblock">Rethinking attention with performers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Ua6zuk0WRH" title="">https://openreview.net/forum?id=Ua6zuk0WRH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. (2011)</span>
<span class="ltx_bibblock">
P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.-H. Teng.

</span>
<span class="ltx_bibblock">Electrical flows, laplacian systems, and faster approximation of
maximum flow in undirected graphs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Forty-Third Annual ACM Symposium on
Theory of Computing</em>, STOC ’11, page 273–282, New York, NY, USA, 2011.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450306911.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1993636.1993674" title="">https://doi.org/10.1145/1993636.1993674</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Czyzowicz et al. (2022)</span>
<span class="ltx_bibblock">
J. Czyzowicz, L. Gasieniec, A. Kosowski, E. Kranakis, P. G. Spirakis, and
P. Uznański.

</span>
<span class="ltx_bibblock">On convergence and threshold properties of discrete lotka-volterra
population protocols.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">J. Comput. Syst. Sci.</em>, 130:1–25, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jcss.2022.06.002" title="">https://doi.org/10.1016/j.jcss.2022.06.002</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabagia et al. (2024)</span>
<span class="ltx_bibblock">
M. Dabagia, C. H. Papadimitriou, and S. S. Vempala.

</span>
<span class="ltx_bibblock">Computation with sequences of assemblies in a model of the brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neural Computation</em>, 37(1):193–233, 12
2024.

</span>
<span class="ltx_bibblock">ISSN 0899-7667.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/neco_a_01720" title="">https://doi.org/10.1162/neco_a_01720</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2019)</span>
<span class="ltx_bibblock">
Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Transformer-XL: Attentive language models beyond a fixed-length
context.

</span>
<span class="ltx_bibblock">In A. Korhonen, D. R. Traum, and L. Màrquez, editors,
<em class="ltx_emph ltx_font_italic">Proceedings of the 57th Conference of the Association for Computational
Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:
Long Papers</em>, pages 2978–2988. Association for Computational Linguistics,
2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/V1/P19-1285</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/p19-1285" title="">https://doi.org/10.18653/v1/p19-1285</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al. (2019)</span>
<span class="ltx_bibblock">
M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Łukasz Kaiser.

</span>
<span class="ltx_bibblock">Universal transformers, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1807.03819" title="">https://arxiv.org/abs/1807.03819</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolev (2000)</span>
<span class="ltx_bibblock">
S. Dolev.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Self-stabilization</em>.

</span>
<span class="ltx_bibblock">The MIT Press, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doty et al. (2021)</span>
<span class="ltx_bibblock">
D. Doty, M. Eftekhari, L. Gasieniec, E. E. Severson, P. Uznański, and
G. Stachowiak.

</span>
<span class="ltx_bibblock">A time and space optimal stable population protocol solving exact
majority.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">62nd IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022</em>, pages
1044–1055. IEEE, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/FOCS52979.2021.00104" title="">https://doi.org/10.1109/FOCS52979.2021.00104</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dudek and Kosowski (2018)</span>
<span class="ltx_bibblock">
B. Dudek and A. Kosowski.

</span>
<span class="ltx_bibblock">Universal protocols for information dissemination using emergent
signals.

</span>
<span class="ltx_bibblock">In I. Diakonikolas, D. Kempe, and M. Henzinger, editors,
<em class="ltx_emph ltx_font_italic">Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018</em>, pages
87–99. ACM, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3188745.3188818" title="">https://doi.org/10.1145/3188745.3188818</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Emberson et al. (2025)</span>
<span class="ltx_bibblock">
L. Emberson, B. Cottier, J. You, T. Adamczewski, and J.-S. Denain.

</span>
<span class="ltx_bibblock">LLM responses to benchmark questions are getting longer over time,
2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epoch.ai/data-insights/output-length" title="">https://epoch.ai/data-insights/output-length</a>.

</span>
<span class="ltx_bibblock">Accessed: 2025-07-25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feinberg (2019)</span>
<span class="ltx_bibblock">
M. Feinberg.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Foundations of Chemical Reaction Network Theory</em>, volume 202 of
<em class="ltx_emph ltx_font_italic">Applied Mathematical Sciences</em>.

</span>
<span class="ltx_bibblock">Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraigniaud and Giakkoupis (2010)</span>
<span class="ltx_bibblock">
P. Fraigniaud and G. Giakkoupis.

</span>
<span class="ltx_bibblock">On the searchability of small-world networks with arbitrary
underlying structure.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Forty-Second ACM Symposium on Theory of
Computing</em>, STOC ’10, page 389–398, New York, NY, USA, 2010. Association
for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450300506.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/1806689.1806744</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1806689.1806744" title="">https://doi.org/10.1145/1806689.1806744</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2017)</span>
<span class="ltx_bibblock">
A. Graves.

</span>
<span class="ltx_bibblock">Adaptive computation time for recurrent neural networks, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1603.08983" title="">https://arxiv.org/abs/1603.08983</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu and Dao (2024)</span>
<span class="ltx_bibblock">
A. Gu and T. Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces,
2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.00752" title="">https://arxiv.org/abs/2312.00752</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haziza et al. (2025)</span>
<span class="ltx_bibblock">
D. Haziza, T. Chou, D. Choudhary, L. Wehrstedt, F. Massa, J. Yu, G. Jeong,
S. Rao, P. Labatut, and J. Cai.

</span>
<span class="ltx_bibblock">Accelerating transformer inference and training with 2:4 activation
sparsity, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.16672" title="">https://arxiv.org/abs/2503.16672</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2010)</span>
<span class="ltx_bibblock">
B. J. He, J. M. Zempel, A. Z. Snyder, and M. E. Raichle.

</span>
<span class="ltx_bibblock">The temporal structures and functional significance of scale-free
brain activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neuron</em>, 66(3):353–369, 2010.

</span>
<span class="ltx_bibblock">ISSN 0896-6273.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neuron.2010.04.020" title="">https://doi.org/10.1016/j.neuron.2010.04.020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hebb (1949)</span>
<span class="ltx_bibblock">
D. O. Hebb.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Organization of behavior.</em>
</span>
<span class="ltx_bibblock">New York: Wiley &amp; Sons, 1949.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herrmann et al. (2025)</span>
<span class="ltx_bibblock">
V. Herrmann, R. Csordás, and J. Schmidhuber.

</span>
<span class="ltx_bibblock">Measuring in-context computation complexity via hidden state
prediction, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.13431" title="">https://arxiv.org/abs/2503.13431</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hilbert (1902)</span>
<span class="ltx_bibblock">
D. Hilbert.

</span>
<span class="ltx_bibblock">Mathematical problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Bull. AMS</em>, 8(10):437–479, 1902.

</span>
<span class="ltx_bibblock">ISSN 0002-9904 (print), 1936-881X (electronic).

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1090/S0002-9904-1902-00923-3" title="">https://doi.org/10.1090/S0002-9904-1902-00923-3</a>.

</span>
<span class="ltx_bibblock">English translation of Hilbert’s famous list of 23 important problems
in mathematics for the 20th Century.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton (2005)</span>
<span class="ltx_bibblock">
G. E. Hinton.

</span>
<span class="ltx_bibblock">What kind of a graphical model is the brain?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 19th International Joint Conference on
Artificial Intelligence</em>, IJCAI’05, page 1765–1775, San Francisco, CA, USA,
2005. Morgan Kaufmann Publishers Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton and Plaut (1987)</span>
<span class="ltx_bibblock">
G. E. Hinton and D. C. Plaut.

</span>
<span class="ltx_bibblock">Using fast weights to deblur old memories.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the ninth annual conference of the Cognitive
Science Society</em>, pages 177–186, 1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirvonen and Suomela (2025)</span>
<span class="ltx_bibblock">
J. Hirvonen and J. Suomela.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Distributed Algorithms 2020 — the book</em>.

</span>
<span class="ltx_bibblock">Aalto University, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jukkasuomela.fi/da2020/da2020.pdf" title="">https://jukkasuomela.fi/da2020/da2020.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neural Comput.</em>, 9(8):1735–1780, Nov.
1997.

</span>
<span class="ltx_bibblock">ISSN 0899-7667.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/neco.1997.9.8.1735" title="">https://doi.org/10.1162/neco.1997.9.8.1735</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofbauer and Sigmund (1998)</span>
<span class="ltx_bibblock">
J. Hofbauer and K. Sigmund.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Evolutionary Games and Population Dynamics</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 1998.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cambridge.org/core/books/evolutionary-games-and-population-dynamics/A8D94EBE6A16837E7CB3CED24E1948F8" title="">https://www.cambridge.org/core/books/evolutionary-games-and-population-dynamics/A8D94EBE6A16837E7CB3CED24E1948F8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holland et al. (1983)</span>
<span class="ltx_bibblock">
P. W. Holland, K. B. Laskey, and S. Leinhardt.

</span>
<span class="ltx_bibblock">Stochastic blockmodels: First steps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Social Networks</em>, 5(2):109–137, 1983.

</span>
<span class="ltx_bibblock">ISSN 0378-8733.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/0378873383900217" title="">https://www.sciencedirect.com/science/article/pii/0378873383900217</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.09685" title="">https://arxiv.org/abs/2106.09685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2025)</span>
<span class="ltx_bibblock">
X. Huang, A. Yang, S. Bhattamishra, Y. Sarrof, A. Krebs, H. Zhou, P. Nakkiran,
and M. Hahn.

</span>
<span class="ltx_bibblock">A formal framework for understanding length generalization in
transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning
Representations</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=U49N5V51rU" title="">https://openreview.net/forum?id=U49N5V51rU</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabr and Rothschild (2012)</span>
<span class="ltx_bibblock">
F. Jabr and A. Rothschild.

</span>
<span class="ltx_bibblock">How brainless slime molds redefine intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 7(1), 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jojic et al. (2023)</span>
<span class="ltx_bibblock">
A. Jojic, Z. Wang, and N. Jojic.

</span>
<span class="ltx_bibblock">Gpt is becoming a turing machine: Here are some ways to program it.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14310</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalev and Hen (2025)</span>
<span class="ltx_bibblock">
A. Kalev and I. Hen.

</span>
<span class="ltx_bibblock">Feynman path integrals for discrete-variable systems: Walks on
hamiltonian graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Phys. Rev. Res.</em>, 7:013220, Feb 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.aps.org/doi/10.1103/PhysRevResearch.7.013220" title="">https://link.aps.org/doi/10.1103/PhysRevResearch.7.013220</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy (2024)</span>
<span class="ltx_bibblock">
A. Karpathy.

</span>
<span class="ltx_bibblock">nanoGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/karpathy/nanoGPT" title="">https://github.com/karpathy/nanoGPT</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karrer and Newman (2011)</span>
<span class="ltx_bibblock">
B. Karrer and M. E. J. Newman.

</span>
<span class="ltx_bibblock">Stochastic blockmodels and community structure in networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Phys. Rev. E</em>, 83:016107, Jan 2011.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.aps.org/doi/10.1103/PhysRevE.83.016107" title="">https://link.aps.org/doi/10.1103/PhysRevE.83.016107</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katharopoulos et al. (2020)</span>
<span class="ltx_bibblock">
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret.

</span>
<span class="ltx_bibblock">Transformers are RNNs: Fast autoregressive transformers with linear
attention.

</span>
<span class="ltx_bibblock">In H. D. III and A. Singh, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 37th
International Conference on Machine Learning</em>, volume 119 of
<em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 5156–5165. PMLR,
13–18 Jul 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v119/katharopoulos20a.html" title="">https://proceedings.mlr.press/v119/katharopoulos20a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kempe et al. (2003)</span>
<span class="ltx_bibblock">
D. Kempe, J. Kleinberg, and E. Tardos.

</span>
<span class="ltx_bibblock">Maximizing the spread of influence through a social network.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">KDD ’03: Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining</em>, pages 137–146, New York,
NY, USA, 2003. ACM Press.

</span>
<span class="ltx_bibblock">ISBN 1-58113-737-0.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.acm.org/10.1145/956750.956769" title="">https://doi.acm.org/10.1145/956750.956769</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitaev et al. (2020)</span>
<span class="ltx_bibblock">
N. Kitaev, Ł. Kaiser, and A. Levskaya.

</span>
<span class="ltx_bibblock">Reformer: The efficient transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rkgNKkHtvB" title="">https://openreview.net/forum?id=rkgNKkHtvB</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
P. Koehn.

</span>
<span class="ltx_bibblock">Europarl: A parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit X: Papers</em>, pages
79–86, Phuket, Thailand, Sept. 13-15 2005.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2005.mtsummit-papers.11/" title="">https://aclanthology.org/2005.mtsummit-papers.11/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kosowski and Uznański (2018)</span>
<span class="ltx_bibblock">
A. Kosowski and P. Uznański.

</span>
<span class="ltx_bibblock">Population protocols are fast.

</span>
<span class="ltx_bibblock">In C. Newport and I. Keidar, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 2018
ACM Symposium on Principles of Distributed Computing, PODC 2018, Egham,
United Kingdom, July 23-27, 2018</em>, pages 475–477. ACM, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1802.06872" title="">https://arxiv.org/abs/1802.06872</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krauthgamer and Sapir (2023)</span>
<span class="ltx_bibblock">
R. Krauthgamer and S. Sapir.

</span>
<span class="ltx_bibblock">Comparison of matrix norm sparsification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Algorithmica</em>, 85(12):3957–3972, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00453-023-01172-6" title="">https://doi.org/10.1007/s00453-023-01172-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2025)</span>
<span class="ltx_bibblock">
A. Kumar, L. Owen, N. R. Chowdhury, and F. Güra.

</span>
<span class="ltx_bibblock">ZClip: Adaptive spike mitigation for LLM pre-training, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2504.02507" title="">https://arxiv.org/abs/2504.02507</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun (2022)</span>
<span class="ltx_bibblock">
Y. LeCun.

</span>
<span class="ltx_bibblock">A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Open Review</em>, 62(1):1–62, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al. (2015)</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 521(7553):436–444, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2017)</span>
<span class="ltx_bibblock">
J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and
J. Sohl-Dickstein.

</span>
<span class="ltx_bibblock">Deep neural networks as gaussian processes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.00165</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
A. C. Lin, A. M. Bygrave, A. De Calignon, T. Lee, and G. Miesenböck.

</span>
<span class="ltx_bibblock">Sparse, decorrelated odor coding in the mushroom body enhances
learned odor discrimination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature neuroscience</em>, 17(4):559–568, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Jegelka (2018)</span>
<span class="ltx_bibblock">
H. Lin and S. Jegelka.

</span>
<span class="ltx_bibblock">Resnet with one-neuron hidden layers is a universal approximator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025)</span>
<span class="ltx_bibblock">
K. Liu, J. Gao, and K. Chen.

</span>
<span class="ltx_bibblock">Scaling up the state size of RNN LLMs for long-context scenarios.

</span>
<span class="ltx_bibblock">In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors,
<em class="ltx_emph ltx_font_italic">Proceedings of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 11516–11529,
Vienna, Austria, July 2025. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">ISBN 979-8-89176-251-0.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.acl-long.564/" title="">https://aclanthology.org/2025.acl-long.564/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
I. Loshchilov and F. Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1711.05101" title="">https://arxiv.org/abs/1711.05101</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mante et al. (2013)</span>
<span class="ltx_bibblock">
V. Mante, D. Sussillo, K. V. Shenoy, and W. T. Newsome.

</span>
<span class="ltx_bibblock">Context-dependent computation by recurrent dynamics in prefrontal
cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">nature</em>, 503(7474):78–84, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Massoulié (2014)</span>
<span class="ltx_bibblock">
L. Massoulié.

</span>
<span class="ltx_bibblock">Community detection thresholds and the weak ramanujan property.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Forty-Sixth Annual ACM Symposium on
Theory of Computing</em>, STOC ’14, page 694–703, New York, NY, USA, 2014.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450327107.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2591796.2591857" title="">https://doi.org/10.1145/2591796.2591857</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCulloch and Pitts (1943)</span>
<span class="ltx_bibblock">
W. S. McCulloch and W. Pitts.

</span>
<span class="ltx_bibblock">A logical calculus of the ideas immanent in nervous activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Bulletin of Mathematical Biophysics</em>, 5(4):115–133, 1943.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McSherry et al. (2013)</span>
<span class="ltx_bibblock">
F. McSherry, D. G. Murray, R. Isaacs, and M. Isard.

</span>
<span class="ltx_bibblock">Differential dataflow.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Sixth Biennial Conference on Innovative Data Systems
Research, CIDR 2013, Asilomar, CA, USA, January 6-9, 2013, Online
Proceedings</em>. www.cidrdb.org, 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf" title="">https://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrill and
Sabharwal (2024)</span>
<span class="ltx_bibblock">
W. Merrill and A. Sabharwal.

</span>
<span class="ltx_bibblock">The expressive power of transformers with chain of thought.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=NjNGlPh8Wh" title="">https://openreview.net/forum?id=NjNGlPh8Wh</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 26, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitropolsky and Papadimitriou (2025)</span>
<span class="ltx_bibblock">
D. Mitropolsky and C. H. Papadimitriou.

</span>
<span class="ltx_bibblock">Simulated language acquisition in a biologically realistic model of
the brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">bioRxiv</em>, 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1101/2025.07.15.664996</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.biorxiv.org/content/early/2025/07/19/2025.07.15.664996" title="">https://www.biorxiv.org/content/early/2025/07/19/2025.07.15.664996</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohsenzadeh et al. (2020)</span>
<span class="ltx_bibblock">
Y. Mohsenzadeh, C. Mullin, B. Lahner, and A. Oliva.

</span>
<span class="ltx_bibblock">Emergence of visual center-periphery spatial organization in deep
convolutional neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Scientific Reports</em>, 10(1):4638, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41598-020-61409-0" title="">https://doi.org/10.1038/s41598-020-61409-0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nair and Hinton (2010)</span>
<span class="ltx_bibblock">
V. Nair and G. E. Hinton.

</span>
<span class="ltx_bibblock">Rectified linear units improve restricted boltzmann machines.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 27th international conference on machine
learning (ICML-10)</em>, pages 807–814, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neal (2012)</span>
<span class="ltx_bibblock">
R. M. Neal.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Bayesian learning for neural networks</em>, volume 118.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neumann (1958)</span>
<span class="ltx_bibblock">
J. v. Neumann.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The computer and the brain</em>.

</span>
<span class="ltx_bibblock">Yale University Press, USA, 1958.

</span>
<span class="ltx_bibblock">ISBN 0300007930.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newman (2006)</span>
<span class="ltx_bibblock">
M. E. J. Newman.

</span>
<span class="ltx_bibblock">Modularity and community structure in networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, 103(23):8577–8582, 2006.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1073/pnas.0601602103</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.0601602103" title="">https://www.pnas.org/doi/abs/10.1073/pnas.0601602103</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olshausen (2018)</span>
<span class="ltx_bibblock">
B. A. Olshausen.

</span>
<span class="ltx_bibblock">(ed.), The Brain and Computation — Simons Institute Program,
Berkeley, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://simons.berkeley.edu/programs/brain-computation" title="">https://simons.berkeley.edu/programs/brain-computation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olshausen and Field (1997)</span>
<span class="ltx_bibblock">
B. A. Olshausen and D. J. Field.

</span>
<span class="ltx_bibblock">Sparse coding with an overcomplete basis set: A strategy employed by
V1?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Vision Research</em>, 37(23):3311–3325, 1997.

</span>
<span class="ltx_bibblock">ISSN 0042-6989.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/S0042-6989(97)00169-7" title="">https://doi.org/10.1016/S0042-6989(97)00169-7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olsson et al. (2022)</span>
<span class="ltx_bibblock">
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann,
A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli,
Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion,
L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan,
S. McCandlish, and C. Olah.

</span>
<span class="ltx_bibblock">In-context learning and induction heads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transformer Circuits Thread</em>, 2022.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" title="">https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostmeier et al. (2025)</span>
<span class="ltx_bibblock">
S. Ostmeier, B. Axelrod, M. Varma, M. E. Moseley, A. Chaudhari, and
C. Langlotz.

</span>
<span class="ltx_bibblock">LieRE: Lie rotational positional encodings, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.10322" title="">https://arxiv.org/abs/2406.10322</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadimitriou et al. (2020)</span>
<span class="ltx_bibblock">
C. H. Papadimitriou, S. S. Vempala, D. Mitropolsky, M. Collins, and W. Maass.

</span>
<span class="ltx_bibblock">Brain computation by assemblies of neurons.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>, 117(25):14464–14472, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.2001893117" title="">https://www.pnas.org/doi/abs/10.1073/pnas.2001893117</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, et al.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peleg (2000)</span>
<span class="ltx_bibblock">
D. Peleg.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Distributed Computing: A Locality-Sensitive Approach</em>.

</span>
<span class="ltx_bibblock">Society for Industrial and Applied Mathematics, 2000.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://epubs.siam.org/doi/abs/10.1137/1.9780898719772" title="">https://epubs.siam.org/doi/abs/10.1137/1.9780898719772</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez et al. (2021)</span>
<span class="ltx_bibblock">
J. Pérez, P. Barceló, and J. Marinkovic.

</span>
<span class="ltx_bibblock">Attention is turing complete.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 22(1), Jan. 2021.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
O. Press, N. A. Smith, and M. Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input
length extrapolation, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2108.12409" title="">https://arxiv.org/abs/2108.12409</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rolla (2020)</span>
<span class="ltx_bibblock">
L. T. Rolla.

</span>
<span class="ltx_bibblock">Activated random walks on <math alttext="\mathbb{Z}^{d}" class="ltx_Math" display="inline" id="bib.bib90.m1" intent=":literal"><semantics><msup><mi>ℤ</mi><mi>d</mi></msup><annotation encoding="application/x-tex">\mathbb{Z}^{d}</annotation></semantics></math>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Probability Surveys</em>, 17, Jan. 2020.

</span>
<span class="ltx_bibblock">ISSN 1549-5787.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dx.doi.org/10.1214/19-PS339" title="">https://dx.doi.org/10.1214/19-PS339</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romanid (2025)</span>
<span class="ltx_bibblock">
Romanid.

</span>
<span class="ltx_bibblock">Wikipedia, the free encyclopedia, 2025.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/w/index.php?title=Romanid&amp;oldid=1275565870" title="">https://en.wikipedia.org/w/index.php?title=Romanid&amp;oldid=1275565870</a>.

</span>
<span class="ltx_bibblock">[Online; accessed 24-July-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rumelhart et al. (1986)</span>
<span class="ltx_bibblock">
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.

</span>
<span class="ltx_bibblock">Learning representations by back-propagating errors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 323(6088):533–536, 1986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusch and Rus (2025)</span>
<span class="ltx_bibblock">
T. K. Rusch and D. Rus.

</span>
<span class="ltx_bibblock">Oscillatory state-space models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning
Representations</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=GRMfXcAAFh" title="">https://openreview.net/forum?id=GRMfXcAAFh</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabour et al. (2017)</span>
<span class="ltx_bibblock">
S. Sabour, N. Frosst, and G. E. Hinton.

</span>
<span class="ltx_bibblock">Dynamic routing between capsules.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidhuber (1993)</span>
<span class="ltx_bibblock">
J. Schmidhuber.

</span>
<span class="ltx_bibblock">Reducing the ratio between learning complexity and number of time
varying variables in fully recurrent nets.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Artificial Neural Networks</em>,
pages 460–463. Springer, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. (2017)</span>
<span class="ltx_bibblock">
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.

</span>
<span class="ltx_bibblock">Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1701.06538" title="">https://arxiv.org/abs/1701.06538</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2022)</span>
<span class="ltx_bibblock">
Z. Shen, H. Yang, and S. Zhang.

</span>
<span class="ltx_bibblock">Optimal approximation rate of ReLU networks in terms of width and
depth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal de Mathématiques Pures et Appliquées</em>, 157:101–135, 2022.

</span>
<span class="ltx_bibblock">ISSN 0021-7824.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.matpur.2021.07.009</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0021782421001124" title="">https://www.sciencedirect.com/science/article/pii/S0021782421001124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shojaee et al. (2025)</span>
<span class="ltx_bibblock">
P. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar.

</span>
<span class="ltx_bibblock">The illusion of thinking: Understanding the strengths and limitations
of reasoning models via the lens of problem complexity, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2506.06941" title="">https://arxiv.org/abs/2506.06941</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2014)</span>
<span class="ltx_bibblock">
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The journal of machine learning research</em>, 15(1):1929–1958, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
J. Su, M. H. M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neurocomputing</em>, 568:127063, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/J.NEUCOM.2023.127063</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2023.127063" title="">https://doi.org/10.1016/j.neucom.2023.127063</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei.

</span>
<span class="ltx_bibblock">Retentive Network: A successor to Transformer for large language
models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.08621" title="">https://arxiv.org/abs/2307.08621</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turing (1950)</span>
<span class="ltx_bibblock">
A. M. Turing.

</span>
<span class="ltx_bibblock">Computing machinery and intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Mind</em>, 59(236):433–460, 1950.

</span>
<span class="ltx_bibblock">ISSN 00264423.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.jstor.org/stable/2251299" title="">https://www.jstor.org/stable/2251299</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Udell and Townsend (2019)</span>
<span class="ltx_bibblock">
M. Udell and A. Townsend.

</span>
<span class="ltx_bibblock">Why are big data matrices approximately low rank?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">SIAM J. Math. Data Sci.</em>, 1(1):144–160,
2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1137/18M1183480</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1137/18M1183480" title="">https://doi.org/10.1137/18M1183480</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V.
Le, and D. Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Neural
Information Processing Systems</em>, NIPS ’22, Red Hook, NY, USA, 2022. Curran
Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781713871088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss et al. (2021)</span>
<span class="ltx_bibblock">
G. Weiss, Y. Goldberg, and E. Yahav.

</span>
<span class="ltx_bibblock">Thinking like transformers.

</span>
<span class="ltx_bibblock">In M. Meila and T. Zhang, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 38th
International Conference on Machine Learning</em>, volume 139 of
<em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 11080–11090. PMLR,
18–24 Jul 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v139/weiss21a.html" title="">https://proceedings.mlr.press/v139/weiss21a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittington et al. (2020)</span>
<span class="ltx_bibblock">
J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and
T. E. Behrens.

</span>
<span class="ltx_bibblock">The Tolman-Eichenbaum Machine: Unifying space and relational memory
through generalization in the hippocampal formation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Cell</em>, 183(5):1249–1263.e23, 2020.

</span>
<span class="ltx_bibblock">ISSN 0092-8674.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.cell.2020.10.024" title="">https://doi.org/10.1016/j.cell.2020.10.024</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittington et al. (2022)</span>
<span class="ltx_bibblock">
J. C. R. Whittington, J. Warren, and T. E. Behrens.

</span>
<span class="ltx_bibblock">Relating transformers to models and neural representations of the
hippocampal formation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=B8DVo9B1YE0" title="">https://openreview.net/forum?id=B8DVo9B1YE0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams and Peng (1990)</span>
<span class="ltx_bibblock">
R. J. Williams and J. Peng.

</span>
<span class="ltx_bibblock">An efficient gradient-based algorithm for on-line training of
recurrent network trajectories.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neural computation</em>, 2(4):490–501, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Chiang (2024)</span>
<span class="ltx_bibblock">
A. Yang and D. Chiang.

</span>
<span class="ltx_bibblock">Counting like transformers: Compiling temporal counting logic into
softmax transformers, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.04393" title="">https://arxiv.org/abs/2404.04393</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang (2019)</span>
<span class="ltx_bibblock">
G. Yang.

</span>
<span class="ltx_bibblock">Wide feedforward or recurrent neural networks of any architecture are
gaussian processes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy.

</span>
<span class="ltx_bibblock">Hierarchical attention networks for document classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1480–1489, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al. (2025)</span>
<span class="ltx_bibblock">
C. You, K. Wu, Z. Jia, L. Chen, S. Bhojanapalli, J. Guo, U. Evci,
J. Wassenberg, P. Netrapalli, J. J. Willcock, S. Subramanian, F. Chern,
A. Andreev, S. Pathak, F. Yu, P. Jain, D. E. Culler, H. M. Levy, and
S. Kumar.

</span>
<span class="ltx_bibblock">Spark transformer: Reactivating sparsity in FFN and attention,
2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2506.06644" title="">https://arxiv.org/abs/2506.06644</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. M. Susskind,
S. Bengio, and P. Nakkiran.

</span>
<span class="ltx_bibblock">What algorithms can transformers learn? A study in length
generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning
Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>.
OpenReview.net, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=AssIuHnmHX" title="">https://openreview.net/forum?id=AssIuHnmHX</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou (2019)</span>
<span class="ltx_bibblock">
M. Zou.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Aspects of Efficiency in Selected Problems of Computation on
Large Graphs</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Paris Diderot University, France, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://tel.archives-ouvertes.fr/tel-02436610" title="">https://tel.archives-ouvertes.fr/tel-02436610</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Connection between generalization of reasoning and computational expressiveness</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p">State-of-the-art reasoning models have the interpretation of (Turing-complete) programs, executed over a certain period of time. This shifts the emphasis of generalization, from discovering the structure of mathematical functions which maps inputs to outputs, to discovering a class of runnable programs, which take as input a given class of input prompts, and process these prompts “in the right direction”.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p">Consider a given reasoning task, whose scope is defined as a set <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p2.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> of valid input prompts, given as bounded-length token sequences over some alphabet <math alttext="\Omega" class="ltx_Math" display="inline" id="A1.p2.m2" intent=":literal"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>. Given a prompt from <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p2.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, a model solving the considered task is eventually (i.e, after some number of steps of reasoning) expected to generate an output, in the form of a bounded-length token sequence over the same alphabet <math alttext="\Omega" class="ltx_Math" display="inline" id="A1.p2.m4" intent=":literal"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>, which is subjected to evaluation. Consider language models sampled from some probability distribution <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A1.p2.m5" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math> over parameter sets in some architecture <math alttext="\mathcal{A}_{1}" class="ltx_Math" display="inline" id="A1.p2.m6" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{1}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p">Now, suppose that for some other model architecture <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A1.p3.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> there exists a distribution <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A1.p3.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{2}</annotation></semantics></math> over language models in <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A1.p3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> such that, for a valid input prompt chosen uniformly at random from <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p3.m4" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, the outputs sampled from a model <math alttext="M_{1}\sim\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A1.p3.m5" intent=":literal"><semantics><mrow><msub><mi>M</mi><mn>1</mn></msub><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">M_{1}\sim\mathcal{M}_{1}</annotation></semantics></math> and the outputs sampled from a model <math alttext="M_{2}\sim\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A1.p3.m6" intent=":literal"><semantics><mrow><msub><mi>M</mi><mn>2</mn></msub><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">M_{2}\sim\mathcal{M}_{2}</annotation></semantics></math>, have (almost) the same distribution in the space of bounded-length sequences over <math alttext="\Omega" class="ltx_Math" display="inline" id="A1.p3.m7" intent=":literal"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>, and are both obtained within some asymptotic bound on the number of steps of reasoning, in expectation. The described setting is equivalent to saying that <em class="ltx_emph ltx_font_italic">models <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A1.p3.m8" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{2}</annotation></semantics></math> have generalized the considered task <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p3.m9" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> in (almost) the same way as models <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A1.p3.m10" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math></em>. Indeed, conversely, if the described condition did not hold, we could, in a finite number of trials, distinguish solutions to problem <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p3.m11" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> obtained by model families <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A1.p3.m12" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math> and <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A1.p3.m13" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{2}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p">Now, consider model architectures <math alttext="\mathcal{A}_{1},\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A1.p4.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>1</mn></msub><mo>,</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathcal{A}_{1},\mathcal{A}_{2}</annotation></semantics></math> which apply Chain-of-Thought reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib105" title="">2022</a>)</cite>. A model in such an architecture has the interpretation of a trainable probabilistic program, taking inputs from <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p4.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, and the architectures themselves represent computational machine architectures. Moving to a discussion of computational expressiveness, we obtain the following statement.</p>
</div>
<div class="ltx_theorem ltx_theorem_observation" id="Thmobservation9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Observation 9</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmobservation9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Given a probability distribution of models <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math> in architecture <math alttext="\mathcal{A}_{1}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{1}</annotation></semantics></math>, suppose there exists a distribution over models in architecture <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> which generalizes on task <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m4" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> in the same way as models from <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m5" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math>. Then, the machine architecture <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m6" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> has sufficient computational expressiveness to simulate programs from <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m7" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math> efficiently on the set of inputs <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m8" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math>, i.e., <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="Thmobservation9.p1.m9" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> contains programs which obtain an (almost) identical distribution of outputs within the given bounds on running time.
∎</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p">In particular, we note that if we were to consider the special case of <math alttext="\mathcal{A}_{1}" class="ltx_Math" display="inline" id="A1.p5.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{1}</annotation></semantics></math> being reasonable human agents, we could say that architecture <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A1.p5.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> generalizes reasoning, in the same way as humans, if we can train models <math alttext="\mathcal{M}_{2}" class="ltx_Math" display="inline" id="A1.p5.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{2}</annotation></semantics></math> in <math alttext="\mathcal{A}_{2}" class="ltx_Math" display="inline" id="A1.p5.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{2}</annotation></semantics></math> which accurately reproduce the outcomes of reasoning for some sample <math alttext="\mathcal{M}_{1}" class="ltx_Math" display="inline" id="A1.p5.m5" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{M}_{1}</annotation></semantics></math> of humans in <math alttext="\mathcal{A}_{1}" class="ltx_Math" display="inline" id="A1.p5.m6" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathcal{A}_{1}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p">This leads us naturally to describe Language Model generalization through a universal reference to the principles of operation of the human brain, treated as a distributed computing architecture, and not through a characterization of language and reasoning prompts <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A1.p6.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> that the model should be able to deal with in some specific way.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Further description of experiments</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Language translation task</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p">We have evaluated our models on a mixed language modeling and translation task derived from the Europarl corpus <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib57" title="">2005</a>)</cite>. The corpus consists of sentence-level aligned translations of transcripts of European Parliament proceedings. For each language pair, we treat the data as a long stream of interleaved source and target sentences (sampling for each sentence which language is the source, and which is the target) on which we train decoder only models. Thus, models are jointly trained as language models and translators. We train all models using Truncated Backpropagation Through Time <cite class="ltx_cite ltx_citemacro_citep">(Williams and Peng, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib109" title="">1990</a>)</cite>. Subsequent minibatches served by the data loader are related: each is a continuation of the previous. Each model maintains a recurrent state, carried across minibatches: <math alttext="{\boldsymbol{\rho}}" class="ltx_Math" display="inline" id="A2.SS1.p1.m1" intent=":literal"><semantics><mi>𝝆</mi><annotation encoding="application/x-tex">{\boldsymbol{\rho}}</annotation></semantics></math> matrix for BDH-GPU and a FIFO buffer of recent KV-cache entries for the TransformerXL <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib26" title="">2019</a>)</cite> baseline. We train all models on raw UTF8 data. We are mainly interested in model comparison and prefer to keep the experimental setup as simple as possible. A few minibatches are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.F16" title="Figure 16 ‣ B.1 Language translation task ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p">The joint language modeling and translation formulation has several benefits:</p>
<ol class="ltx_enumerate" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p">Next token prediction is representative for LLM training. Simple architectures, such as decoder-only models are sufficient.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p">The task promotes models with long context capabilities — subsequent sentences are related and the model can meaningfully
utilize long context to model the source language sentences.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p">The task promotes models which carry state across minibatches, as training data is temporally coherent and the final model state at the end of one minibatch is a natural initialization of hidden state on the next minibatch.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A2.I1.i4.p1">
<p class="ltx_p">Translation can be seen as language modeling coupled with fuzzy copying. Successful models will need to develop in-context learning capabilities such as inductive heads <cite class="ltx_cite ltx_citemacro_citep">(Olsson et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib82" title="">2022</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="A2.F16"><pre class="ltx_verbatim ltx_centering ltx_font_typewriter">
        0. |&lt;F:en&gt;For countries such as Sweden and Finland, another system o|
        1. |f allocation would be extremely significant.&lt;T:es&gt;Por ejemplo, p|
        2. |ara pases como Suecia y Finlandia tendra un gran significado|
        3. | que se hiciese otra forma de distribucin.&lt;F:es&gt;El diputado Fe|
        4. |rber ha presentado una propuesta que implica una distribucin m|
        5. |s flexible, y yo respaldo esta enmienda.&lt;T:en&gt;Mr Ferber has ta|
        6. |bled an amendment which involves our looking in a considerably m|
        7. |ore flexible way at the present allocation, and I support this a|
        8. |mendment.&lt;F:en&gt;.&lt;T:es&gt;.&lt;F:en&gt;(NL) Mr President, I would like to |
        9. |start by thanking both parliamentary committees and not least bo|
            
</pre>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Exemplary sequence of 10 successive minibatches from the translation task. The model is trained on raw UTF8 bytes (for visualization we pad multi-byte UTF8 characters with “•” symbol). Special token strings <span class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;F:lang_code&gt;</span> and <span class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;T:lang_code&gt;</span> delimit source and target sentences. Minibatches are temporally coherent: source sentences are followed by their translations, and subsequent source sentences are part of the same larger document.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>BDH Scaling Experimental Details</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p">We provide details on models used in scaling experiments described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S4.SS2" title="4.2 Comparison of BDH-GPU to GPT2-like Transformers ‣ 4 Implementation and scaling laws ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4.2</span></a>. All models were implemented in PyTorch <cite class="ltx_cite ltx_citemacro_citep">(Paszke et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib85" title="">2019</a>)</cite> and trained on the Europarl <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib57" title="">2005</a>)</cite> task described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS1" title="B.1 Language translation task ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.1</span></a>. We have kept the same training regime for all models at all sizes: En-PL and En-Cs language pairs (380MB total). All models trained on raw UTF8 bytes seeing a total of 1.2B tokens (about 3 epochs). All minibatches were 2048 tokens long, but we have varied the number of examples in the minibatch (varying number of tokens in each minibatch) to accommodate different memory requirements of different models. We have used multi-GPU training using the Distributed Data Parallel approach using AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib67" title="">2019</a>)</cite> with learning rate <math alttext="10^{-3}" class="ltx_Math" display="inline" id="A2.SS2.p1.m1" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math>, and 1000 warm-up step followed by linear learning rate decay over the course of training to <math alttext="10^{-4}" class="ltx_Math" display="inline" id="A2.SS2.p1.m2" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><annotation encoding="application/x-tex">10^{-4}</annotation></semantics></math>, adaptive gradient clipping <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib60" title="">2025</a>)</cite>, and weight decay <math alttext="0.1" class="ltx_Math" display="inline" id="A2.SS2.p1.m3" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math>. Models were trained to operate on a context longer than minibatch length using Truncated Backpropagation Through time <cite class="ltx_cite ltx_citemacro_citep">(Williams and Peng, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib109" title="">1990</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p">The Baseline model, dubbed GPTXL, was a GPT2-like transformer <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib89" title="">2019</a>)</cite> based off the NanoGPT <cite class="ltx_cite ltx_citemacro_citep">(Karpathy, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib52" title="">2024</a>)</cite> implementation with KV-cache carried across minibatches as in TransformerXL <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib26" title="">2019</a>)</cite>. We have used ALiBi positional biases <cite class="ltx_cite ltx_citemacro_citet">Press et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib88" title="">2022</a>)</cite>. We list its hyperparameters for various model sizes in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.T4" title="Table 4 ‣ B.2 BDH Scaling Experimental Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>. Optimal Dropout was selected using a small sweep at each model size.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">embd</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">MLP</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">dropout</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Carried KV-cache</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">size</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">layer</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">dim</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">head</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">dim</th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">size</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">25M</td>
<td class="ltx_td ltx_align_left ltx_border_t">9</td>
<td class="ltx_td ltx_align_left ltx_border_t">480</td>
<td class="ltx_td ltx_align_left ltx_border_t">5</td>
<td class="ltx_td ltx_align_left ltx_border_t">1920</td>
<td class="ltx_td ltx_align_left ltx_border_t">0.01</td>
<td class="ltx_td ltx_align_left ltx_border_t">4096</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">50M</td>
<td class="ltx_td ltx_align_left">12</td>
<td class="ltx_td ltx_align_left">576</td>
<td class="ltx_td ltx_align_left">6</td>
<td class="ltx_td ltx_align_left">2304</td>
<td class="ltx_td ltx_align_left">0.02</td>
<td class="ltx_td ltx_align_left">4096</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">100M</td>
<td class="ltx_td ltx_align_left">15</td>
<td class="ltx_td ltx_align_left">768</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">3072</td>
<td class="ltx_td ltx_align_left">0.02</td>
<td class="ltx_td ltx_align_left">4096</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">200M</td>
<td class="ltx_td ltx_align_left">18</td>
<td class="ltx_td ltx_align_left">960</td>
<td class="ltx_td ltx_align_left">10</td>
<td class="ltx_td ltx_align_left">3840</td>
<td class="ltx_td ltx_align_left">0.002</td>
<td class="ltx_td ltx_align_left">4096</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">400M</td>
<td class="ltx_td ltx_align_left">25</td>
<td class="ltx_td ltx_align_left">1152</td>
<td class="ltx_td ltx_align_left">12</td>
<td class="ltx_td ltx_align_left">4608</td>
<td class="ltx_td ltx_align_left">0.005</td>
<td class="ltx_td ltx_align_left">4096</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">800M</td>
<td class="ltx_td ltx_align_left ltx_border_bb">28</td>
<td class="ltx_td ltx_align_left ltx_border_bb">1536</td>
<td class="ltx_td ltx_align_left ltx_border_bb">16</td>
<td class="ltx_td ltx_align_left ltx_border_bb">6144</td>
<td class="ltx_td ltx_align_left ltx_border_bb">0.15</td>
<td class="ltx_td ltx_align_left ltx_border_bb">4096</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Hyperparameters for GPTXL baselines in scaling experiments. The model architecture follows GPT2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib89" title="">2019</a>)</cite>, with a FIFO buffer of past KV-cache entries <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib26" title="">2019</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.SS2.p3">
<p class="ltx_p">BDH-GPU directly uses model code provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A5" title="Appendix E BDH-GPU PyTorch code listing ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">E</span></a>. BDH-GPU’ adds xLSTM-like gating mechanism <cite class="ltx_cite ltx_citemacro_citep">(Beck et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib10" title="">2024</a>)</cite>, and merges next token predictions from all layers. Both BDH-GPU and BDH-GPU’ use same architectural hyperparameters, gathered in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.T5" title="Table 5 ‣ B.2 BDH Scaling Experimental Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="A2.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><math alttext="d" class="ltx_Math" display="inline" id="A2.T5.m1" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><math alttext="n" class="ltx_Math" display="inline" id="A2.T5.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">dropout</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">size</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">layer</th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">head</th>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">25M</td>
<td class="ltx_td ltx_align_left ltx_border_t">8</td>
<td class="ltx_td ltx_align_left ltx_border_t">256</td>
<td class="ltx_td ltx_align_left ltx_border_t">32768</td>
<td class="ltx_td ltx_align_left ltx_border_t">4</td>
<td class="ltx_td ltx_align_left ltx_border_t">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">50M</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">65536</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">100M</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">131072</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">200M</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">262144</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">400M</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">524288</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">800M</td>
<td class="ltx_td ltx_align_left ltx_border_bb">8</td>
<td class="ltx_td ltx_align_left ltx_border_bb">256</td>
<td class="ltx_td ltx_align_left ltx_border_bb">1048576</td>
<td class="ltx_td ltx_align_left ltx_border_bb">4</td>
<td class="ltx_td ltx_align_left ltx_border_bb">0.1</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Hyperparameters for BDH-GPU models in scaling experiments.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>BDH Monosemantic Synapse Experiment Details</h3>
<div class="ltx_para ltx_noindent" id="A2.SS3.p1">
<p class="ltx_p">We provide details for models used in exploration of monosemantic synapses in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.SS2" title="6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6.2</span></a>. The model was trained on Europarl <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib57" title="">2005</a>)</cite> described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS1" title="B.1 Language translation task ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.1</span></a>. It had <math alttext="d=256,n=49152" class="ltx_Math" display="inline" id="A2.SS3.p1.m1" intent=":literal"><semantics><mrow><mrow><mi>d</mi><mo>=</mo><mn>256</mn></mrow><mo>,</mo><mrow><mi>n</mi><mo>=</mo><mn>49152</mn></mrow></mrow><annotation encoding="application/x-tex">d=256,n=49152</annotation></semantics></math>, <math alttext="4" class="ltx_Math" display="inline" id="A2.SS3.p1.m2" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> attention heads, and <math alttext="8" class="ltx_Math" display="inline" id="A2.SS3.p1.m3" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> layers. The model was trained on about one epoch of En-Es, En-Pt, and En-Fr data (total 1.9B tokens) in a Distributed Data Parallel setup using AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib67" title="">2019</a>)</cite> with learning rate <math alttext="10^{-3}" class="ltx_Math" display="inline" id="A2.SS3.p1.m4" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math>, 1000 warm-up step followed by linear learning rate decay over the course of training to <math alttext="10^{-4}" class="ltx_Math" display="inline" id="A2.SS3.p1.m5" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><annotation encoding="application/x-tex">10^{-4}</annotation></semantics></math>, adaptive gradient clipping <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib60" title="">2025</a>)</cite>, and weight decay <math alttext="0.1" class="ltx_Math" display="inline" id="A2.SS3.p1.m6" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math>. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length <math alttext="2048" class="ltx_Math" display="inline" id="A2.SS3.p1.m7" intent=":literal"><semantics><mn>2048</mn><annotation encoding="application/x-tex">2048</annotation></semantics></math> characters at a time. We have used minimal Dropout <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib99" title="">2014</a>)</cite> of <math alttext="0.01" class="ltx_Math" display="inline" id="A2.SS3.p1.m8" intent=":literal"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>BDH Merging Experiment Details</h3>
<div class="ltx_para ltx_noindent" id="A2.SS4.p1">
<p class="ltx_p">We provide details for models described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S7.SS1" title="7.1 Model merging: concatenating two models ‣ 7 Playing with the Hatchling ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7.1</span></a>
All models were trained on Europarl <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib57" title="">2005</a>)</cite> described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS1" title="B.1 Language translation task ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.1</span></a>. We provide model architecture hyperparametrs in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.T6" title="Table 6 ‣ B.4 BDH Merging Experiment Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>. Models were trained on about two passes over the training set in a Distributed Data Parallel setup using AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib67" title="">2019</a>)</cite> with learning rate <math alttext="10^{-3}" class="ltx_Math" display="inline" id="A2.SS4.p1.m1" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><annotation encoding="application/x-tex">10^{-3}</annotation></semantics></math>, 1000 warmup step followed by linear learning rate decay over the course of training to <math alttext="10^{-4}" class="ltx_Math" display="inline" id="A2.SS4.p1.m2" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><annotation encoding="application/x-tex">10^{-4}</annotation></semantics></math>, adaptive gradient clipping <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib60" title="">2025</a>)</cite>, and weight decay <math alttext="0.1" class="ltx_Math" display="inline" id="A2.SS4.p1.m3" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math>. We have used Truncated Backpropagation Through time, carrying over the recurrent state of attention and training on sequences of length <math alttext="2048" class="ltx_Math" display="inline" id="A2.SS4.p1.m4" intent=":literal"><semantics><mn>2048</mn><annotation encoding="application/x-tex">2048</annotation></semantics></math> characters at a time. We have used minimal Dropout <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib99" title="">2014</a>)</cite> of <math alttext="0.01" class="ltx_Math" display="inline" id="A2.SS4.p1.m5" intent=":literal"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A2.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Init.</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Data size</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">n</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">d</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num.</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">num.</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">param.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">from</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">(bytes)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">tokens</th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">heads</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">layers</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column">count</th>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">BaseEnEs</td>
<td class="ltx_td ltx_align_left ltx_border_t">—</td>
<td class="ltx_td ltx_align_left ltx_border_t">En-Es</td>
<td class="ltx_td ltx_align_left ltx_border_t">612M</td>
<td class="ltx_td ltx_align_left ltx_border_t">1.2B</td>
<td class="ltx_td ltx_align_left ltx_border_t">24576</td>
<td class="ltx_td ltx_align_left ltx_border_t">256</td>
<td class="ltx_td ltx_align_left ltx_border_t">4</td>
<td class="ltx_td ltx_align_left ltx_border_t">8</td>
<td class="ltx_td ltx_align_left ltx_border_t">19M</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">TunedEnFr</td>
<td class="ltx_td ltx_align_left">BaseEnEs</td>
<td class="ltx_td ltx_align_left">En-Fr</td>
<td class="ltx_td ltx_align_left">640M</td>
<td class="ltx_td ltx_align_left">1.2B</td>
<td class="ltx_td ltx_align_left">24576</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">19M</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">TunedEnPt</td>
<td class="ltx_td ltx_align_left">BaseEnEs</td>
<td class="ltx_td ltx_align_left">En-Pt</td>
<td class="ltx_td ltx_align_left">616M</td>
<td class="ltx_td ltx_align_left">1.2B</td>
<td class="ltx_td ltx_align_left">24576</td>
<td class="ltx_td ltx_align_left">256</td>
<td class="ltx_td ltx_align_left">4</td>
<td class="ltx_td ltx_align_left">8</td>
<td class="ltx_td ltx_align_left">19M</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">MergedEnEsFrPt</td>
<td class="ltx_td ltx_align_left ltx_border_bb">TunedEnFr+TunedEnPt</td>
<td class="ltx_td ltx_align_left ltx_border_bb">—</td>
<td class="ltx_td ltx_align_left ltx_border_bb">—</td>
<td class="ltx_td ltx_align_left ltx_border_bb">—</td>
<td class="ltx_td ltx_align_left ltx_border_bb">49152</td>
<td class="ltx_td ltx_align_left ltx_border_bb">256</td>
<td class="ltx_td ltx_align_left ltx_border_bb">4</td>
<td class="ltx_td ltx_align_left ltx_border_bb">8</td>
<td class="ltx_td ltx_align_left ltx_border_bb">38M</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Architecture and training details for model merging experiments.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Omitted formal claims and proofs</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Proof of Observation <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmobservation1" title="Observation 1. ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>
</h3>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p">The equivalence is straightforward to verify, rewriting the linear-algebraic multiplication expressions of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E6" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">6</span></a>) in Einstein summation notation and comparing respective index pairs. At any time, during the execution of rules for layer <math alttext="l" class="ltx_Math" display="inline" id="A3.SS1.p1.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>, variables <math alttext="X(i)" class="ltx_Math" display="inline" id="A3.SS1.p1.m2" intent=":literal"><semantics><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(i)</annotation></semantics></math>, <math alttext="Y(i)" class="ltx_Math" display="inline" id="A3.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(i)</annotation></semantics></math> and <math alttext="\sigma_{l}(i,j)" class="ltx_Math" display="inline" id="A3.SS1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>σ</mi><mi>l</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma_{l}(i,j)</annotation></semantics></math> in the protocol description, for <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="A3.SS1.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math> correspond to the <math alttext="i" class="ltx_Math" display="inline" id="A3.SS1.p1.m6" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th coordinate of vectors <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m7" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math> (based on <math alttext="x_{t,l-1}" class="ltx_Math" display="inline" id="A3.SS1.p1.m8" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><annotation encoding="application/x-tex">x_{t,l-1}</annotation></semantics></math> from the previous round), <math alttext="y_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m9" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l}</annotation></semantics></math> (based on <math alttext="y_{t,l-1}" class="ltx_Math" display="inline" id="A3.SS1.p1.m10" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><annotation encoding="application/x-tex">y_{t,l-1}</annotation></semantics></math> from the previous round), and matrix entry <math alttext="{\boldsymbol{\sigma}}_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m11" intent=":literal"><semantics><msub><mi>𝝈</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t,l}</annotation></semantics></math> (based on <math alttext="{\boldsymbol{\sigma}}_{t-1,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m12" intent=":literal"><semantics><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t-1,l}</annotation></semantics></math> from the previous token). The auxiliary variable <math alttext="A(i)" class="ltx_Math" display="inline" id="A3.SS1.p1.m13" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(i)</annotation></semantics></math> corresponds to a similar auxiliary vector <math alttext="a_{t,l}:={\boldsymbol{\sigma}}_{t-1,l}x_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m14" intent=":literal"><semantics><mrow><msub><mi>a</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>:=</mo><mrow><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">a_{t,l}:={\boldsymbol{\sigma}}_{t-1,l}x_{t,l}</annotation></semantics></math> in an intermediate step of computation of <math alttext="y_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m15" intent=":literal"><semantics><msub><mi>y</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">y_{t,l}</annotation></semantics></math> from <math alttext="x_{t,l}" class="ltx_Math" display="inline" id="A3.SS1.p1.m16" intent=":literal"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">x_{t,l}</annotation></semantics></math>. The parameter <math alttext="u(i,j)\in R^{+}" class="ltx_Math" display="inline" id="A3.SS1.p1.m17" intent=":literal"><semantics><mrow><mrow><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><msup><mi>R</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">u(i,j)\in R^{+}</annotation></semantics></math> associated with an element of state follows from the definition of matrix <math alttext="U" class="ltx_Math" display="inline" id="A3.SS1.p1.m18" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>; we assume for simplicity that <math alttext="U" class="ltx_Math" display="inline" id="A3.SS1.p1.m19" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is diagonal (which corresponds to the case of ALiBi). Finally, in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S2.T1" title="Table 1 ‣ Inference dynamics of BDH. ‣ 2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) ‣ 2 BDH: a language model architecture given by local distributed graph dynamics ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">1</span></a>, the auxiliary node variables <math alttext="X^{\mathfrak{e}}(i),X^{\mathfrak{i}}(i),Y^{\mathfrak{e}}(i),Y^{\mathfrak{i}}(i)" class="ltx_Math" display="inline" id="A3.SS1.p1.m20" intent=":literal"><semantics><mrow><mrow><msup><mi>X</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mi>X</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mi>Y</mi><mi>𝔢</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msup><mi>Y</mi><mi>𝔦</mi></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">X^{\mathfrak{e}}(i),X^{\mathfrak{i}}(i),Y^{\mathfrak{e}}(i),Y^{\mathfrak{i}}(i)</annotation></semantics></math> are used to handle the thresholding of the inhibitory circuit.
∎</p>
</div>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Formal statement of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim7" title="Claim 7 (informal statement). ‣ Expressiveness of linear attention in dimension 𝑛. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a> (linear attention)</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p">We provide the following Claim, expressing the operation of attention under <em class="ltx_emph ltx_font_italic"><math alttext="C" class="ltx_Math" display="inline" id="A3.SS2.p1.m1" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>-non-adversarial</em> key vectors <math alttext="(k_{\tau})" class="ltx_Math" display="inline" id="A3.SS2.p1.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_{\tau})</annotation></semantics></math>, <math alttext="t=1\ldots t" class="ltx_Math" display="inline" id="A3.SS2.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">t=1\ldots t</annotation></semantics></math>, understood in the sense that there exists <math alttext="C\in N" class="ltx_Math" display="inline" id="A3.SS2.p1.m4" intent=":literal"><semantics><mrow><mi>C</mi><mo>∈</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">C\in N</annotation></semantics></math>, <math alttext="0\leq C&lt;t-1" class="ltx_Math" display="inline" id="A3.SS2.p1.m5" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>C</mi><mo>&lt;</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">0\leq C&lt;t-1</annotation></semantics></math> such that, if considering <math alttext="(k_{\tau})" class="ltx_Math" display="inline" id="A3.SS2.p1.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_{\tau})</annotation></semantics></math> as a sequence of random variables, each <math alttext="f(k_{\tau})" class="ltx_Math" display="inline" id="A3.SS2.p1.m7" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(k_{\tau})</annotation></semantics></math>, <math alttext="\tau=1\ldots t" class="ltx_Math" display="inline" id="A3.SS2.p1.m8" intent=":literal"><semantics><mrow><mi>τ</mi><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">\tau=1\ldots t</annotation></semantics></math>, can be considered sampled independently at random in <math alttext="S^{\nu}" class="ltx_Math" display="inline" id="A3.SS2.p1.m9" intent=":literal"><semantics><msup><mi>S</mi><mi>ν</mi></msup><annotation encoding="application/x-tex">S^{\nu}</annotation></semantics></math> with respect to all keys sampled previously, except for at most <math alttext="C" class="ltx_Math" display="inline" id="A3.SS2.p1.m10" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> such keys. We put <math alttext="C=t-1" class="ltx_Math" display="inline" id="A3.SS2.p1.m11" intent=":literal"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">C=t-1</annotation></semantics></math> for adversarial inputs, or if this condition cannot be satisfied at all due to the nature of function <math alttext="f" class="ltx_Math" display="inline" id="A3.SS2.p1.m12" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.</p>
</div>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim8">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 8</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="\Lambda" class="ltx_Math" display="inline" id="Thmclaim8.p1.m1" intent=":literal"><semantics><mi mathvariant="normal">Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> be a space of keys and queries, let <math alttext="\phi:\Lambda\times\Lambda\to[-1,1]" class="ltx_Math" display="inline" id="Thmclaim8.p1.m2" intent=":literal"><semantics><mrow><mi>ϕ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi mathvariant="normal">Λ</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi mathvariant="normal">Λ</mi></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">[</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\phi:\Lambda\times\Lambda\to[-1,1]</annotation></semantics></math> be an attention affinity function, and let <math alttext="f:\Lambda\to S^{\nu}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m3" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi mathvariant="normal">Λ</mi><mo stretchy="false">→</mo><msup><mi>S</mi><mi>ν</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f:\Lambda\to S^{\nu}</annotation></semantics></math>, for some <math alttext="\nu=O(\mathrm{poly}(n))" class="ltx_Math" display="inline" id="Thmclaim8.p1.m4" intent=":literal"><semantics><mrow><mi>ν</mi><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>poly</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\nu=O(\mathrm{poly}(n))</annotation></semantics></math>, be such that for any <math alttext="q,k\in R" class="ltx_Math" display="inline" id="Thmclaim8.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>q</mi><mo>,</mo><mi>k</mi></mrow><mo>∈</mo><mi>R</mi></mrow><annotation encoding="application/x-tex">q,k\in R</annotation></semantics></math>, we have <math alttext="f(q)\cdot f(k)=\phi(q,k)\pm O(n^{-100})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m6" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>q</mi><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>ϕ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mrow><mo>±</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>n</mi><mrow><mo>−</mo><mn>100</mn></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">f(q)\cdot f(k)=\phi(q,k)\pm O(n^{-100})</annotation></semantics></math>.
Fix <math alttext="\delta&gt;0" class="ltx_Math" display="inline" id="Thmclaim8.p1.m7" intent=":literal"><semantics><mrow><mi>δ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta&gt;0</annotation></semantics></math> and <math alttext="C\in\mathbb{N}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m8" intent=":literal"><semantics><mrow><mi>C</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">C\in\mathbb{N}</annotation></semantics></math>. Let <math alttext="A_{\phi,t}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m9" intent=":literal"><semantics><msub><mi>A</mi><mrow><mi>ϕ</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">A_{\phi,t}</annotation></semantics></math> be a block which computes attention <math alttext="a_{t}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m10" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> given by Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.E14" title="In State capacity vs. distinction capacity. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>), for a given sequence of key-query inputs <math alttext="(k_{1},\ldots,k_{t})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m11" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_{1},\ldots,k_{t})</annotation></semantics></math> and values <math alttext="(v_{1},\ldots,v_{t})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m12" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>v</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v_{1},\ldots,v_{t})</annotation></semantics></math>, where <math alttext="t&lt;\delta n/((C+1)\log n)" class="ltx_Math" display="inline" id="Thmclaim8.p1.m13" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>C</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t&lt;\delta n/((C+1)\log n)</annotation></semantics></math> is fixed, <math alttext="k_{\tau}\in\Lambda" class="ltx_Math" display="inline" id="Thmclaim8.p1.m14" intent=":literal"><semantics><mrow><msub><mi>k</mi><mi>τ</mi></msub><mo>∈</mo><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">k_{\tau}\in\Lambda</annotation></semantics></math>, and <math alttext="v_{\tau}\in R^{d}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m15" intent=":literal"><semantics><mrow><msub><mi>v</mi><mi>τ</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">v_{\tau}\in R^{d}</annotation></semantics></math> are of similar strength in the L2-norm, with <math alttext="c_{1}\leq\|v_{\tau}\|\leq c_{2}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m16" intent=":literal"><semantics><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>≤</mo><mrow><mo stretchy="false">‖</mo><msub><mi>v</mi><mi>τ</mi></msub><mo stretchy="false">‖</mo></mrow><mo>≤</mo><msub><mi>c</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">c_{1}\leq\|v_{\tau}\|\leq c_{2}</annotation></semantics></math>, for all <math alttext="\tau=1\ldots t" class="ltx_Math" display="inline" id="Thmclaim8.p1.m17" intent=":literal"><semantics><mrow><mi>τ</mi><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">\tau=1\ldots t</annotation></semantics></math>, for some constants <math alttext="0&lt;c_{1}\leq c_{2}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m18" intent=":literal"><semantics><mrow><mn>0</mn><mo>&lt;</mo><msub><mi>c</mi><mn>1</mn></msub><mo>≤</mo><msub><mi>c</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">0&lt;c_{1}\leq c_{2}</annotation></semantics></math>. Then the (simplified) linear attention equation of BDH-GPU:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{a^{*}_{t}}:=\sum_{\tau=1}^{t-1}{v_{\tau}}{x_{\tau}}^{T}{x_{t}}" class="ltx_Math" display="block" id="A3.E17.m1" intent=":literal"><semantics><mrow><msubsup><mi>a</mi><mi>t</mi><mo>∗</mo></msubsup><mo rspace="0.111em">:=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>v</mi><mi>τ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mi>τ</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">{a^{*}_{t}}:=\sum_{\tau=1}^{t-1}{v_{\tau}}{x_{\tau}}^{T}{x_{t}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">expresses <math alttext="A_{\phi,t}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m19" intent=":literal"><semantics><msub><mi>A</mi><mrow><mi>ϕ</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">A_{\phi,t}</annotation></semantics></math> with <math alttext="O(\sqrt{\delta})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m20" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>δ</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\sqrt{\delta})</annotation></semantics></math>-error in the L2-norm (i.e., <math alttext="\|a^{*}_{\tau}-a_{\tau}\|=O(\sqrt{\delta})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m21" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">‖</mo><mrow><msubsup><mi>a</mi><mi>τ</mi><mo>∗</mo></msubsup><mo>−</mo><msub><mi>a</mi><mi>τ</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>δ</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\|a^{*}_{\tau}-a_{\tau}\|=O(\sqrt{\delta})</annotation></semantics></math>, provided that the input vector <math alttext="(k_{\tau})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m22" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k_{\tau})</annotation></semantics></math> is <math alttext="C" class="ltx_Math" display="inline" id="Thmclaim8.p1.m23" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>-non-adversarial, under a suitable randomly chosen key preparation function <math alttext="f^{\prime}:\Lambda\to R^{n}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m24" intent=":literal"><semantics><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi mathvariant="normal">Λ</mi><mo stretchy="false">→</mo><msup><mi>R</mi><mi>n</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f^{\prime}:\Lambda\to R^{n}</annotation></semantics></math> , <math alttext="x_{\tau}:=f^{\prime}(k_{\tau})" class="ltx_Math" display="inline" id="Thmclaim8.p1.m25" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>τ</mi></msub><mo>:=</mo><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{\tau}:=f^{\prime}(k_{\tau})</annotation></semantics></math>, where <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m26" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math> depends on <math alttext="f" class="ltx_Math" display="inline" id="Thmclaim8.p1.m27" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, w.h.p. in <math alttext="n" class="ltx_Math" display="inline" id="Thmclaim8.p1.m28" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> with respect to choice of <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="Thmclaim8.p1.m29" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof (sketch).</h6>
<div class="ltx_para ltx_noindent" id="A3.SS2.p2">
<p class="ltx_p">To simplify notation, assume w.l.o.g. that <math alttext="\Lambda=S^{\nu}" class="ltx_Math" display="inline" id="A3.SS2.p2.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Λ</mi><mo>=</mo><msup><mi>S</mi><mi>ν</mi></msup></mrow><annotation encoding="application/x-tex">\Lambda=S^{\nu}</annotation></semantics></math> and <math alttext="f=idem" class="ltx_Math" display="inline" id="A3.SS2.p2.m2" intent=":literal"><semantics><mrow><mi>f</mi><mo>=</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">f=idem</annotation></semantics></math>; to undo this assumption, at the end of the proof we apply <math alttext="f\circ f^{\prime}" class="ltx_Math" display="inline" id="A3.SS2.p2.m3" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.222em" rspace="0.222em">∘</mo><msup><mi>f</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">f\circ f^{\prime}</annotation></semantics></math> for preparation in place of <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="A3.SS2.p2.m4" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p3">
<p class="ltx_p">All vectors <math alttext="v" class="ltx_Math" display="inline" id="A3.SS2.p3.m1" intent=":literal"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> and the result <math alttext="a_{t}" class="ltx_Math" display="inline" id="A3.SS2.p3.m2" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> we are looking to calculate are in <math alttext="R^{d}" class="ltx_Math" display="inline" id="A3.SS2.p3.m3" intent=":literal"><semantics><msup><mi>R</mi><mi>d</mi></msup><annotation encoding="application/x-tex">R^{d}</annotation></semantics></math>. With this notation, the attention task we are approximating is:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.E18">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="a_{t}=q\sum_{\tau=1}^{t}k_{\tau}^{T}v_{\tau}." class="ltx_Math" display="block" id="A3.E18.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>q</mi><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><msubsup><mi>k</mi><mi>τ</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>v</mi><mi>τ</mi></msub></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">a_{t}=q\sum_{\tau=1}^{t}k_{\tau}^{T}v_{\tau}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">(this is still the general form of attention almost precisely equivalent to (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.E14" title="In State capacity vs. distinction capacity. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">14</span></a>), not a special case).</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p4">
<p class="ltx_p">The goal is to show how, subject to <math alttext="t&lt;\delta n/\log^{2}n" class="ltx_Math" display="inline" id="A3.SS2.p4.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em">​</mo><mi>n</mi></mrow><mo>/</mo><mrow><msup><mi>log</mi><mn>2</mn></msup><mo lspace="0.167em">⁡</mo><mi>n</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">t&lt;\delta n/\log^{2}n</annotation></semantics></math>, linear attention in dimension <math alttext="n" class="ltx_Math" display="inline" id="A3.SS2.p4.m2" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> given by (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.E17" title="In Claim 8. ‣ C.2 Formal statement of Claim 7 (linear attention) ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">17</span></a>) is a sufficiently precise estimation of (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.E18" title="In C.2 Formal statement of Claim 7 (linear attention) ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">18</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p5">
<p class="ltx_p">Consider now, with <math alttext="\Lambda=S^{\nu}" class="ltx_Math" display="inline" id="A3.SS2.p5.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Λ</mi><mo>=</mo><msup><mi>S</mi><mi>ν</mi></msup></mrow><annotation encoding="application/x-tex">\Lambda=S^{\nu}</annotation></semantics></math>, <math alttext="f^{\prime}:S^{\nu}\to R^{n}" class="ltx_Math" display="inline" id="A3.SS2.p5.m2" intent=":literal"><semantics><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>S</mi><mi>ν</mi></msup><mo stretchy="false">→</mo><msup><mi>R</mi><mi>n</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f^{\prime}:S^{\nu}\to R^{n}</annotation></semantics></math>, where we recall that <math alttext="x_{\tau}:=f^{\prime}(k_{\tau})" class="ltx_Math" display="inline" id="A3.SS2.p5.m3" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>τ</mi></msub><mo>:=</mo><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{\tau}:=f^{\prime}(k_{\tau})</annotation></semantics></math>, to be a suitable dimensionality reduction preserving approximation of scalar product between <math alttext="R^{\nu}" class="ltx_Math" display="inline" id="A3.SS2.p5.m4" intent=":literal"><semantics><msup><mi>R</mi><mi>ν</mi></msup><annotation encoding="application/x-tex">R^{\nu}</annotation></semantics></math> and <math alttext="R^{n}" class="ltx_Math" display="inline" id="A3.SS2.p5.m5" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math>. For simplicity of argument, we let <math alttext="f^{\prime}:R^{\nu}\to R^{n}" class="ltx_Math" display="inline" id="A3.SS2.p5.m6" intent=":literal"><semantics><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>R</mi><mi>ν</mi></msup><mo stretchy="false">→</mo><msup><mi>R</mi><mi>n</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f^{\prime}:R^{\nu}\to R^{n}</annotation></semantics></math> be a standard Johnson-Lindenstrauss transform, with the additional property that <math alttext="f^{\prime}(-z)=-f^{\prime}(z)" class="ltx_Math" display="inline" id="A3.SS2.p5.m7" intent=":literal"><semantics><mrow><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mi>z</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">f^{\prime}(-z)=-f^{\prime}(z)</annotation></semantics></math> for all <math alttext="z\in R^{\nu}" class="ltx_Math" display="inline" id="A3.SS2.p5.m8" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><msup><mi>R</mi><mi>ν</mi></msup></mrow><annotation encoding="application/x-tex">z\in R^{\nu}</annotation></semantics></math> (easy to obtain from any other Johnson-Lindenstrauss transform <math alttext="f^{\prime\prime}" class="ltx_Math" display="inline" id="A3.SS2.p5.m9" intent=":literal"><semantics><msup><mi>f</mi><mo>′′</mo></msup><annotation encoding="application/x-tex">f^{\prime\prime}</annotation></semantics></math> by taking <math alttext="f^{\prime}(z):=(f^{\prime\prime}(z)-f^{\prime\prime}(-z))/2" class="ltx_Math" display="inline" id="A3.SS2.p5.m10" intent=":literal"><semantics><mrow><mrow><msup><mi>f</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msup><mi>f</mi><mo>′′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msup><mi>f</mi><mo>′′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo>−</mo><mi>z</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">f^{\prime}(z):=(f^{\prime\prime}(z)-f^{\prime\prime}(-z))/2</annotation></semantics></math>). The distortion of scalar product in <math alttext="R^{n}" class="ltx_Math" display="inline" id="A3.SS2.p5.m11" intent=":literal"><semantics><msup><mi>R</mi><mi>n</mi></msup><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math> is then known to be bounded as follows: <math alttext="|{k_{\tau}}^{T}{k_{t}}-{x_{\tau}}^{T}{x_{t}}|=O(\varepsilon)(\|k_{\tau}\|+\|k_{t}\|)=O(\varepsilon)" class="ltx_Math" display="inline" id="A3.SS2.p5.m12" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mrow><mrow><mmultiscripts><mi>k</mi><mi>τ</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><mo>−</mo><mrow><mmultiscripts><mi>x</mi><mi>τ</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">‖</mo><msub><mi>k</mi><mi>τ</mi></msub><mo stretchy="false">‖</mo></mrow><mo>+</mo><mrow><mo stretchy="false">‖</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">|{k_{\tau}}^{T}{k_{t}}-{x_{\tau}}^{T}{x_{t}}|=O(\varepsilon)(\|k_{\tau}\|+\|k_{t}\|)=O(\varepsilon)</annotation></semantics></math>, w.h.p. with respect to choice of <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="A3.SS2.p5.m13" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math>. Here, <math alttext="\varepsilon=\sqrt{\log n/n}=O(\sqrt{\delta})/\sqrt{(C+1)t\log t}" class="ltx_Math" display="inline" id="A3.SS2.p5.m14" intent=":literal"><semantics><mrow><mi>ε</mi><mo>=</mo><msqrt><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mrow><mi>n</mi><mo>/</mo><mi>n</mi></mrow></mrow></msqrt><mo>=</mo><mrow><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>δ</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msqrt><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>C</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>t</mi></mrow></mrow></msqrt></mrow></mrow><annotation encoding="application/x-tex">\varepsilon=\sqrt{\log n/n}=O(\sqrt{\delta})/\sqrt{(C+1)t\log t}</annotation></semantics></math>, where the last inequality follows from the assumption on <math alttext="t" class="ltx_Math" display="inline" id="A3.SS2.p5.m15" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> made in the Claim.</p>
</div>
<div class="ltx_para" id="A3.SS2.p6">
<p class="ltx_p">We now consider the sequence <math alttext="r_{\tau}:={k_{\tau}}^{T}{k_{t}}-{x_{\tau}}^{T}{x_{t}}" class="ltx_Math" display="inline" id="A3.SS2.p6.m1" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>τ</mi></msub><mo>:=</mo><mrow><mrow><mmultiscripts><mi>k</mi><mi>τ</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><mo>−</mo><mrow><mmultiscripts><mi>x</mi><mi>τ</mi><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{\tau}:={k_{\tau}}^{T}{k_{t}}-{x_{\tau}}^{T}{x_{t}}</annotation></semantics></math>, for <math alttext="\tau&lt;t" class="ltx_Math" display="inline" id="A3.SS2.p6.m2" intent=":literal"><semantics><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\tau&lt;t</annotation></semantics></math>. Set aside the (at most <math alttext="C" class="ltx_Math" display="inline" id="A3.SS2.p6.m3" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>) elements <math alttext="r_{\tau}" class="ltx_Math" display="inline" id="A3.SS2.p6.m4" intent=":literal"><semantics><msub><mi>r</mi><mi>τ</mi></msub><annotation encoding="application/x-tex">r_{\tau}</annotation></semantics></math> for which <math alttext="k_{\tau}" class="ltx_Math" display="inline" id="A3.SS2.p6.m5" intent=":literal"><semantics><msub><mi>k</mi><mi>τ</mi></msub><annotation encoding="application/x-tex">k_{\tau}</annotation></semantics></math> and <math alttext="k_{t}" class="ltx_Math" display="inline" id="A3.SS2.p6.m6" intent=":literal"><semantics><msub><mi>k</mi><mi>t</mi></msub><annotation encoding="application/x-tex">k_{t}</annotation></semantics></math> are not independent. For all other elements, consider that <math alttext="|r_{\tau}|=O(\varepsilon)" class="ltx_Math" display="inline" id="A3.SS2.p6.m7" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><msub><mi>r</mi><mi>τ</mi></msub><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">|r_{\tau}|=O(\varepsilon)</annotation></semantics></math> as established previously, and the sign <math alttext="r_{\tau}/|r_{\tau}|" class="ltx_Math" display="inline" id="A3.SS2.p6.m8" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>τ</mi></msub><mo>/</mo><mrow><mo stretchy="false">|</mo><msub><mi>r</mi><mi>τ</mi></msub><mo stretchy="false">|</mo></mrow></mrow><annotation encoding="application/x-tex">r_{\tau}/|r_{\tau}|</annotation></semantics></math> is chosen independently at random with respect to all but at least <math alttext="C" class="ltx_Math" display="inline" id="A3.SS2.p6.m9" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> elements by the conditions imposed on <math alttext="f^{\prime}" class="ltx_Math" display="inline" id="A3.SS2.p6.m10" intent=":literal"><semantics><msup><mi>f</mi><mo>′</mo></msup><annotation encoding="application/x-tex">f^{\prime}</annotation></semantics></math> and <math alttext="k_{\tau}" class="ltx_Math" display="inline" id="A3.SS2.p6.m11" intent=":literal"><semantics><msub><mi>k</mi><mi>τ</mi></msub><annotation encoding="application/x-tex">k_{\tau}</annotation></semantics></math>. It follows that <math alttext="\sum_{\tau=1}^{t}r_{\tau}" class="ltx_Math" display="inline" id="A3.SS2.p6.m12" intent=":literal"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>τ</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msub><mi>r</mi><mi>τ</mi></msub></mrow><annotation encoding="application/x-tex">\sum_{\tau=1}^{t}r_{\tau}</annotation></semantics></math> can be represented as a sum of <math alttext="O(C)" class="ltx_Math" display="inline" id="A3.SS2.p6.m13" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(C)</annotation></semantics></math> martingales, each of which has length <math alttext="O(t/(C+1))" class="ltx_Math" display="inline" id="A3.SS2.p6.m14" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>t</mi><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>C</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(t/(C+1))</annotation></semantics></math> and all elements bounded by <math alttext="O(\varepsilon)" class="ltx_Math" display="inline" id="A3.SS2.p6.m15" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\varepsilon)</annotation></semantics></math> with <math alttext="\varepsilon=O(\sqrt{\delta})/\sqrt{(C+1)t\log t}" class="ltx_Math" display="inline" id="A3.SS2.p6.m16" intent=":literal"><semantics><mrow><mi>ε</mi><mo>=</mo><mrow><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mi>δ</mi></msqrt><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msqrt><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>C</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>t</mi></mrow></mrow></msqrt></mrow></mrow><annotation encoding="application/x-tex">\varepsilon=O(\sqrt{\delta})/\sqrt{(C+1)t\log t}</annotation></semantics></math>. The Claim follows directly, by applying Azuma’s inequality to each of these martingales independently.
∎</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p7">
<p class="ltx_p">Considering the extreme cases of <math alttext="C=0" class="ltx_Math" display="inline" id="A3.SS2.p7.m1" intent=":literal"><semantics><mrow><mi>C</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">C=0</annotation></semantics></math> and <math alttext="C=t-1" class="ltx_Math" display="inline" id="A3.SS2.p7.m2" intent=":literal"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">C=t-1</annotation></semantics></math>, the above Claim leads directly to Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim7" title="Claim 7 (informal statement). ‣ Expressiveness of linear attention in dimension 𝑛. ‣ 6.1 Macro-expressiveness of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">7</span></a>, clarifying over what time, linear attention can be used to express general attention.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Proof of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim3" title="Claim 3. ‣ 3.4.1 Expressing matrices 𝐷_𝑥,𝐷_𝑦,𝐸 as graphs 𝐺_𝑥,𝐺_𝑦 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a>
</h3>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p">The proof is almost immediate, through the construction of an appropriate neuron-synapse interaction graphs <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m1" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m2" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math> such that <math alttext="G^{\mathfrak{e}}={H^{\mathfrak{e}}}^{2}[V]" class="ltx_Math" display="inline" id="A3.SS3.p1.m3" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>=</mo><mrow><mmultiscripts><mi>H</mi><mrow></mrow><mi>𝔢</mi><mrow></mrow><mn>2</mn></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}={H^{\mathfrak{e}}}^{2}[V]</annotation></semantics></math> and <math alttext="G^{\mathfrak{i}}={H^{\mathfrak{i}}}^{2}[V]" class="ltx_Math" display="inline" id="A3.SS3.p1.m4" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔦</mi></msup><mo>=</mo><mrow><mmultiscripts><mi>H</mi><mrow></mrow><mi>𝔦</mi><mrow></mrow><mn>2</mn></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{i}}={H^{\mathfrak{i}}}^{2}[V]</annotation></semantics></math>. Consider <math alttext="E^{\prime}\in(R^{+})^{2d\times n}" class="ltx_Math" display="inline" id="A3.SS3.p1.m5" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{\prime}\in(R^{+})^{2d\times n}</annotation></semantics></math> such that <math alttext="E^{\prime}_{\alpha,j}=\left(E_{\alpha,j}\right)^{+}" class="ltx_Math" display="inline" id="A3.SS3.p1.m6" intent=":literal"><semantics><mrow><msubsup><mi>E</mi><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow><mo>′</mo></msubsup><mo>=</mo><msup><mrow><mo>(</mo><msub><mi>E</mi><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">E^{\prime}_{\alpha,j}=\left(E_{\alpha,j}\right)^{+}</annotation></semantics></math> and <math alttext="E^{\prime}_{\alpha+d,j}=\left(-E_{\alpha,j}\right)^{+}" class="ltx_Math" display="inline" id="A3.SS3.p1.m7" intent=":literal"><semantics><mrow><msubsup><mi>E</mi><mrow><mrow><mi>α</mi><mo>+</mo><mi>d</mi></mrow><mo>,</mo><mi>j</mi></mrow><mo>′</mo></msubsup><mo>=</mo><msup><mrow><mo>(</mo><mrow><mo>−</mo><msub><mi>E</mi><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">E^{\prime}_{\alpha+d,j}=\left(-E_{\alpha,j}\right)^{+}</annotation></semantics></math>, for <math alttext="j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="A3.SS3.p1.m8" intent=":literal"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">j\in\{1,\ldots,n\}</annotation></semantics></math> and <math alttext="\alpha\in\{1,\ldots,d\}" class="ltx_Math" display="inline" id="A3.SS3.p1.m9" intent=":literal"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in\{1,\ldots,d\}</annotation></semantics></math>. Define <math alttext="D^{\mathfrak{e}},D^{\mathfrak{i}}\in(R^{+})^{n\times 2d}" class="ltx_Math" display="inline" id="A3.SS3.p1.m10" intent=":literal"><semantics><mrow><mrow><msup><mi>D</mi><mi>𝔢</mi></msup><mo>,</mo><msup><mi>D</mi><mi>𝔦</mi></msup></mrow><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D^{\mathfrak{e}},D^{\mathfrak{i}}\in(R^{+})^{n\times 2d}</annotation></semantics></math> so that:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex32">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(D^{\mathfrak{e}}-D^{\mathfrak{i}})E^{\prime}=DE." class="ltx_Math" display="block" id="A3.Ex32.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>D</mi><mi>𝔢</mi></msup><mo>−</mo><msup><mi>D</mi><mi>𝔦</mi></msup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>′</mo></msup></mrow><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(D^{\mathfrak{e}}-D^{\mathfrak{i}})E^{\prime}=DE.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Indeed, notice that this is always possible by redistributing elements of <math alttext="D" class="ltx_Math" display="inline" id="A3.SS3.p1.m11" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> into <math alttext="D^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m12" intent=":literal"><semantics><msup><mi>D</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">D^{\mathfrak{e}}</annotation></semantics></math> and <math alttext="D^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m13" intent=":literal"><semantics><msup><mi>D</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">D^{\mathfrak{i}}</annotation></semantics></math> (putting <math alttext="D^{\mathfrak{e}}_{i,\alpha}=D^{\mathfrak{i}}_{i+d,\alpha}=\left(D_{i,\alpha}\right)^{+}" class="ltx_Math" display="inline" id="A3.SS3.p1.m14" intent=":literal"><semantics><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow><mi>𝔢</mi></msubsup><mo>=</mo><msubsup><mi>D</mi><mrow><mrow><mi>i</mi><mo>+</mo><mi>d</mi></mrow><mo>,</mo><mi>α</mi></mrow><mi>𝔦</mi></msubsup><mo>=</mo><msup><mrow><mo>(</mo><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow></msub><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">D^{\mathfrak{e}}_{i,\alpha}=D^{\mathfrak{i}}_{i+d,\alpha}=\left(D_{i,\alpha}\right)^{+}</annotation></semantics></math>) and <math alttext="D^{\mathfrak{i}}_{i,\alpha}=D^{\mathfrak{e}}_{i+d,\alpha}=\left(-D_{i,\alpha}\right)^{+}" class="ltx_Math" display="inline" id="A3.SS3.p1.m15" intent=":literal"><semantics><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow><mi>𝔦</mi></msubsup><mo>=</mo><msubsup><mi>D</mi><mrow><mrow><mi>i</mi><mo>+</mo><mi>d</mi></mrow><mo>,</mo><mi>α</mi></mrow><mi>𝔢</mi></msubsup><mo>=</mo><msup><mrow><mo>(</mo><mrow><mo>−</mo><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">D^{\mathfrak{i}}_{i,\alpha}=D^{\mathfrak{e}}_{i+d,\alpha}=\left(-D_{i,\alpha}\right)^{+}</annotation></semantics></math>), so that, for all <math alttext="i,j\in\{1,\ldots,n\}" class="ltx_Math" display="inline" id="A3.SS3.p1.m16" intent=":literal"><semantics><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i,j\in\{1,\ldots,n\}</annotation></semantics></math> and <math alttext="\alpha\in\{1,\ldots,d\}" class="ltx_Math" display="inline" id="A3.SS3.p1.m17" intent=":literal"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in\{1,\ldots,d\}</annotation></semantics></math>, we have:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex33">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(D^{\mathfrak{e}}_{i,\alpha}-D^{\mathfrak{i}}_{i,\alpha})E^{\prime}_{\alpha,j}+(D^{\mathfrak{e}}_{i,\alpha+d}-D^{\mathfrak{i}}_{i,\alpha+d})E^{\prime}_{\alpha+d,j}=D_{i,\alpha}E_{\alpha,j}." class="ltx_Math" display="block" id="A3.Ex33.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow><mi>𝔢</mi></msubsup><mo>−</mo><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow><mi>𝔦</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>E</mi><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow><mo>′</mo></msubsup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>α</mi><mo>+</mo><mi>d</mi></mrow></mrow><mi>𝔢</mi></msubsup><mo>−</mo><msubsup><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>α</mi><mo>+</mo><mi>d</mi></mrow></mrow><mi>𝔦</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>E</mi><mrow><mrow><mi>α</mi><mo>+</mo><mi>d</mi></mrow><mo>,</mo><mi>j</mi></mrow><mo>′</mo></msubsup></mrow></mrow><mo>=</mo><mrow><msub><mi>D</mi><mrow><mi>i</mi><mo>,</mo><mi>α</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>E</mi><mrow><mi>α</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">(D^{\mathfrak{e}}_{i,\alpha}-D^{\mathfrak{i}}_{i,\alpha})E^{\prime}_{\alpha,j}+(D^{\mathfrak{e}}_{i,\alpha+d}-D^{\mathfrak{i}}_{i,\alpha+d})E^{\prime}_{\alpha+d,j}=D_{i,\alpha}E_{\alpha,j}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Considering <math alttext="S=\{1,\ldots,2d\}" class="ltx_Math" display="inline" id="A3.SS3.p1.m18" intent=":literal"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S=\{1,\ldots,2d\}</annotation></semantics></math>, the definition of <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m19" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math> as the union of edges of <math alttext="D^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m20" intent=":literal"><semantics><msup><mi>D</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">D^{\mathfrak{e}}</annotation></semantics></math> and <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="A3.SS3.p1.m21" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">E^{\prime}</annotation></semantics></math> on input neuron layer <math alttext="V" class="ltx_Math" display="inline" id="A3.SS3.p1.m22" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>, hidden layer <math alttext="S" class="ltx_Math" display="inline" id="A3.SS3.p1.m23" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>, and output neuron layer <math alttext="V" class="ltx_Math" display="inline" id="A3.SS3.p1.m24" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> follows. Likewise, we define <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m25" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math> as the union of edges of <math alttext="D^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.p1.m26" intent=":literal"><semantics><msup><mi>D</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">D^{\mathfrak{i}}</annotation></semantics></math> and <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="A3.SS3.p1.m27" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">E^{\prime}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A3.SS3.p2">
<p class="ltx_p">We verify that for <math alttext="G^{\mathfrak{e}}={H^{\mathfrak{e}}}^{2}[V]" class="ltx_Math" display="inline" id="A3.SS3.p2.m1" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>=</mo><mrow><mmultiscripts><mi>H</mi><mrow></mrow><mi>𝔢</mi><mrow></mrow><mn>2</mn></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}={H^{\mathfrak{e}}}^{2}[V]</annotation></semantics></math> and <math alttext="G^{\mathfrak{i}}={H^{\mathfrak{i}}}^{2}[V]" class="ltx_Math" display="inline" id="A3.SS3.p2.m2" intent=":literal"><semantics><mrow><msup><mi>G</mi><mi>𝔦</mi></msup><mo>=</mo><mrow><mmultiscripts><mi>H</mi><mrow></mrow><mi>𝔦</mi><mrow></mrow><mn>2</mn></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mi>V</mi><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{i}}={H^{\mathfrak{i}}}^{2}[V]</annotation></semantics></math>, we have <math alttext="G^{\mathfrak{e}}-G^{\mathfrak{i}}=DE" class="ltx_Math" display="inline" id="A3.SS3.p2.m3" intent=":literal"><semantics><mrow><mrow><msup><mi>G</mi><mi>𝔢</mi></msup><mo>−</mo><msup><mi>G</mi><mi>𝔦</mi></msup></mrow><mo>=</mo><mrow><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></mrow><annotation encoding="application/x-tex">G^{\mathfrak{e}}-G^{\mathfrak{i}}=DE</annotation></semantics></math>, and the Claim holds.
∎</p>
</div>
</div>
<section class="ltx_paragraph" id="A3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Considerations of building linear circuits.</h5>
<div class="ltx_para ltx_noindent" id="A3.SS3.SSS0.Px1.p1">
<p class="ltx_p">The above proof makes the neuron-synapse interaction graphs <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m1" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m2" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math> sparse in terms of the number of edges, as required to show that the number of parameters are preserved by correspondence. However, it is a purely technical construction, and nodes in the synaptic layer have high degree, <math alttext="n" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. While preserving strict equivalence of linear dynamics, the degrees of nodes of the considered graphs in the synaptic layer can be reduced in this construction, at the cost of increasing the number of edges of graphs <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m4" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m5" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math>. (For example, subdividing each node of the synaptic layer into <math alttext="a^{2}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m6" intent=":literal"><semantics><msup><mi>a</mi><mn>2</mn></msup><annotation encoding="application/x-tex">a^{2}</annotation></semantics></math> nodes can be used to reduce their degree <math alttext="\Theta(a)" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m7" intent=":literal"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(a)</annotation></semantics></math>-times, while increasing the number of edges <math alttext="\Theta(a)" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m8" intent=":literal"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(a)</annotation></semantics></math>-times; putting <math alttext="a=\sqrt{n/d}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m9" intent=":literal"><semantics><mrow><mi>a</mi><mo>=</mo><msqrt><mrow><mi>n</mi><mo>/</mo><mi>d</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">a=\sqrt{n/d}</annotation></semantics></math> we reach graphs <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m10" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m11" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math> with degree <math alttext="O(\sqrt{nd})" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m12" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msqrt><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msqrt><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\sqrt{nd})</annotation></semantics></math> in both the neuron and synaptic layers, and consequently <math alttext="O(n\sqrt{nd})" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p1.m13" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msqrt></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n\sqrt{nd})</annotation></semantics></math> edges.)</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.SSS0.Px1.p2">
<p class="ltx_p">Reduction of internal degrees in this circuit is also possible by introducing more than 1 hidden layer, creating a form of branching circuit. The implementation for this in a distributed way remains very simple, as the considered dynamics of the form <math alttext="z\to Gz" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p2.m1" intent=":literal"><semantics><mrow><mi>z</mi><mo stretchy="false">→</mo><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mi>z</mi></mrow></mrow><annotation encoding="application/x-tex">z\to Gz</annotation></semantics></math> are linear (token-propagation dynamics). The bound on the number of edges needed to represent such a circuit remains <math alttext="O(nd)" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p2.m2" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math>, even when the circuit has constant degree.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.SSS0.Px1.p3">
<p class="ltx_p">The technical construction of the linear circuits <math alttext="H^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p3.m1" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="H^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p3.m2" intent=":literal"><semantics><msup><mi>H</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">H^{\mathfrak{i}}</annotation></semantics></math> provided in this Appendix do not affect results concerning the analysis of the structure of neuron-neuron interaction graphs <math alttext="G^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p3.m3" intent=":literal"><semantics><msup><mi>G</mi><mi>𝔢</mi></msup><annotation encoding="application/x-tex">G^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="G^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS3.SSS0.Px1.p3.m4" intent=":literal"><semantics><msup><mi>G</mi><mi>𝔦</mi></msup><annotation encoding="application/x-tex">G^{\mathfrak{i}}</annotation></semantics></math>. These neuron-neuron interaction graphs plausibly maintain a heavy-tailed, power-law-like degree distribution, as is the case for the models considered empirically in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S5.SS5" title="5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products ‣ 5 Analysis: emergence of modularity and scale-free structure ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Formal statement of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim4" title="Claim 4. ‣ 3.4.2 Expressing BDH-GPU attention on graphs: sparsification and trainability of 𝐺_𝑠 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>
</h3>
<div class="ltx_theorem ltx_theorem_claim" id="Thmclaim9">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Claim 9</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmclaim9.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="{D_{y}},E" class="ltx_Math" display="inline" id="Thmclaim9.p1.m1" intent=":literal"><semantics><mrow><msub><mi>D</mi><mi>y</mi></msub><mo>,</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">{D_{y}},E</annotation></semantics></math> be parameter matrices of BDH-Normfree. Then, there exists a graph <math alttext="{G_{y}}\in\mathcal{G}(n,O(nd))" class="ltx_Math" display="inline" id="Thmclaim9.p1.m2" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>y</mi></msub><mo>∈</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒢</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{G_{y}}\in\mathcal{G}(n,O(nd))</annotation></semantics></math>, expressible through a sparse linear circuit, a graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m3" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> having <math alttext="O(nd)" class="ltx_Math" display="inline" id="Thmclaim9.p1.m4" intent=":literal"><semantics><mrow><mi>O</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(nd)</annotation></semantics></math> edges, and a sparse linear value preparation function <math alttext="A:{R^{+}}^{n}\to{R^{+}}^{n}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m5" intent=":literal"><semantics><mrow><mi>A</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mmultiscripts><mi>R</mi><mrow></mrow><mo>+</mo><mrow></mrow><mi>n</mi></mmultiscripts><mo stretchy="false">→</mo><mmultiscripts><mi>R</mi><mrow></mrow><mo>+</mo><mrow></mrow><mi>n</mi></mmultiscripts></mrow></mrow><annotation encoding="application/x-tex">A:{R^{+}}^{n}\to{R^{+}}^{n}</annotation></semantics></math>, such that, for any sequence of keys <math alttext="(x_{\tau,l})_{0\leq\tau\leq t}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m6" intent=":literal"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mn>0</mn><mo>≤</mo><mi>τ</mi><mo>≤</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">(x_{\tau,l})_{0\leq\tau\leq t}</annotation></semantics></math> and values <math alttext="(y_{\tau,l-1})_{0\leq\tau\leq t}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m7" intent=":literal"><semantics><msub><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mn>0</mn><mo>≤</mo><mi>τ</mi><mo>≤</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">(y_{\tau,l-1})_{0\leq\tau\leq t}</annotation></semantics></math>, with <math alttext="x_{\tau,l},y_{\tau,l-1}\in{R^{+}}^{n}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m8" intent=":literal"><semantics><mrow><mrow><msub><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub></mrow><mo>∈</mo><mmultiscripts><mi>R</mi><mrow></mrow><mo>+</mo><mrow></mrow><mi>n</mi></mmultiscripts></mrow><annotation encoding="application/x-tex">x_{\tau,l},y_{\tau,l-1}\in{R^{+}}^{n}</annotation></semantics></math>, we have:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex34">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="({G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}){\boldsymbol{\sigma}}^{*}_{t-1,l}x_{t,l}={D_{y}}E{\boldsymbol{\sigma}}_{t-1,l}x_{t,l}," class="ltx_Math" display="block" id="A3.Ex34.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>−</mo><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>=</mo><mrow><msub><mi>D</mi><mi>y</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">({G_{y}}^{\mathfrak{e}}-{G_{y}}^{\mathfrak{i}}){\boldsymbol{\sigma}}^{*}_{t-1,l}x_{t,l}={D_{y}}E{\boldsymbol{\sigma}}_{t-1,l}x_{t,l},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="{\boldsymbol{\sigma}}_{t-1,l}=\sum_{\tau&lt;t}{y_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m9" intent=":literal"><semantics><mrow><msub><mi>𝛔</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><mo rspace="0.111em">=</mo><mrow><msub><mo>∑</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mrow><msub><mi>y</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mi>t</mi><mo>−</mo><mi>τ</mi></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t-1,l}=\sum_{\tau&lt;t}{y_{\tau,l-1}}{x_{\tau,l}}^{T}U^{t-\tau}</annotation></semantics></math> represents the attention state of BDH-Normfree following Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S6.E16" title="In 6.2 Micro-interpretation of attention in BDH-GPU ‣ 6 Analysis: linear attention, sparse positive activation, and monosemanticity ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">16</span></a>), and <math alttext="{\boldsymbol{\sigma}}^{*}_{t-1,l}=\left(\sum_{\tau&lt;t}{A(y_{\tau,l-1})}{x_{\tau,l}}^{T}U^{t-\tau}\right)\odot{G_{s}}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m10" intent=":literal"><semantics><mrow><msubsup><mi>𝛔</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup><mo>=</mo><mrow><mrow><mo>(</mo><mrow><msub><mo lspace="0em">∑</mo><mrow><mi>τ</mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mrow><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>τ</mi><mo>,</mo><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></mrow></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mmultiscripts><mi>x</mi><mrow><mi>τ</mi><mo>,</mo><mi>l</mi></mrow><mrow></mrow><mrow></mrow><mi>T</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>U</mi><mrow><mi>t</mi><mo>−</mo><mi>τ</mi></mrow></msup></mrow></mrow><mo rspace="0.055em">)</mo></mrow><mo rspace="0.222em">⊙</mo><msub><mi>G</mi><mi>s</mi></msub></mrow></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}^{*}_{t-1,l}=\left(\sum_{\tau&lt;t}{A(y_{\tau,l-1})}{x_{\tau,l}}^{T}U^{t-\tau}\right)\odot{G_{s}}</annotation></semantics></math> represents the corresponding attention state of the BDH system with sparse attention on graph <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m11" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>, subject to appropriate preparation of attention values using function <math alttext="f_{y}" class="ltx_Math" display="inline" id="Thmclaim9.p1.m12" intent=":literal"><semantics><msub><mi>f</mi><mi>y</mi></msub><annotation encoding="application/x-tex">f_{y}</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="A3.SS4.p1">
<p class="ltx_p">Before we start the proof, we make a general point about the formulation of the claim. We are considering the problem of expressing (or more generally, approximating) the matrix operator <math alttext="{\boldsymbol{\sigma}}_{t-1,l}" class="ltx_Math" display="inline" id="A3.SS4.p1.m1" intent=":literal"><semantics><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}_{t-1,l}</annotation></semantics></math> by another, sparser one. The setting of our problem can be distilled into obtaining an equality or approximation of the form <math alttext="E{\boldsymbol{\sigma}}_{t-1,l}\approx E^{*}{\boldsymbol{\sigma}}^{*}_{t-1,l}" class="ltx_Math" display="inline" id="A3.SS4.p1.m2" intent=":literal"><semantics><mrow><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>≈</mo><mrow><msup><mi>E</mi><mo>∗</mo></msup><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝝈</mi><mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mi>l</mi></mrow><mo>∗</mo></msubsup></mrow></mrow><annotation encoding="application/x-tex">E{\boldsymbol{\sigma}}_{t-1,l}\approx E^{*}{\boldsymbol{\sigma}}^{*}_{t-1,l}</annotation></semantics></math>, where <math alttext="E\in R^{d\times n}" class="ltx_Math" display="inline" id="A3.SS4.p1.m3" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d\times n}</annotation></semantics></math> is a given low-rank matrix, <math alttext="E^{*}\in R^{d\times n}" class="ltx_Math" display="inline" id="A3.SS4.p1.m4" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{*}\in R^{d\times n}</annotation></semantics></math> can be defined arbitrarily, and <math alttext="{\boldsymbol{\sigma}}^{*}" class="ltx_Math" display="inline" id="A3.SS4.p1.m5" intent=":literal"><semantics><msup><mi>𝝈</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}^{*}</annotation></semantics></math> is defined as in the statement of the Claim. If we content ourselves with an approximation, then it is possible to have <math alttext="{\boldsymbol{\sigma}}^{*}={\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="A3.SS4.p1.m6" intent=":literal"><semantics><mrow><msup><mi>𝝈</mi><mo>∗</mo></msup><mo>=</mo><mi>𝝈</mi></mrow><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}^{*}={\boldsymbol{\sigma}}</annotation></semantics></math> (i.e., put <math alttext="f_{y}=idem" class="ltx_Math" display="inline" id="A3.SS4.p1.m7" intent=":literal"><semantics><mrow><msub><mi>f</mi><mi>y</mi></msub><mo>=</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">f_{y}=idem</annotation></semantics></math>), using for example the stochastic sparsification framework of <cite class="ltx_cite ltx_citemacro_citep">(Achlioptas and Mcsherry, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib2" title="">2007</a>)</cite>, or a value-dependent variant (cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Krauthgamer and Sapir, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib59" title="">2023</a>)</cite>). The samples chosen by such a framework in a value-dependent variant would lead to a graph <math alttext="G_{s}" class="ltx_Math" display="inline" id="A3.SS4.p1.m8" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">G_{s}</annotation></semantics></math> which plausibly reflects the power-law element distributions that we empirically observe in <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="A3.SS4.p1.m9" intent=":literal"><semantics><mi>𝝈</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="A3.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="218" id="A3.F17.g1" src="x8.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Non-uniform graph attention: interpretation of <math alttext="E({\boldsymbol{\sigma}}_{l,t}\odot G_{s})" class="ltx_Math" display="inline" id="A3.F17.m3" intent=":literal"><semantics><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝝈</mi><mrow><mi>l</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em">⊙</mo><msub><mi>G</mi><mi>s</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">E({\boldsymbol{\sigma}}_{l,t}\odot G_{s})</annotation></semantics></math> after sparsification of graph <math alttext="G_{s}" class="ltx_Math" display="inline" id="A3.F17.m4" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">G_{s}</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS4.p2">
<p class="ltx_p">While the spirit of such an approximation is generally valid, we opt in the proof for a simpler, purely technical argument applicable to our specific setting, which gives a strict equality in the statement of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim9" title="Claim 9. ‣ C.4 Formal statement of Claim 4 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a> subject to linear preparation of attention values with a function <math alttext="A" class="ltx_Math" display="inline" id="A3.SS4.p2.m1" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. In practice, this would mean that two successive layers of BDH with sparse state are sufficient to express a layer of BDH-Normfree under this reduction.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS4.p3">
<p class="ltx_p">To prove the claim, it is enough to embed the connection structure of the encoder matrix, treating it as a graph, into <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="A3.SS4.p3.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="A3.SS4.p4">
<p class="ltx_p">(of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim9" title="Claim 9. ‣ C.4 Formal statement of Claim 4 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">9</span></a>)
Fix arbitrarily subset <math alttext="D\subseteq V" class="ltx_Math" display="inline" id="A3.SS4.p4.m1" intent=":literal"><semantics><mrow><mi>D</mi><mo>⊆</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">D\subseteq V</annotation></semantics></math> of neurons, with <math alttext="|D|=2d" class="ltx_Math" display="inline" id="A3.SS4.p4.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi>D</mi><mo stretchy="false">|</mo></mrow><mo>=</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></mrow><annotation encoding="application/x-tex">|D|=2d</annotation></semantics></math>. For the given matrix <math alttext="E\in R^{d\times n}" class="ltx_Math" display="inline" id="A3.SS4.p4.m3" intent=":literal"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E\in R^{d\times n}</annotation></semantics></math> from BDH-GPU, let <math alttext="E^{\prime}\in(R^{+})^{2d\times n}" class="ltx_Math" display="inline" id="A3.SS4.p4.m4" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>R</mi><mo>+</mo></msup><mo stretchy="false">)</mo></mrow><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{\prime}\in(R^{+})^{2d\times n}</annotation></semantics></math> be defined as in the proof of Claim <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmclaim3" title="Claim 3. ‣ 3.4.1 Expressing matrices 𝐷_𝑥,𝐷_𝑦,𝐸 as graphs 𝐺_𝑥,𝐺_𝑦 ‣ 3.4 Expressing BDH-GPU using BDH: preserving parameter and state size ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">3</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A3.SS3" title="C.3 Proof of Claim 3 ‣ Appendix C Omitted formal claims and proofs ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">C.3</span></a>, and let <math alttext="{D_{y}}^{\mathfrak{e}}" class="ltx_Math" display="inline" id="A3.SS4.p4.m5" intent=":literal"><semantics><mmultiscripts><mi>D</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><annotation encoding="application/x-tex">{D_{y}}^{\mathfrak{e}}</annotation></semantics></math>, <math alttext="{D_{y}}^{\mathfrak{i}}" class="ltx_Math" display="inline" id="A3.SS4.p4.m6" intent=":literal"><semantics><mmultiscripts><mi>D</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><annotation encoding="application/x-tex">{D_{y}}^{\mathfrak{i}}</annotation></semantics></math> also be applied as in that proof for considerations of decoder <math alttext="{D_{y}}" class="ltx_Math" display="inline" id="A3.SS4.p4.m7" intent=":literal"><semantics><msub><mi>D</mi><mi>y</mi></msub><annotation encoding="application/x-tex">{D_{y}}</annotation></semantics></math>. Define the value preparation function <math alttext="A" class="ltx_Math" display="inline" id="A3.SS4.p4.m8" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> as the immersion of vectors over <math alttext="V" class="ltx_Math" display="inline" id="A3.SS4.p4.m9" intent=":literal"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> into <math alttext="D" class="ltx_Math" display="inline" id="A3.SS4.p4.m10" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> using <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="A3.SS4.p4.m11" intent=":literal"><semantics><msup><mi>E</mi><mo>′</mo></msup><annotation encoding="application/x-tex">E^{\prime}</annotation></semantics></math>. Define <math alttext="{G_{s}}" class="ltx_Math" display="inline" id="A3.SS4.p4.m12" intent=":literal"><semantics><msub><mi>G</mi><mi>s</mi></msub><annotation encoding="application/x-tex">{G_{s}}</annotation></semantics></math> to be the all-ones matrix on the <math alttext="2d" class="ltx_Math" display="inline" id="A3.SS4.p4.m13" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">2d</annotation></semantics></math> columns corresponding to <math alttext="D" class="ltx_Math" display="inline" id="A3.SS4.p4.m14" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, and zeros elsewhere. Then, define <math alttext="E^{*}\in R^{2d\times n}" class="ltx_Math" display="inline" id="A3.SS4.p4.m15" intent=":literal"><semantics><mrow><msup><mi>E</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>R</mi><mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">E^{*}\in R^{2d\times n}</annotation></semantics></math> to be a diagonal matrix acting on its first <math alttext="2d" class="ltx_Math" display="inline" id="A3.SS4.p4.m16" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">2d</annotation></semantics></math> elements (corresponding to <math alttext="D" class="ltx_Math" display="inline" id="A3.SS4.p4.m17" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>), and zeros elsewhere. Setting <math alttext="{G_{y}}^{\mathfrak{e}}={D_{y}}^{\mathfrak{e}}E^{*}" class="ltx_Math" display="inline" id="A3.SS4.p4.m18" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>D</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔢</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">{G_{y}}^{\mathfrak{e}}={D_{y}}^{\mathfrak{e}}E^{*}</annotation></semantics></math> and <math alttext="{G_{y}}^{\mathfrak{i}}={D_{y}}^{\mathfrak{i}}E^{*}" class="ltx_Math" display="inline" id="A3.SS4.p4.m19" intent=":literal"><semantics><mrow><mmultiscripts><mi>G</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo>=</mo><mrow><mmultiscripts><mi>D</mi><mi>y</mi><mrow></mrow><mrow></mrow><mi>𝔦</mi></mmultiscripts><mo lspace="0em" rspace="0em">​</mo><msup><mi>E</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding="application/x-tex">{G_{y}}^{\mathfrak{i}}={D_{y}}^{\mathfrak{i}}E^{*}</annotation></semantics></math>, we obtain the claim.
∎</p>
</div>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Desirable properties of a local graph dynamics for language models</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p">We outline several general criteria of computational expressiveness and computational efficiency which a distributed computing system has to meet to effectively deal with language and reasoning. For this, we take a first-principles approach, relying only on very fundamental properties which an attention-based language model appears to need to capture, and which are applicable far beyond the specific case of BDH — plausibly, being equally applicable to human and human-like reasoning.<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>In particular, the reader will have no doubt observed that graph settings applicable to language inference and reasoning systems, which involve task inputs spread out over time and the emergence of graph structure, are very different from graph-based frameworks which directly associate the task to solve with the communication graph (the latter case includes most considerations of: Graph Neural Networks, Graph Transformers, the LOCAL/CONGEST model of distributed computing, Approximate Message Passing systems, etc.)</span></span></span></p>
</div>
<div class="ltx_theorem ltx_theorem_hypothesis" id="Thmhypothesis2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Hypothesis 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmhypothesis2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">We expect any efficient graph-based distributed system dealing with language and reasoning using an attention-based approach to have the following characteristics:</span></p>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">[No Easy Simulation] The system achieves computationally irreducible dynamics, i.e., it provides no systematic opportunity to predict the outcomes of its inference or approximate its dynamics in a numerically easier way than by running the system itself.</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">[Particles Talk] The state-space dynamics of the distributed system is a non-linear interacting particle dynamics, i.e., the system does not admit an efficient representation as a non-interacting particle system, but relies on a form of non-linear evolution expressed through (at least) two-particle interactions. (Such interactions are necessary, in particular, to enable multi-point correlation analysis on language inputs, when assuming only a small number of inference steps of the system per output token.)</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">[Attention Deforms Pairwise Connections] The system is capable of computing correlations between pairs of scalar variables localized at different nodes of the distributed system, and storing the state of such correlations so that the result is accessible from these two nodes. (This is plausibly needed to express attention in a state-space system.)</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">[Time Dictates Structure] The communication graph of the distributed system does not, in itself, represent any specific task input to solve, but reflects a trained model (a program), whereas tasks are represented as inputs to this program, presented over time. The communication graphs used to solve language and reasoning problems are expected to display modular, scale-free structure.</span></p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">A detailed discussion of the four items of the Hypothesis is provided below.</em></p>
</div>
<section class="ltx_subsubsection" id="A4.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">[No Easy Simulation] <math alttext="\diamond" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.m1" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> Computational models have irreducible dynamics.</h4>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p1">
<p class="ltx_p">We start by recalling a general observation which is applicable to most learning systems <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> (machine learning models, biological systems) that have learned how to do computations: they are likely to have chosen state-space dynamics that will allow them to resolve their computational problem with the least effort during inference. In other words, <em class="ltx_emph ltx_font_italic">if there is a physical system <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> that solves a given computational problem, and if there exists a simulation <math alttext="S(P)" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m3" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(P)</annotation></semantics></math> of this physical system that would approximate system <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m4" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> with less effort, the learning system <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m5" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> will be following the dynamics of <math alttext="S(P)" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m6" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(P)</annotation></semantics></math>, not those of <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p1.m7" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>.</em></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p2">
<p class="ltx_p">We provide a few hypothetical examples for intuition, anchored in different areas of particle dynamics.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p3">
<p class="ltx_p">If <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p3.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> were the particle dynamics of electrons in a resistor network, the simulation <math alttext="S(P)" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p3.m2" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(P)</annotation></semantics></math> could be a calculation based on Ohm’s law with a Laplacian solver — and we would consequently expect the dynamics of our computational system <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p3.m3" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> to follow the Laplacian solver code, and not to simulate electron dynamics.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p4">
<p class="ltx_p">If <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p4.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> were the ensemble of billions of Internet users performing short walks clicking through links of the world wide web, the simulation <math alttext="S(P)" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p4.m2" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(P)</annotation></semantics></math> would be a calculation of aggregate behavior, reminiscent of PageRank, and we would expect <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p4.m3" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> to encode the parallel dynamics of Map-Reduce matrix operations of PageRank, not the simulation of individual agents.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p5">
<p class="ltx_p">If <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p5.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> were a quantum system amenable to approximation by perturbation theory, we would expect <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p5.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> to simulate the (classical) calculus of this perturbation theory, and not the quantum system <math alttext="P" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p5.m3" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> directly.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p6">
<p class="ltx_p">Most mechanical systems admit some form of more efficient simulation, which means the the dynamics of such systems are rarely a suitable choice for neuronal models. Anecdotally, in nature, only very simple systems like the Physarum slime mold <cite class="ltx_cite ltx_citemacro_citep">(Jabr and Rothschild, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib49" title="">2012</a>)</cite> rely on direct action (with hydrostatic pressure gradients) to perform their optimization process; and contemporary neuroscience research suggests that even the simplest neuronal brains do not perform their work in a similar “fluid-mechanical” manner.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx1.p7">
<p class="ltx_p">The irreducibility of <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx1.p7.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> means that this system is stretched to the limits of stability, just as a highly optimized numerical algorithm would be have been simplified and optimized to the limit of numerical stability. This relates to the limits of dimensionality reduction techniques that we have explored through a largely equivalent information-lens perspective of loss of precision and loss of information which it inflicts upon the model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS0.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">[Particles Talk] <math alttext="\diamond" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.m1" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> Latent concept spaces arise from outcomes of particle-particle interactions.</h4>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p1">
<p class="ltx_p">Dynamics of systems with multiple particles moving around in a (deformable) environment fall into two broad categories, depending on the strength of interaction between different parts of the dynamics. In the simpler setting, particles can be assumed <em class="ltx_emph ltx_font_italic">at short time scales</em> to be moving in an environment unchanged by other particles — the concurrent action of other particles, which would change the environment, does not need to be taken into account when representing individual particle motion, nor is it necessary to consider particle-particle interactions. By contrast, in the more general setting, the dynamics of multiple particles are tightly coupled, and their dynamics need to be modeled (simulated) together.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p2">
<p class="ltx_p">An example of a dynamics with no coupling would be a dynamics of multiple independent random walkers, such as the previously mentioned dynamics of electricity in wires, or the dynamics of PageRank. Examples of dynamics including interactions between particles, which may either happen directly or be moderated through the environment, include cellular automata, particle method simulations and molecular simulations, or swarms of communicating agents.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p3">
<p class="ltx_p">The natural representation of state-space models as moving particles comes from the following interpretation. A distributed system with depth-<math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> computations (not least BDH or the BDH-GPU model given by the state equations (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E4" title="In Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>)) is amenable to interpretation as a system of walker particles performing an <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-step walk over layers, starting at some token <math alttext="t_{0}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m3" intent=":literal"><semantics><msub><mi>t</mi><mn>0</mn></msub><annotation encoding="application/x-tex">t_{0}</annotation></semantics></math> in the input layer <math alttext="0" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m4" intent=":literal"><mn>0</mn></math> and, in each time step <math alttext="t\geq t_{0}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m5" intent=":literal"><semantics><mrow><mi>t</mi><mo>≥</mo><msub><mi>t</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">t\geq t_{0}</annotation></semantics></math>, either pausing (skipping a time step) or moving on to the next layer, until they reach the last layer <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m6" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> in some time step <math alttext="t_{f}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m7" intent=":literal"><semantics><msub><mi>t</mi><mi>f</mi></msub><annotation encoding="application/x-tex">t_{f}</annotation></semantics></math>, at which point they leave the system, contributing to the distribution of the <math alttext="t_{f}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m8" intent=":literal"><semantics><msub><mi>t</mi><mi>f</mi></msub><annotation encoding="application/x-tex">t_{f}</annotation></semantics></math>-th output token. When attempting this approach with <em class="ltx_emph ltx_font_italic">independent</em> walkers, the distribution of tokens output by such a system could be described by correlation functions following or resembling the Dyson series, <math alttext="\sum_{\tau_{L}=0}^{t}\sum_{\tau_{L-1}=0}^{\tau_{L}-1}\ldots\sum_{\tau_{1}=0}^{\tau_{2}-1}F(\textrm{input}(\tau_{1}),\ldots,\textrm{input}(\tau_{L}))" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m9" intent=":literal"><semantics><mrow><msubsup><mo>∑</mo><mrow><msub><mi>τ</mi><mi>L</mi></msub><mo>=</mo><mn>0</mn></mrow><mi>t</mi></msubsup><mrow><msubsup><mo lspace="0.167em">∑</mo><mrow><msub><mi>τ</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>τ</mi><mi>L</mi></msub><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><msub><mi>τ</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>τ</mi><mn>2</mn></msub><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>input</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mtext>input</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mi>L</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\sum_{\tau_{L}=0}^{t}\sum_{\tau_{L-1}=0}^{\tau_{L}-1}\ldots\sum_{\tau_{1}=0}^{\tau_{2}-1}F(\textrm{input}(\tau_{1}),\ldots,\textrm{input}(\tau_{L}))</annotation></semantics></math>. However, the output of attention (e.g., the linear attention output <math alttext="a^{*}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m10" intent=":literal"><semantics><msup><mi>a</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">a^{*}</annotation></semantics></math> given by equation (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E4" title="In Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>) for BDH-GPU, or defined similarly in other state space models based on linear attention), cannot be represented as a Dyson formula when unrolling the dynamics backwards through layers (even if it looks deceptively similar at first glance). Each entry retrieved from attention is an interplay between two moments of time: the moment at which the key-value pair was entered, and the moment at which the corresponding query arrived. In consequence, the considered dynamics can be represented, in each layer, as a linear sum of two-point correlations between current time <math alttext="t" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m11" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and some point <math alttext="\tau" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m12" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> in the past. Thus, in the <math alttext="l" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m13" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>-th layer, this recursion can (with some approximation) be unrolled into a linear combination of functions of sets of <math alttext="2^{l}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m14" intent=":literal"><semantics><msup><mn>2</mn><mi>l</mi></msup><annotation encoding="application/x-tex">2^{l}</annotation></semantics></math> input tokens (provided in the <math alttext="0" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m15" intent=":literal"><mn>0</mn></math>-th layer), but cannot be represented through correlation functions <math alttext="F" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m16" intent=":literal"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math> on smaller sets of tokens (e.g., of size linear in <math alttext="l" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m17" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>). Otherwise put, a system like BDH can be described using particles performing <math alttext="l" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m18" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>-step walks when <em class="ltx_emph ltx_font_italic">relying on intermediate elements of KV-state <math alttext="{\boldsymbol{\sigma}}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m19" intent=":literal"><semantics><mi>𝛔</mi><annotation encoding="application/x-tex">{\boldsymbol{\sigma}}</annotation></semantics></math></em>, which are produced during interactions with other walker particles in intermediate layers, but needs to be viewed through at least <math alttext="2^{l}" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p3.m20" intent=":literal"><semantics><msup><mn>2</mn><mi>l</mi></msup><annotation encoding="application/x-tex">2^{l}</annotation></semantics></math>-point correlation functions defined directly on input tokens in the input layer.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p4">
<p class="ltx_p">The considered point is relevant because it <em class="ltx_emph ltx_font_italic">precludes many forms of modeling of attention-based language dynamics, in particular those using non-interacting particle theories</em>. The precluded approaches include:</p>
<ul class="ltx_itemize" id="A4.I2">
<li class="ltx_item" id="A4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i1.p1">
<p class="ltx_p"><math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-grams, <span class="ltx_text ltx_font_sansserif">word2vec</span>-like <math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-skip-grams <cite class="ltx_cite ltx_citemacro_citep">(Mikolov et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib73" title="">2013</a>)</cite>, as well as any other <math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i1.p1.m3" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-point correlations of past input tokens.</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i2.p1">
<p class="ltx_p"><math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i2.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-step non-interacting random walk models (walks inside the network structure, which move from input layers towards output layers across time).</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i3.p1">
<p class="ltx_p">systems known to be equivalent to the above, such as approximations of classical spin-chain systems by means of Feynman integral path lengths bounded by <math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i3.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Kalev and Hen, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib51" title="">2025</a>)</cite>, and many forms of graph/GNN kernels based on <math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i3.p1.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-th powers of the graph Laplacian.</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A4.I2.i4.p1">
<p class="ltx_p">by extension, <math alttext="L" class="ltx_Math" display="inline" id="A4.I2.i4.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-layer state-space systems which perform excessive compression (size reduction) of their state, in a way which eliminates most long-term correlations.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p5">
<p class="ltx_p">We can ask if this requirement for communication between particles is an artifact of the construction of BDH (and similarly, of the Transformer), or if it comes from a genuine need related to language and reasoning tasks. For language problems <span class="ltx_text ltx_font_slanted">per se</span>, the need for multi-point token correlation in <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p5.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>-layer language modeling plausibly follows from the expectation that the model should have the ability to create a syntax tree of a sentence by means of a single quick parallel scan over words in this sentence. With this assumption, the depth <math alttext="L" class="ltx_Math" display="inline" id="A4.SS0.SSSx2.p5.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> of computation used to build a language syntax tree should be sufficient to represent the number of <em class="ltx_emph ltx_font_italic">levels</em> of the syntax tree that the model is able to process naturally, but can be (and in general, should plausibly be) much smaller than the number of <em class="ltx_emph ltx_font_italic">leaves</em> (words) of this syntax tree. This is consistent with the RASP-L-based understanding of the Transformer’s capabilities, which allows for expressing depth-L trees in a depth-L Transformer.<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>This does not mean the problem is easy; synthetic problems inspired by this type of tree problem were (for us) among the hardest to train into a Transformer with no Chain-of-Thought — as compared to RASP-L problems described in <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib114" title="">2024</a>)</cite> and others we tested.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx2.p6">
<p class="ltx_p">Such a way of mapping the tree structure of problems into the model’s layers, from bottom to top, also essentially captures the “generative” nature of the considered models, which rely on concept spaces created and stored in state in intermediate layers, to guide both language comprehension and reasoning on language. Thus, the ability to handle language syntax trees efficiently, in itself, precludes the previously-mentioned types of modeling approaches.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS0.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">[Attention Deforms Pairwise Connections] <math alttext="\diamond" class="ltx_Math" display="inline" id="A4.SS0.SSSx3.m1" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> The interaction process <math alttext="X(i),Y(j)\to\sigma(i,j)" class="ltx_Math" display="inline" id="A4.SS0.SSSx3.m2" intent=":literal"><semantics><mrow><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">X(i),Y(j)\to\sigma(i,j)</annotation></semantics></math> describes attention.</h4>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx3.p1">
<p class="ltx_p">The preceding discussion in paragraph [Particles Talk] grounds state-of-the-art state-space language models in the world of interacting particle systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx3.p2">
<p class="ltx_p">Whenever the global vector-based description of a state-space model calls for a three-point operation, such as the trilinear operation of key-value-query attention, this translates into the nature of pairwise (for polynomial interaction terms, degree-two) non-linear particle interactions in the transition equations of the same model when described at the level of particles. Notably, at scale, <em class="ltx_emph ltx_font_italic">the state-space transition equations of an attention-based model plausibly involve altering or deforming correlation strength between pairs of particles, with such pairs being represented as interaction variables in the state of the system</em>. This requirement on structure, repeated across layers, can be seen as sufficient: interactions of particle pairs are about the only requirement on non-linear rulesets that the system needs to be support, as demonstrated by the simple local transition rules of BDH.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx3.p3">
<p class="ltx_p">Overall, the statement “attention is all you need”, which describes a system-level global property,
translates into “<math alttext="{X(i),Y(j)\to\sigma(i,j)}" class="ltx_Math" display="inline" id="A4.SS0.SSSx3.p3.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>X</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>Y</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">→</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{X(i),Y(j)\to\sigma(i,j)}</annotation></semantics></math> is all you need” at the level of particle dynamics of a state-space language model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS0.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">[Time Dictates Structure] <math alttext="\diamond" class="ltx_Math" display="inline" id="A4.SS0.SSSx4.m1" intent=":literal"><semantics><mo>⋄</mo><annotation encoding="application/x-tex">\diamond</annotation></semantics></math> Inputs to reasoning problems are sequential, not graph-based.</h4>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p1">
<p class="ltx_p">Many real-world graphs are anchored in a spatial embedding of their nodes which is given by external constraints. For example, the structure of many social and transportation networks is impacted by the geographical placement of people and infrastructure on the globe.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p2">
<p class="ltx_p">In designing the dynamics for BDH, we are free from such spatial constraints. The graph topology corresponding to the model is free to take the shape needed to best resolve the problem. The problem itself is encoded as a sequence of tokens which arrive over time to the model (we take here a state-space view of the system).
We can naturally presume that the structure of the model graph of BDH is shaped in a way which follows from two aspects: this temporal encoding of information, and from the abstract (Platonic) latent space of concepts needed to deal with language and reasoning.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p3">
<p class="ltx_p">When looking for the right particle dynamics for language models, it seems reasonable to discard all <em class="ltx_emph ltx_font_italic">unnecessary</em> aspects of spatial constraints.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p4">
<p class="ltx_p">One example of a particle interaction system which includes externally imposed constraints on the structure of the state space is that of cellular automata operating on a two-dimensional grid. While 2D cellular automata have appealed to public imagination, appearing in attempts to observe the emergence of intelligence at least since the 1970’s, they are, in fact, an extremely cumbersome choice for representing in-context reasoning or language for any attention-based model. State-of-the-art language models seem to have no structural need for a low-dimensional grid in their dynamics. Arguably, the connection structure which needs to emerge in a graph system, allowing it to work efficiently in a setting of efficient information search
is precisely the opposite: it is a multi-scale, expander-like system of shortcuts, cf. e.g. <cite class="ltx_cite ltx_citemacro_citep">(Fraigniaud and Giakkoupis, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib33" title="">2010</a>)</cite>. This scale-free graph structure is expected to correspond to the scale-free temporal behavior observed in natural systems <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib37" title="">2010</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p5">
<p class="ltx_p">In the rest of this paragraph we briefly review other areas of computer science, and how they relate to the particle dynamics we are looking for in terms of their relationship to handling temporal inputs and the constraints they impose on the structure of the state-space.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p6">
<p class="ltx_p">The freedom of choice of graph topology in solving problems around language and in-context reasoning, which we are dealing with here, can be contrasted with settings in which the graph is, at the same time, part of the system dynamics (encoding interactions in the system) and a part of the statement of the problem input. This is particularly true for models of distributed computing inspired by computer networking (LOCAL, CONGEST, etc.) and other forms of interaction networks (Approximate Message Passing, quantum LOCC, etc.), where the same graph <math alttext="G" class="ltx_Math" display="inline" id="A4.SS0.SSSx4.p6.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> represents the communication network for the dynamics, and encodes the problem input — with the required output being some function of <math alttext="G" class="ltx_Math" display="inline" id="A4.SS0.SSSx4.p6.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> (e.g., a clustering, coloring, spanning tree, etc.). Some distributed problems on graphs can be formulated so that the input and required output are independent of the graph structure, the notable ones being: majority consensus, leader election, information broadcasting, and computing aggregates. For such problems, the graph represents only a communication system, whose topology is more an obstacle to overcome, than an actual help in solving the problem. This applies also to architectures in Machine Learning which adhere to a known graph structure, such as Graph Neural Networks or Graph Transformers, when solving problems whose inputs are not naturally embedded in such a structure.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS0.SSSx4.p7">
<p class="ltx_p">A handful of approaches in distributed computing are intended to describe systems which compute a function of an input signal which, like language, is spread out sequentially over time, and where computations happen while this signal is still arriving. In particular, some forms of particle dynamics can be distilled from the theory of self-stabilizing systems <cite class="ltx_cite ltx_citemacro_citep">(Dolev, <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib28" title="">2000</a>)</cite>, giving rise to settings where the system is expected to adapt its state in response to a time-changing input (see e.g. <cite class="ltx_cite ltx_citemacro_citep">(Boczkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib14" title="">2019</a>)</cite>).
Among distributed streaming frameworks, one approach which, owing to its design, admits an elegant particle-based interpretation for time-changing inputs, is the incremental computing framework <cite class="ltx_cite ltx_citemacro_citep">(McSherry et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#bib.bib71" title="">2013</a>)</cite>. This framework emphasizes temporal commutativity, and is well suited to expressing dynamics of non-interacting particles, such as PageRank-like computation performed incrementally with Map-Reduce on time-changing graphs, or building nearest-neighbor indexes on sets of changing vectors. It does not naturally extend to the non-linear particle-particle interaction dynamics that appear in the context of attention (see paragraph [Particles Talk]).</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>BDH-GPU PyTorch code listing</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p">The code listing below implements BDH-GPU (Definition <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#Thmdefinition4" title="Definition 4 (inference dynamics of BDH-GPU). ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">4</span></a>) for PyTorch version 2.7. It is self-contained, except for the implementation of RoPE which needs to be filled by the user. With respect to the state dynamics of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>), it provides an extension supporting heads. The placement of layer norms and residual connections is modified with respect to Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#S3.E8" title="In Figure 3 ‣ State-space representation. ‣ 3.2 Definition of BDH-GPU as a state-space system ‣ 3 BDH-GPU: a tensor-friendly version of the BDH architecture ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">8</span></a>); in general, this aspect offers some flexibility.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.p2">
<p class="ltx_p">This implementation assumes the simplest case of a fixed context window of length <math alttext="T" class="ltx_Math" display="inline" id="A5.p2.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>. An unbounded context window is technically supported using a state-space kernel for Linear Attention, and works best following appropriate adaptation of the model for truncated backpropagation through time (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.26507v1#A2.SS2" title="B.2 BDH Scaling Experimental Details ‣ Appendix B Further description of experiments ‣ The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"><span class="ltx_text ltx_ref_tag">B.2</span></a>).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A5.p3">
<div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,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">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_keyword ltx_font_bold">import</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_keyword ltx_font_bold">import</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>.<span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">functional</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">as</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">F</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_keyword ltx_font_bold">from</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">import</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_identifier">D</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>256<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>internal<span class="ltx_text ltx_lst_space"> </span>dimension</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier">H</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>4<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>heads</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_identifier">N</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>32768<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>neurons</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier">L</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>6<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>layers</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier">dropout</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>0.05
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier">vocab_size</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span>256
</div>
<div class="ltx_listingline" id="lstnumberx11">
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_lst_keyword ltx_font_bold">class</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">BDH_GPU</span>(<span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Module</span>):
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">def</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">__init__</span>(<span class="ltx_text ltx_lst_identifier">self</span>):
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">ln</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">LayerNorm</span>(<span class="ltx_text ltx_lst_identifier">D</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">elementwise_affine</span>=<span class="ltx_text ltx_lst_identifier">False</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">bias</span>=<span class="ltx_text ltx_lst_identifier">False</span>)
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">wte</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Embedding</span>(<span class="ltx_text ltx_lst_identifier">vocab_size</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">D</span>)
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">drop</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Dropout</span>(<span class="ltx_text ltx_lst_identifier">dropout</span>)
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">encoder</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Parameter</span>(
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>.<span class="ltx_text ltx_lst_identifier">zeros</span>((<span class="ltx_text ltx_lst_identifier">N</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">D</span>)).<span class="ltx_text ltx_lst_identifier">normal_</span>(<span class="ltx_text ltx_lst_identifier">std</span>=0.02)
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_space"> </span>)
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">decoder_x</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Parameter</span>(
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>.<span class="ltx_text ltx_lst_identifier">zeros</span>((<span class="ltx_text ltx_lst_identifier">H</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">D</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">N</span>//<span class="ltx_text ltx_lst_identifier">H</span>)).<span class="ltx_text ltx_lst_identifier">normal_</span>(<span class="ltx_text ltx_lst_identifier">std</span>=0.02)
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_lst_space"> </span>)
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">decoder_y</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Parameter</span>(
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>.<span class="ltx_text ltx_lst_identifier">zeros</span>((<span class="ltx_text ltx_lst_identifier">H</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">D</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">N</span>//<span class="ltx_text ltx_lst_identifier">H</span>)).<span class="ltx_text ltx_lst_identifier">normal_</span>(<span class="ltx_text ltx_lst_identifier">std</span>=0.02)
</div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_text ltx_lst_space"> </span>)
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">readout</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Parameter</span>(
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">torch</span>.<span class="ltx_text ltx_lst_identifier">zeros</span>((<span class="ltx_text ltx_lst_identifier">D</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">vocab_size</span>)).<span class="ltx_text ltx_lst_identifier">normal_</span>(<span class="ltx_text ltx_lst_identifier">std</span>=0.02)
</div>
<div class="ltx_listingline" id="lstnumberx28">
<span class="ltx_text ltx_lst_space"> </span>)
</div>
<div class="ltx_listingline" id="lstnumberx29">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">attn</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">LinearAttention</span>()
</div>
<div class="ltx_listingline" id="lstnumberx30">
</div>
<div class="ltx_listingline" id="lstnumberx31">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">def</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">forward</span>(<span class="ltx_text ltx_lst_identifier">self</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">idx</span>):
</div>
<div class="ltx_listingline" id="lstnumberx32">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">B</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">T</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">idx</span>.<span class="ltx_text ltx_lst_identifier">size</span>()<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>mini-batch<span class="ltx_text ltx_lst_space"> </span>dimensions</span>
</div>
<div class="ltx_listingline" id="lstnumberx33">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">v_ast</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">ln</span>(<span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">wte</span>(<span class="ltx_text ltx_lst_identifier">idx</span>).<span class="ltx_text ltx_lst_identifier">unsqueeze</span>(1))<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>B,<span class="ltx_text ltx_lst_space"> </span>1,<span class="ltx_text ltx_lst_space"> </span>T,<span class="ltx_text ltx_lst_space"> </span>D</span>
</div>
<div class="ltx_listingline" id="lstnumberx34">
</div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">for</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">_</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">in</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold">range</span>(<span class="ltx_text ltx_lst_identifier">L</span>):
</div>
<div class="ltx_listingline" id="lstnumberx36">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">x</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">F</span>.<span class="ltx_text ltx_lst_identifier">relu</span>(<span class="ltx_text ltx_lst_identifier">v_ast</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">decoder_x</span>)<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>B,<span class="ltx_text ltx_lst_space"> </span>H,<span class="ltx_text ltx_lst_space"> </span>T,<span class="ltx_text ltx_lst_space"> </span>N//H</span>
</div>
<div class="ltx_listingline" id="lstnumberx37">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">a_ast</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">attn</span>(
</div>
<div class="ltx_listingline" id="lstnumberx38">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">Q</span>=<span class="ltx_text ltx_lst_identifier">x</span>,
</div>
<div class="ltx_listingline" id="lstnumberx39">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">K</span>=<span class="ltx_text ltx_lst_identifier">x</span>,
</div>
<div class="ltx_listingline" id="lstnumberx40">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">V</span>=<span class="ltx_text ltx_lst_identifier">v_ast</span>,
</div>
<div class="ltx_listingline" id="lstnumberx41">
<span class="ltx_text ltx_lst_space"> </span>)
</div>
<div class="ltx_listingline" id="lstnumberx42">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">y</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">F</span>.<span class="ltx_text ltx_lst_identifier">relu</span>(<span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">ln</span>(<span class="ltx_text ltx_lst_identifier">a_ast</span>)<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">decoder_y</span>)<span class="ltx_text ltx_lst_space"> </span>*<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">x</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>B,<span class="ltx_text ltx_lst_space"> </span>H,<span class="ltx_text ltx_lst_space"> </span>T,<span class="ltx_text ltx_lst_space"> </span>N//H</span>
</div>
<div class="ltx_listingline" id="lstnumberx43">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">y</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">y</span>.<span class="ltx_text ltx_lst_identifier">transpose</span>(1,<span class="ltx_text ltx_lst_space"> </span>2).<span class="ltx_text ltx_lst_identifier">reshape</span>(<span class="ltx_text ltx_lst_identifier">B</span>,<span class="ltx_text ltx_lst_space"> </span>1,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">T</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">N</span>)
</div>
<div class="ltx_listingline" id="lstnumberx44">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">y</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">drop</span>(<span class="ltx_text ltx_lst_identifier">y</span>)
</div>
<div class="ltx_listingline" id="lstnumberx45">
</div>
<div class="ltx_listingline" id="lstnumberx46">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>Start<span class="ltx_text ltx_lst_space"> </span>of<span class="ltx_text ltx_lst_space"> </span>layer<span class="ltx_text ltx_lst_space"> </span>with<span class="ltx_text ltx_lst_space"> </span>vectors<span class="ltx_text ltx_lst_space"> </span>x,<span class="ltx_text ltx_lst_space"> </span>y</span>
</div>
<div class="ltx_listingline" id="lstnumberx47">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">v_ast</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">v_ast</span><span class="ltx_text ltx_lst_space"> </span>+<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">ln</span>(<span class="ltx_text ltx_lst_identifier">y</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">encoder</span>)<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>B,<span class="ltx_text ltx_lst_space"> </span>1,<span class="ltx_text ltx_lst_space"> </span>T,<span class="ltx_text ltx_lst_space"> </span>D</span>
</div>
<div class="ltx_listingline" id="lstnumberx48">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">v_ast</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">ln</span>(<span class="ltx_text ltx_lst_identifier">v_ast</span>)
</div>
<div class="ltx_listingline" id="lstnumberx49">
</div>
<div class="ltx_listingline" id="lstnumberx50">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">return</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">v_ast</span>.<span class="ltx_text ltx_lst_identifier">squeeze</span>(1)<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">self</span>.<span class="ltx_text ltx_lst_identifier">readout</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_comment ltx_font_italic">#<span class="ltx_text ltx_lst_space"> </span>B,<span class="ltx_text ltx_lst_space"> </span>T,<span class="ltx_text ltx_lst_space"> </span>vocab_size</span>
</div>
<div class="ltx_listingline" id="lstnumberx51">
</div>
<div class="ltx_listingline" id="lstnumberx52">
<span class="ltx_text ltx_lst_keyword ltx_font_bold">class</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">LinearAttention</span>(<span class="ltx_text ltx_lst_identifier">nn</span>.<span class="ltx_text ltx_lst_identifier">Module</span>):
</div>
<div class="ltx_listingline" id="lstnumberx53">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">def</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">forward</span>(<span class="ltx_text ltx_lst_identifier">Q</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">K</span>,<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">V</span>):
</div>
<div class="ltx_listingline" id="lstnumberx54">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">Qr</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">RoPE</span>(<span class="ltx_text ltx_lst_identifier">Q</span>)
</div>
<div class="ltx_listingline" id="lstnumberx55">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">Kr</span><span class="ltx_text ltx_lst_space"> </span>=<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">RoPE</span>(<span class="ltx_text ltx_lst_identifier">K</span>)
</div>
<div class="ltx_listingline" id="lstnumberx56">
</div>
<div class="ltx_listingline" id="lstnumberx57">
<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold">return</span><span class="ltx_text ltx_lst_space"> </span>(<span class="ltx_text ltx_lst_identifier">Qr</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">Kr</span>.<span class="ltx_text ltx_lst_identifier">mT</span>).<span class="ltx_text ltx_lst_identifier">tril</span>(<span class="ltx_text ltx_lst_identifier">diagonal</span>=-1)<span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">@</span><span class="ltx_text ltx_lst_space"> </span><span class="ltx_text ltx_lst_identifier">V</span>
</div>
</div>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 30 16:50:07 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
